[["index.html", "Adventures in Data Science Overview", " Adventures in Data Science Dr. Carl G. Stahmer Dr. Pamela L. Reynolds Dr. Tyler Shoemaker Carrie Alexander Arthur Koehl Nick Ulle Jared Joseph Dr. Wesley Brooks 2021-03-08 Overview This is the course reader for IST008, Adventures in Data Science: Social Science Edition. The course is designed to provide students with a basic understanding of computing and network architecture, basic programming skills, and an introduction to common methods in Data Science and Digital Humanities. This coure reader provides background information that will help you to better understand the concepts that we will discuss in class and to better participate in the hands-on portion of the course. "],["working-with-the-command-line.html", "1 Working with the Command Line 1.1 Interacting with the Command Line 1.2 Common Command Line Commands 1.3 Command Line Text Editors 1.4 Basic Vim Commands", " 1 Working with the Command Line Most users interact with their computer through a Graphical User Interface (GUI) that allows them to use a mouse, keyboard, and graphical elements on screen (such as file menus, pictures of folders and files, etc.) to perform their work. Users tend to conflate their Operating System and their GUI because computer hardware and software manufacturers tightly pack these two concerns as a convenience to users. But the Windows 10 or Mac Big Sur operating system that makes your computer work and the Windows 10 or Mac Big Sur GUI that you interact with are, in fact completely different and separable software packages and it is possible to use different methods/software to interact with your computer than the stock, tightly coupled GUI that launches automatically when you turn on your computer. Because computer manufacturers like Windows and Mac devote so many resources to the development of their system GUIs, there are few viable (at present, none, commercially available) competing GUIs for these platforms. This is not the case in the Linux world, however, where users have several system GUI packages from which to choose and can seamlessly switch between them as desired. Despite the lack of competition/choice on the GUI front when it comes to interacting with your computer, there are other, non-graphical ways of communicating directly with your operating system that exist for all operating systems. We call these “Command Line” interfaces. The Command Line offers a text-only, non graphical means of interacting with your computer. In the early days of computing, all user interaction with the computer happened at the command line. In the current days of graphical user interfaces, using the Command Line requires you to launch a special program that provides Command Line access. Mac users will use an application called “Terminal” which ships by default with the Mac operating system. To launch the Terminal application, go to: Applications -&gt; Utilities -&gt; Terminal When you launch the application, you will see something like this: Windows users will use an application called Git Bash, which was installed on your system when you installed Git. To launch Git Bash, go to: Click on the Windows Start Menu and search for “Git Bash” Alternatively, Click on the Windows Start Menu, select Programs, and browse to Git Bash When you launch the application, you will see something like this: 1.1 Interacting with the Command Line While it can look intimidating to those raised on the GUI, working with the Command Line is actually quite simple. Instead of pointing and clicking on things to make them happen, you type written commands. The figure below shows a new, empty Command Line Interface in the Mac Terminal application The Command Line prompt contains a lot of valuable information. The beginning of the line, “(base) MacPro-F5KWP01GF694” tells us exactly which computer we are communication with. This may seem redundant, but it is actually possible to interact with computers other than the one you are typing on by connecting to them via the Command Line over the network. The bit of information after the colon, in this example the “~” character tells us where in the computer’s filesystem we are. We’ll learn more about this later, for now you need to undersant that the “~” character means that you are in your home directory. The next piece of information we are given is the username under which we are logged into the computer, in this case, my local username, “cstahmer.” After the username, we see the “$” character. This is known as the Command Prompt. It is an indicator that the Command Line application is waiting for you to enter something. The Command Prompt character is used througout these materials when giving command examples. When working through materials, DO NOT ENTER the Command Prompt. It will already be there telling you that the computer is ready to receive your command. Depending on your system and/or Command Line interface, you may or may not also see a solid or flashing box that appears after the Command Prompt. This is a Cursor Position Indicator, which tells you where the current cursor is in the terminal. This is useful if you need to go gack and correct an error. Generally speaking, you can’t click a mouse in a terminal app to edit text. You need to use your computer’s right and left arrows to move the cursor to the correct location and then make your edit. As noted earlier, we interact with the Command Line by typing commands. The figure below shows an example of a simple command, “echo” being entered into the Command Line. The “echo” command prints back to screen any text that you supply to the command It literally echoes your text. To execute, this or any command, you simply hit the “return” or “enter” key on your keyboard. You’ll see that when you execute a Command Line command the sytem performs the indicated operation, prints any output from the operation to screen and then delivers a new Command Line prompt. Note that depending on your particular system and/or Command Line interface, things might look slightly different on your computer. However, the basic presentation and function as described above will be the same. 1.2 Common Command Line Commands During our hands-on, in-class session we will practice using the following Command Line commands. Be prepared to have this page ready as a reference during class to make things easier. Table 1.1: Command Name Function ls List Lists all files in the current directory. ls -l List with Long flag Lists additional information about each file. ls -a List with All flag Lists all files, including hidden files. pwd Print Working Directory Prints the current working directory. mkdir Make Directory Creates a new file directory. cd Change Directory Navigates to another directory on the file system. mv Move Moves files. cp Copy Copies files. rm Remove/delete Deletes files. For a more complete list of Unix Commands, see the Unix Cheat Sheet. 1.3 Command Line Text Editors The Command Line also features a variety of different text editors, similar in nature to Microsoft Word or Mac Pages but much more stripped down. These editors are only accessible from the Command Line; we won’t spend very much time with them, but it is important to know how to use them so that you can open, read, and write directly in the Command Line window. Macs and Git Bash both ship with a text editor called Vim (other common editors include Emacs and Nano). To open a file with vim, type vi in a Command Line window, followed by the filename. If you want to create a new file, simply type the filename you’d like to use for that file after vi. Vim works a bit differently than other text editors and word processors. It has a number of ‘modes,’ which provide different forms of interaction with a file’s data. We will focus on two modes, Normal mode and Insert. When you open a file with Vim, the program starts in Normal mode. This mode is command-based and, somewhat strangely, it doesn’t let you insert text directly in the document (the reasons for this have to do with Vim’s underlying design philosophy: we edit text more than we write it on the Command Line). To insert text in your document, switch to Insert mode by pressing i. You can check whether you’re in Insert mode by looking at the bottom left hand portion of the window, which should read -- INSERT --. Once you are done inserting text, pressing ESC (the Escape key) will bring you back to Normal mode. From here, you can save and quit your file, though these actions differ from other text editors and word processors: saving and quitting with Vim works through a sequence of key commands (or chords), which you enter from Normal mode. To save a file in Vim, make sure you are in Normal mode and then enter :w. Note the colon, which must be included. After you’ve entered this key sequence, in the bottom left hand corner of your window you should see “[filename] XL, XC written” (L stands for “lines” and C stands for “characters”). To quit Vim, enter :q. This should take you back to your Command Line and, if you have created a new file, you will now see that file in your window. If you don’t want to save the changes you’ve made in a file, you can toss them out by typing :q! in place of :w and then :q. Also, in Vim key sequences for save, quit, and hundreds of other commands can be chained together. For example, instead of separately inputting :w and :q to save and quite a file, you can use :wq, which will produce the same effect. There are dozens of base commands like this in Vim, and the program can be customized far beyond what we need for our class. More information about this text editor can be found here. 1.4 Basic Vim Commands Table 1.2: Command Function esc Enter Normal mode. i Enter Insert mdoe. :w Save. :q Quit. :q! Quit without saving. For a more complete list of Vim commands, see this Cheat Sheet. "],["introduction-to-version-control.html", "2 Introduction to Version Control 2.1 What is Version Control? 2.2 Software Assisted Version Control 2.3 Local vs Server Based Version Control 2.4 Central Version Control Systems 2.5 Distributed Version Control Systems 2.6 The Best of Both Worlds 2.7 VCS and the Computer File System 2.8 How Computers Store and Access Information 2.9 How VCS Manage Your Files 2.10 Graph-Based Data Management 2.11 Additional Resources", " 2 Introduction to Version Control This section covers the basics of using Version Control Software (VCS) to track and record changes to files on your local computer. It provides background information that will help you to better understand what VCS is, why we use it, and how it does its work. 2.1 What is Version Control? Version control describes a process of storing and organizing multiple versions (or copies) of documents that you create. Approaches to version control range from simple to complex and can involve the use of various human workflows and/or software applications to accomplish the overall goal of storing and managing multiple versions of the same document(s). Most people have a folder/directory somewhere on their computer that looks something like this: Or perhaps, this: This is a rudimentary form of version control that relies completely on the human workflow of saving multiple versions of a file. This system works minimally well, in that it does provide you with a history of file versions theoretically organized by their time sequence. But this filesystem method provides no information about how the file has changed from version to version, why you might have saved a particular version, or specifically how the various versions are related. This human-managed filesystem approach is more subject to error than software-assisted version control systems. It is not uncommon for users to make mistakes when naming file versions, or to go back and eit files out of sequence. Software-assisted version control systems (VCS) such as Git were designed to solve this problem. 2.2 Software Assisted Version Control Version control software has its roots in the software development community, where it is common for many coders to work on the same file, sometimes synchronously, amplifying the need to track and understand revisions. But nearly all types of computer files, not just code, can be tracked using modern version control systems. IBM’s OS/360 IEBUPDTE software update tool is widely regarded as the earliest and most widely adopted precursor to modern, version control systems. Its release in 1972 of the Source Code Control System (SCCS) package marked the first, fully fledged system designed specifically for software version control. Today’s marketplace offers many options when it comes to choosing a version control software system. They include systems such as Git, Visual Source Safe, Subversion, Mercurial, CVS, and Plastic SCM, to name a few. Each of these systems offers its twist on version control, differing sometimes in the area of user functionality, sometimes in how it handles things on the back-end, and sometimes both. This tutorial focuses on the Git VCS, but in the sections that follow we offer some general information about classes of version control systems to help you better understand how Git does what it does and help you make more informed decisions about how to deploy it for you own work. 2.3 Local vs Server Based Version Control There are two general types of version control systems: Local and Server (sometimes called Cloud) based systems. When working with a Local version control system, all files, metadata, and everything associated with the version control system live on your local drive in a universe unto itself. Working locally is a perfectly reasonable option for those who work independently (not as part of a team), have no need to regularly share their files or file versions, and who have robust back-up practices for their local storage drive(s). Working locally is also sometimes the only option for projects involving protected data and/or proprietary code that cannot be shared. Server based VCS utilize software running on your local computer that communicates with a remote server (or servers) that store your files and data. Depending on the system being deployed, files and data may reside exclusively on the server and are downloaded to temporary local storage only when a file is being actively edited. Or, the system may maintain continuous local and remote versions of your files. Server based systems facilitate team science because they allow multiple users to have access to the same files, and all their respective versions, via the server. They can also provide an important, non-local back-up of your files, protecting you from loss of data should your local storage fail. Git is a free Server based version control system that can store files both locally and on a remote server. While the sections that follow offer a broader description of Server based version control, in this workshop we will focus only on using Git locally and will not configure the software to communicate with, store files on, or otherwise interact with a remote server. DataLab’s companion “Git for Teams” workshop focuses on using Git with the GitHub cloud service to capitalize on Git’s distributed version control capabilities. Server based version control systems can generally be segmented into two distinct categories: 1) Centralized Version Control Systems (Centralized VCS) and 2) Distributed Version Control Systems (Distributed VCS). 2.4 Central Version Control Systems Centralized VCS is the oldest and, surprisingly to many, still the dominant form of version control architecture worldwide. Centralized VCS implement a “spoke and wheel” architecture to provided server based version control. With the spoke and wheel architecture, the server maintains a centralized collection of file versions. Users utilize version control clients to “check-out” a file of interest to their local file storage, where they are free to make changes to the file. Centralized VCS typically restrict other users from checking out editable versions of a file if another user currently has the file checked out. Once the user who has checked out the file has finished making changes, they “check-in” their new version, which is then stored on the server from where it can be retrieved and “checked-out” by another user. As can be seen, Centralized VCS provide a very controlled and ordered universe that ensures file integrity and tracking of changes. However, this regulation comes at a cost. Namely, it reduces the ease with which multiple users can work simultaneously on the same file. 2.5 Distributed Version Control Systems Distributed VCS are not dependent on a central repository as a means of sharing files or tracking versions. Distributed VCS implement a network architecture (as opposed to the spoke and wheel of the Centralized VCS as pictured above) to allow each user to communicate directly with every other user. In Distributed VCS, each user maintains their own version history of the files being tracked, and the VCS software communicates between users to keep the various local file systems in sync with each other. With this type of system, the local versions of two different users will diverge from each other if both users make changes to the file. This divergence will remain in place until the local repositories are synced, at which time the VCS stitches (or merges) the two different versions of the file into a single version that reflects the changes made by each individual, and then saves the stitched version of the file onto both systems as the current version. Various mechanisms can then be used to resolve the conflicts that may arise during this merge process. Distributed VCS offer greater flexibility and facilitate collaborative work, but a lack of understanding of the sync/merge workflow can cause problems. It is not uncommon for a user to forget to synch their local repository with the repositories of other team members and, as a result, work for extended periods of time on outdated files that don’t reflect their teammates and result in work inefficiencies and merge challenges. 2.6 The Best of Both Worlds An important feature of Distributed VCS is that many users and organizations choose to include a central server as a node in the distributed network. This creates an hybrid universe in which some users will sync directly to each other while other users will sync through a central server. Syncing with a cloud-based server provides an extra level of backup for your files and also facilitates communication between users. But treating the server as just another node on the network (as opposed to a centralized point of control) puts the control and flexibility back in the hands of the individual developer. For example, in a true Centralized CVS, if the server goes down then nobody can check files in and out of the server, which means that nobody can work. But in a Distributed CVS this is not an issue. Users can continue to work on local versions and the system will sync any changes when the server becomes available. Git, which is the focus of this tutorial, is a Distributed VCS. You can use Git to share and sync repositories directly with other users or through a central Git server such as, for example, GitHub or GitLab. 2.7 VCS and the Computer File System When we think about Version Control, we typically think about managing changes to individual files. From the user perspective, the File is typically the minimum accessible unit of information. Whether working with images, tabular data, or written text, we typically use software to open a File that contains the information we want to view or edit. As such, it comes as a surprise to most users that the concept of Files, and their organizing containers (Folders or Directories), are not intrinsic to how computers themselves store and interact with data. In this section of the tutorial we will learn about how computers store and access information and how VCS interact with this process to track and manage files. 2.8 How Computers Store and Access Information For all of their computing power and seeming intelligence, computers still only know two things: 0 and 1. In computer speak, we call this a binary system, and the unit of memory on a hard-disk, flash drive, or computer chip that stores each 1 or 0 is called a bit. You can think of your computer’s storage device (regardless of what kind it is) as a presenting a large grid, where each box is a bit: In the above example, as with most computer storage, the bits in our storage grid are addressable, meaning that we can designate a particular bit using a row and column number such as, for example, A7, or E12. Also, remember, that each bit can only contain one of two values: 0 or 1. So, in practice, our storage grid would actually look something like this: All of the complex information that we store in the computer is translated to this binary language prior to storage using a system called Unicode. You can think of Unicode as a codebook that assigns a unique combination of 8, 16, 32, 64, etc. (depending on how old your computer is) ones and zeros to each letter, numeral, or symbol. For example, the 8-bit Unicode for the upper case letter “A” is “01000001,” and the 8-bit Unicode character for the digit “3” is “00110011.” The above grid actually spells out the phrase, “Call me Ishmael,” the opening line of Herman Melville’s novel Moby Dick. An important aspect of how computers story information in binary form is that, unlike most human readable forms of data storage, there is no right to left, up or down, or any other regularized organization of bits on a storage medium. When you save a file on your computer, the computer simply looks for any open bits and starts recording information. The net result is that the contents of single file are frequently randomly interleaved with data from other files. This mode of storage is used because it maximizes the use of open bits on the storage device. But it presents the singular problem of not making data readable in a regularized, linear fashion. To solve this problem, all computers reserve a particular part of their internal memory for a “Directory” which stores a sector map of all chunks of data. For example, if you create a file called README.txt with the word “hello” in it, the computer would randomly store the Unicode for the five characters in the word “hello” on the storage device and make a directory entry something like the following: Understanding the Directory concept and how computers store information is crucial to understanding how VCS mange your Files. 2.9 How VCS Manage Your Files Most users think about version control as a process of managing files. For example, if I might have a directory called “My Project” that holds several files related to this project as follows: One approach to managing changes to the above project files would be to store multiple versions of each file as in the figure below for the file analysis.r: In fact, many VCS do exactly this. They treat each file as the minimum unit of data and simply save various versions of each file along with some additional information about the version. This approach can work reasonably well. However, it has limitations. First, this approach can unnecessarily consume space on the local storage device, especially if you are saving many versions of a very large file. It also has difficulty dealing with changes in filenames, typically treating the same file with a new name as a completely new file, thereby breaking the chain of version history. To combat these issues, good VCS don’t actually manage files at all. They manage Directories. Distributed VCS like Git take this alternate approach to data storage that is Directory, rather than file, based. 2.10 Graph-Based Data Management Git (and many other Distributed VCS) manage your files as collections of data rather than collections of files. Git’s primary unit of management is the “Repository,” or “Repo” for short, which is aligned with your computer’s Directory/Folder structure. Consider, for example, the following file structure: Here we see a user, Tom’s, home directory, which contains three sub directories (Data, Thesis, and Tools) and one file (Notes.txt). Both the Data and Tools directories contain sub files and/or directories. If Tom wanted to track changes to the two files in the Data directory, he would first create a Git repository by placing the Data directory “under version control.” When a repository is created, the Git system writes a collection of hidden files into the Data Directory that it uses to store information about all of the data that lives under that directory. This includes information about the addition, renaming, and deletion of both files and folders as well as information about changes to the data contained in the files themselves. Additions, deletions and versions of files are tracked and stored not as copies of files, but rather as a set of instructions that describes changes made to the underling data and the directory structure that describes them. 2.11 Additional Resources The Git Book is the defintive Git resource and provides an excellent reference for everythign that we will cover in the Interactive session. There is no need to read the book prior to the session, but it’s a good reference resource to have avaialable as you begin to work with Git after the workshop. "],["introduction-to-git.html", "3 Introduction to Git 3.1 Save, Stage, Commit 3.2 Creating Your First Repo 3.3 Checking the Status of a Repo 3.4 Version of a File 3.5 View a History of Your Commits 3.6 Comparing Commits 3.7 Comparing Files 3.8 To View an Earlier Commit 3.9 Undoing Things 3.10 When Things go Wrong!", " 3 Introduction to Git Put some intro text here 3.1 Save, Stage, Commit Git does not automatically preserve versions of every “saved” file. When working with Git, you save files as you always do, but this has no impact on the versions that are preserved in the repository. To create a “versions,” you must first add saved files to a Staging area and then “Commit” your staged files to the repository. The Commits that you make constituted the versions of files that are preserved in the repository. 3.2 Creating Your First Repo Move to your Home directory $ cd ~ note: The $ character represents your command promt. DO NOT type it into your terminal Create a new directory for this workshop $ mkdir introtogit Change to the new directory $ cd introtogit Put the new directory under version control $ git init 3.3 Checking the Status of a Repo To check the status of a repository use the followign command $ git status 3.4 Version of a File In Gitspeak, we ‘commit’ if version of a file to the repository to save a copy of the current working version of a file as a version. This is a multi-step process in which we first ‘stage’ the file to be committed and then ‘commit’ the file. STEP 1: Place the file you want to version into the Staging Area $ git add &lt;filename&gt; Replace in the command above with the actual name of the file you want to version. STEP 2: Commit Staged Files $ git commit -m &#39;A detailed comment explaining the nature of the versio being committed. Do not include any apostrophe&#39;s in your comment.&#39; 3.5 View a History of Your Commits To get a history of commits $ git log To see commit history with patch data (insertions and deletions) for a specified number of commits $ git log -p -2 To see abbreviated stats for the commit history $ git log --stat You can save a copy of your Git log to a text file with the following command: $ git --no-pager log &gt; log.txt 3.6 Comparing Commits $ git diff &lt;commit&gt; &lt;commit&gt; 3.7 Comparing Files $ git diff &lt;commit&gt; &lt;file&gt; or $ git diff &lt;commit&gt;:&lt;file&gt; &lt;commit&gt;:&lt;file&gt; 3.8 To View an Earlier Commit $ git checkout &lt;commit&gt; To solve Detached Head problem either RESET HEAD as described below or just chekout another branch git checkout &lt;branch&gt; To save this older version as a parallel branch execute $ git checkout -b &lt;new_branch_name This will save the older commit as a new branch running parallel to master. 3.9 Undoing Things One of the common undos takes place when you commit too early and possibly forget to add some files, or you mess up your commit message. If you want to redo that commit, make the additional changes you forgot, stage them, and commit again using the –amend option $ git commit --amend To unstage a file for commit use $ git reset HEAD &lt;file&gt; Throwing away changes you’ve made to a file $ git checkout -- &lt;file&gt; Rolling everything back to the last commit $ git reset --hard HEAD Rolling everything back to the next to last commit (The commit before the HEAD commit) $ git reset --hard HEAD^ Rolling everything back tp two commits before the head $ git reset --hard HEAD^2 Rolling everything back to an identified commit using HASH/ID from log $ git reset --hard &lt;commit&gt; 3.10 When Things go Wrong! To reset everything back to an earlier commit and make sure that the HEAD pointer is pointing to the newly reset HEAD, do the following $ git reset --hard &lt;commit&gt; $ git reset --soft HEAD@{1} "],["git-branching.html", "4 Git Branching 4.1 Merging Branches 4.2 Branching Workflows", " 4 Git Branching Branching provides a simple way to maintain multiple, side-by-side versions of the files in a repository. Conceptually, branching a repository creates a copy of the codebase in its current state that you can work on without affecting the primary version from which it was copied. This alows you to work down multiple paths without affecting the main (or other) codebase. To see a list of branches in your repository $ git branch To create a new branch $ git checkout -b hotfix New branches are created of the current working branch. To change branches use $ git checkout &lt;branch name&gt; 4.1 Merging Branches When you merge a branch, git folds any changes that you made to files in an identified branch into the current working branch. It also adds any new files. When you perform a merge, a new commit will be automatically created to track the merge. To merge branches, commit any changes to the branch you want to merge (in this example, the ‘hotfix’ branch) then checkout the branch into which you want to merge (for example, master), and then execute a merge command. $ git commit -m &#39;commiting staged files in hotfix branch&#39; $ git checkout master $ git merge hotfix 4.2 Branching Workflows "],["introduction-to-r.html", "5 Introduction to R 5.1 Learning objectives 5.2 Before We Start 5.3 Mathematical Operations 5.4 HELP! 5.5 Calls 5.6 Variables 5.7 Data Types and Classes 5.8 Vectors 5.9 Matrices, Arrays &amp; Lists 5.10 Data Frames 5.11 Subsetting", " 5 Introduction to R 5.1 Learning objectives After this lecture, you should be able to: define reproducible research and the role of programming languages explain what R and RStudio are, how they relate to eachother, and identify the purpose of the different RStudio panes create and save a script file for later use; use comments to annotate solve simple mathematical operations in R create variables and dataframes inspect the contents of vectors in R and manipulate their content subset and extract values from vectors use the help function 5.2 Before We Start What is R and RStudio? “R” is both a free and open source programming language designed for statistical computing and graphics, and the software for interpreting the code written in the R language. RStudio is an integrative development environment (IDE) within which you can write and execute code, and interact with the R software. It’s an interface for working with the R software that allows you to see your code, plots, variables, etc. all on one screen. This functionality can help you work with R, connect it with other tools, and manage your workspace and projects. You cannot run RStudio without having R installed. While RStudio is a commercial product, the free version is sufficient for most researchers. Why learn R? There are many advantages to working with R. Scientific integrity. Working with a scripting language like R facilitates reproducible research. Having the commands for an analysis captured in code promotes transparency and reproducibility. Someone using your code and data should be able to exactly reproduce your analyses. An increasing number of research journals not only encourage, but are beginning to require, submission of code along with a manuscript. Many data types and sizes. R was designed for statistical computing and thus incorporates many data structures and types to facilitate analyses. It can also connect to local and cloud databases. Graphics. R has buit-in plotting functionalities that allow you to adjust any aspect of your graph to effectively tell the story of your data. Open and cross-platform. Because R is free, open-source software that works across many different operating systems, anyone can inspect the source code, and report and fix bugs. It is supported by a large community of users and developers. Interdisciplinary and extensible. Because anyone can write and share R packages, it provides a framework for integrating approaches across domains, encouraging innovation. Navigating the interface Source is your script. You can save this as a .R file and re-run to reproduce your results. Console - this is where you run the code. You can type directly here, but it won’t save anything entered here when you exit RStudio. Environment/history lists all the objects you have created and the commands you have run. Files/plots/packages/help/viewer pane is useful for locating files on your machine to read into R, inspecting any graphics you create, seeing a list of available packages, and getting help. To interact with R, compose your code in the script and use the commands execute (or run) to send them to the console. (Shortcuts: You can use the shortcut Ctrl + Enter, or Cmd + Return, to run a line of code). Create a script file for today’s lecture and save it to your lecture_4 folder under ist008_2021 in your home directory. (It’s good practice to keep your projects organized., Some suggested sub-folders for a research project might be: data, documents, scripts, and, depending on your needs, other relevant outputs or products such as figures. 5.3 Mathematical Operations R works by the process of “REPL”: Read-Eval-Print Loop: R waits for you to type an expression (a single piece of code) and press Enter. R then reads in your commands and parses them. It reads whether the command is syntactically correct. If so, it will then evaluate the code to compute a result. R then prints the result in the console and loops back around to wait for your next command. You can use R like a calculator to see how it processes commands. Arithmetic in R follows an order of operations (aka PEMDAS): parenthesis, exponents, multiplication and division, addition and subtraction. 7 + 2 7 - 2 244/12 2 * 12 To see the complete order of operations, use the help command: ?Syntax 5.4 HELP! This is just the beginning, and there are lots of resources to help you learn more. R has built-in help files that can be accessed with the ? and help() commands (to get help with arithmetic commands, you must put the symbol in single or double quotes). You can search within the help documentation using the ?? commands. You can view the package documentation using packageDescription(\"Name\"). And, you can always ask the community: Google, Stack Overflow [r], topic-specific mailing lists, and the R-help mailing list. On CRAN, check out the Intro to R Manual and R FAQ. When asking for help, clearly state the problem and provide a reproducible example. R also has a posting guide to help you write questions that are more likely to get a helpful reply. It’s also a good idea to save your sessionInfo() so you can show others how your machine and session was configured. 5.5 Calls R has many functions (reusable commands) built-in that allow you to compute mathematical operations, statistics, and other computing tasks. Code that uses a function is said to call that function. When you call a function, the values that you assign as input are called arguments. Some functions have multiple parameters and can accept multiple arguments. log(10) sqrt(9) sum(5, 4, 1) 5.6 Variables A variable is a name for a stored value. Variables allow you to reuse the result of a computation, write general expressions (such as a*x + b), and break up your code into smaller steps so it’s easier to test and understand. Variable names can contain letters or numbers, but they cannot begin with a number. In general, variable names should be descriptive but concise, and should not use the same name as common (base R) functions, like mean, T, median, sum, etc. x &lt;- 10 y &lt;- 24 fantastic.variable2 = x x&lt;-y/2 In R, variables are copy-on-write. When we change a variable (a “write”), R automatically copies the original value so dependent variables are unchanged until they are re-run. x = 13 y = x x = 16 y 5.7 Data Types and Classes R categorizes data into different types that specify how the object is stored in memory. Some common types are: character (\"marie curie\", \"grace hooper\") complex (3i) double (2, 3, 5.7) integer (2L, 4L) logical (TRUE, FALSE) Types higher in the list are more general than types lower in the list. For instance, we can represent a logical value as a character string (\"TRUE\", \"FALSE\"), but can’t represent an arbitrary character string as a logical value. R will automatically convert objects to more general types as needed (but not to less general types!). Perhaps more useful than types for day-to-day programming is an object’s class, which specifies how it behaves in R. There are classes that correspond to each of the data types above, with the same name (exception: the class for type double is called numeric). Other classes also exist, and an object can have multiple classes. You can check the class of an object with class(): x &lt;- 2 class(x) y &lt;- &quot;two&quot; class(y) class(TRUE) class(mean) R’s types and classes differ from how we categorize data in statistics: continuous (real numbers) discrete (integers, or finite number of values) logical (1 or 0, T or F) nominal (unordered categorical values) ordinal (ordered categorical values) graph (network data) character (text data) 5.8 Vectors A vector is an ordered collection of values. The elements in the vector must have the same data type. (While class and type are independent, for vectors they are typically the same and thus you can expect that they typically should have the same class.) You can combine or concatenate values to create a vector using c(). v&lt;-c(16, 3, 4, 2, 3, 1, 4, 2, 0, 7, 7, 8, 8, 2, 25) class(v) place &lt;- c(&quot;Mandro&quot;, &quot;Cruess&quot;, &quot;ARC&quot;, &quot;CoHo&quot;, &quot;PES&quot;, &quot;Walker&quot;, &quot;ARC&quot;, &quot;Tennis Courts&quot;, &quot;Library&quot;, &quot;Arboretum&quot;, &quot;Arboretum&quot;, &quot;Disneyland&quot;, &quot;West Village&quot;, &quot;iTea&quot;, &quot;MU&quot;) class(place) What happens if you make a typo or try to combine different data types in the same vector? R resolves this for you and automatically converts elements within the vector to be the same data type. It does so through implicit coercion where it conserves the most information possible (logical -&gt; integer -&gt; numeric -&gt; complex -&gt; character). Sometimes this is very helpful, and sometimes it isn’t. 5.8.1 Basic Statistics on Vectors You can use functions built into R to inspect a vector and calculate basic statistics. length(v) # returns how many elements are within the object length(place) min(v) # minimum value max(v) # maximum value mean(v) median(v) sd(v) # standard deviation 5.9 Matrices, Arrays &amp; Lists Matrices are two-dimensional containers for values. All elements within a matrix must have the same data type. Arrays generalize vectors and matrices to higher dimensions. In contrast, lists are containers for elements with different data types. 5.10 Data Frames We frequently work with 2-dimensional tables of data. For a tabular data set, typically each row corresponds to a single subject and is called an observation. Each column corresponds to the data measures or responses – a feature or covariable. (Sometimes people will also refer to these as variables, but that can be confusing as “variable” means something else in R, so here we’ll try to avoid that term.) R’s structure for tabular data is the data frame. A data frame is a list of column vectors. Thus, elements of a column must all have the same type (like a vector), but elements of a row can have different types (like a list). Additionally, every row must be the same length. To make a data frame in R, you can combine vectors using the data.frame() command. distance.mi &lt;- c(3.1, 0.6, 0.8, 0.2, 0.5, 0.2, 0.7, 0.5, 0, 1.2, 1.2, 501, 1.6, 0.4, 4.7) time.min &lt;- v major &lt;- c(&quot;nutrition&quot;, &quot;psychology&quot;, &quot;global disease&quot;, &quot;political science&quot;, &quot;sociology&quot;, &quot;sustainable agriculture&quot;, &quot;economics&quot;, &quot;political science&quot;, &quot;undeclared&quot;, &quot;psychology&quot;, &quot;undeclared&quot;,&quot;economics&quot;,&quot;political science&quot;, &quot;english&quot;, &quot;economics&quot;) my.data &lt;- data.frame(place, distance.mi, time.min, major) 5.10.1 Inspecting Data Frames You can print a small dataset, but it can be slow and hard to read especially if there are a lot of coumns. R has many other functions to inspect objects: head(my.data) tail(my.data) nrow(my.data) ncol(my.data) ls(my.data) rownames(my.data) str(my.data) summary(my.data) 5.11 Subsetting Sometimes you will want to work with only specific elements in a vector or data frame. To do that, you can refer to the position of the element, which is also also called the index. length(time.min) time.min[15] You can also subset by using the name of an element in a list. The $ operator extracts a named element from a list, and is useful for extracting the columns from data frames. How can we use subsetting to look only at the distance response? my.data$distance.mi my.data[,2] distances2&lt;-my.data[[&quot;distance.mi&quot;]] distances3&lt;-my.data[[2]] What are the responses for political science majors? polisci_majors &lt;- my.data[which(my.data$major == &#39;political science&#39;), ] View(polisci_majors) which(my.data$major == &quot;political science&quot;) shortframe&lt;-my.data[c(4,8,13),] What are the majors of the first 5 students who replied? shortframe2 &lt;- my.data[1:5,&quot;major&quot;] # range for rows, columns You can also use $ to create an element within the data frame. my.data$mpm &lt;- my.data$distance.mi / my.data$time.min Factors* are the class that R uses to represent categorical data. Levels are categories of a factor. levels(my.data$major) "],["control-structures.html", "6 Control Structures 6.1 If Statement 6.2 Relationship Operators 6.3 If Else Statement 6.4 ifelse Statement 6.5 The switch Statement 6.6 The which Statement", " 6 Control Structures Control Structures are functions in computer programming the evaluate conditions (like, for example, the value of a variable) and change the way code behaves based upon evaluated values. For example, you might to perform one function if the value stored in the variable x is greater than 5 and a different function if it is less than less than 5. The Wikiversit Control Structures page contains a good, general description of control structures that is not programming language specific. The information that follows provides examples of the most frequetly used R control structures and how to implement them. For more complete documentation on control strcutures in R run the following help command: ?Control 6.1 If Statement The “If Statement” is the most basic of the R control structures. It tests whether a particular condition is true. For example, the below statement tests whether the value of the variable x is greater than 5. If it is, the code prints the phrase “Yay!” to screen. If it is not, the code does nothing: x &lt;- 7 if (x &gt; 5) { print(&quot;Yay!&quot;) } Note, the general syntax in the example is: control_statement (condition) { #code to execute condition is true } While you will occasionally see variations in how control structures are present, this is a fairly universal syntax across computer programming languages. The specific control structure being invoked is followed by the condition to be tested. Any actions to be performed if the condition evaluates to TRUE are place between curly brackets {} following the condition. 6.2 Relationship Operators The most common conditions evaluate whether one value is equal to ( x == y), equal to or greater than (x =&gt; y), equal to or lesser than (x &lt;= y), greater than (x &gt; y), or lesser than (x &lt; y) another value. Another common task is to test whether a BOOLEAN value is TRUE or FALSE. The syntax for this evaluation is: if (*x*) { #do something} Control structures in R also have a negation symbol which allows you to specify a negative condition. For example, the conditional statement in the following code evaluates to TRUE (meaning any code placed between the curly brackets will be executed) if the x IS NOT EQUAL to 5: if (x !=5) { #do something} 6.3 If Else Statement The “If Else” statement is similar to the “If Statement,” but it allows you specify one code path to execute if the conditional evaluates to TRUE and another to execute if the conditional evaluates to FALSE: x &lt;- 7 if (x &gt; 5) { print(&quot;Yay!&quot;) } else { print(&quot;Boo!&quot;) } 6.4 ifelse Statement R also offers a combined if/else syntax for quick execution of small code chunks: x &lt;- 12 ifelse(x &lt;= 10, &quot;x less than 10&quot;, &quot;x greater than 10&quot;) 6.5 The switch Statement The switch statement provides a mechanism for selecting between multiple possible conditions. For example, the following code returns one of several possible values from a list based upon the value of a variable: x &lt;- 3 switch(x,&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;) Note: if you pass switch a value that exceeds the number of elements in the list R will not compute a reply. 6.6 The which Statement The which statement is not a true conditional statement, but it provides a very useful way to test the values of a dataset and tell you which elements match a particular condition. In the example below, we load the R IRIS dataset and find out which rows have a Petal.Length greater than 1.4: data(&quot;iris&quot;) rows &lt;- which(iris$Petal.Length &gt; 1.4) note: you can see all of the R. build in datasets with the data() command. "],["iterating-loops.html", "7 Iterating (Loops) 7.1 For i in x Loops 7.2 While Loops 7.3 Repeat Loops 7.4 Break and Next 7.5 Iterating Data.Frame Rows in R 7.6 lapply()", " 7 Iterating (Loops) In computer programming iteration is a specific type of control structure that repeatedly runs a specified operation either for a set numbe of iterations or untul some condition is met. For example, you might want your code to peform the same math operation on all of the numbers stored in a vector of values; or perhaps you want the computer to look through a list until it finds the first entry with a value greater than 10; or, maybe you just want the computer to sound an alarm exactly 5 times. Each of these is a type of iteration or “Loop” as they are also commonly called. 7.1 For i in x Loops The most common type of loop is the “For i in x” loop which interates through each value (i) in a list (x) and does something with each value. For example, assume that x is a vector containing the following four names names: Sue, John, Heather, George, and that we want to print each of these names to screen. We can do so with the followig code: x &lt;- c(&quot;Sue&quot;, &quot;John&quot;, &quot;Heather&quot;, &quot;George&quot;) for (i in x) { print(i) } In the first line of code, we create our vecctor of names (x). Next we begin our “For i in x loop,” which has the following general syntax, which is similar to that of the conditional statements you’ve already mastered: for (condition) {} Beginning with the first element of the vector x, which in our case is “Sue,” for each iteration of the for loop the value of the corresponding element in x is assiged to the variable i and then i can be acted upon in the code icnluded between the curly brackets of the function call. In our case we simply tell the conputer to print the value of i to the sreen. Witgh each iteration, the next value in our vector is assigned to i and is subsequently printed to screen, resulting in the following output: [1] &quot;Sue&quot; [1] &quot;John&quot; [1] &quot;Heather&quot; [1] &quot;George&quot; In addition to acting on vectors or lists, For loops can also be coded to simply execute a chunk of code a designated number of times. For example, the following code will print “Hello World!” to screen exactly 10 times: for (i in 1:10) { print(&quot;Hello World!&quot; } 7.2 While Loops Unlike For loops, which iterate a defined number of times based on the length of a list of range of values provided in the method declaration, While loops continue to iterate infinitely as long as (while) a defined condition is met. For example, assume you have a boolean variable x the value of which is TRUE. You might want to write code that performs some function repeatly until the value of x is switched to FALSE. A good example of this is a case where your program asks the user to enter data, which can then be evaluated for correctness before the you allow the program to move on in its execution. In the example below, we ask the user to tell us the secret of the universe. If the user answeres with the correct answer (42), the code moves on. But if the user provides and incorrect answer, the code iterates back to the beginning of the loop and asks for input again. response &lt;- 0 while (response!=42) { response &lt;- as.integer(readline(prompt=&quot;What is the answer to the Ultimate Question of Life, the Universe, and Everything? &quot;)); } 7.3 Repeat Loops Like While loops, Repeat loops continue to iterate until a specified condition is met; but with Repeat loops that condition is defined not as an argument to the function but is a specific call to “break” that appears in the functions executable code. In the example below we assign the value 1 to a variable i and then loop through code that prints and then iterates the value of i until it reaches 10, at which time we forceably exit the loop: i &lt;- 1 repeat { print(i) i = i+1 if (i &gt; 10){ break } } 7.4 Break and Next In the previous section we saw the use of the break statement to force an exit from a repeat loop based on a conditional evaluation in an if statement. Break can actually be used inside any conditional (for, while, repeat) in order to force the end of iteration. This can be useful in a variety of contexts where you want to test for multiple conditions as a means of stopping iteration. The next command is similar to break in that it can be used inside any iteration structure to force R to skip execution of the iteration code for particular cases only. For example, we use next below to iterate through the nunbers 1 to 10 and print all values to screen EXCEPT the value 5: for (i in 1:10) { if (i == 5){ next } print(i) } 7.5 Iterating Data.Frame Rows in R In the section on for loops above, we learned that you can easily iterate across all values of a list using a “for i in x” loop. Working with R data.frames adds a bit of complexity to this process. Because R was developed as a language for statistial analysis, which always involves the comparison of multiple observations of the same variable (for example, all of the weights recroded across all patients), the default behavior of the “for i in x” loop when applied to data.frames is to iterate across columns (variables) rather than rows (observations). Consider the following example: for (i in iris) { print(i) } If you run the above code, in the first iteration R will assign the vector of values contained in the firt column (Sepal.Length) to i, in the second iteration it will assign vectore of values contained in the second column (Sepal.Width) to i, etc. Iterating through the data columns of a data.frame is useful for many (if not most) operations. However, there are time when we want to iterate through data one observation at a time. To accomplish this, we nee do specifically direct R to move through the data.frame by row, as follows: for (i in 1:nrow(iris)) { thisrow &lt;- iris[i,] print(thisrow) } 7.6 lapply() R has a built-in class of functions known as the apply family that provide a shorthand for iterating through collections of data. These behave like a for loop, but require much less actual code to accomplish. The lapply function iterates across lists, such as vectors. When you invoke lapply it applies a defined operation to each item in the subitted list and returns a list of equal length that contains the results of this calculation. In the code below, we assign the values 1 through 10 to a vector and then use lapply to subtract 1 from each item in the vector and finally print the results to screen: v &lt;- c(1:10) results &lt;- lapply(v, function(x) (x-1)) print(results) We could accomplish the exact same thing with the following for loop v &lt;- c(1:10) for (i in v) { x &lt;- i - 1 print(x) } The basic syntax of lapply is: lapply(list, function) where “list” is some list object supplied and “function” is pre-defined chunk of code that will be exectuted. You’ll learn more about functions in a future lesson. "],["packages-and-functions.html", "8 Packages and Functions 8.1 Learning objectives 8.2 What is a function? 8.3 What is the basic syntax of a function in R? 8.4 Step 1: Building a function 8.5 Step 2: Calling the function 8.6 A function can have more than one argument 8.7 Using a package and function to graph data and export a .png 8.8 Saving functions and calling them from another file", " 8 Packages and Functions 8.1 Learning objectives After this lecture, you should be able to: explain what a function is read and understand the basic syntax of a function in R use this syntax to call a function use this syntax to build your own function test your function install packages in R load libraries in R 8.2 What is a function? Why build code several or a hundred times when you can build it once and then call and run it as many times as you want? The answer is, don’t! A function allows you to perform an action multiple times in R by calling it and applying it in similar contexts. For instance, if you build a function that checks the class of all vectors in a dataframe, you can name this function and then apply it to do the same operation with any other dataframe. Or, if you build a function that graphs the correlation between two numeric vectors and exports this graph to a .png file, you can call this same function and apply it to two other vectors, again and again as needed. Functions can greatly increase the efficiency of your programming, and allow you to create flexible and customized solutions. 8.3 What is the basic syntax of a function in R? The basic syntax of a function in R, or the way it should be written so that R recognizes it and applies it do perform actions, is usually stated as follows: function_name &lt;- function(argument_1, argument_2, ...) { Function body } What this does not demonstrate is that there are actually two steps to a function: building it, and applying it. We will look at both steps in the following code from DataCamp: 8.4 Step 1: Building a function myFirstFun&lt;-function(n) { # Compute the square of integer `n` n*n } The code chunk builds the function, setting “myFirstFun” as the name, or variable, to which they have assigned the function. The function itself runs from the word “function” down through the closing curly brace. What is an argument? In the above example, “(n)” is the argument. R looks for this argument (in this case, “n”) in the body of the function, which in this case is n*n. When we run the above script, the function is saved as an object into the global environment so that it can be called elsewhere, as demonstrated in the code chunks below. The function has no effect unless you apply it. Until that happens, the function will do nothing but wait to be called. 8.5 Step 2: Calling the function The code chunk below calls “myFirstFun(n)” and tells R to assign the results of the operation the function performs (n*n) to the variable “u.” But if we run this code as it is (with “n” in the parentheses), we will get an error (unless we have previously assigned “n” as a variable with a value that will accept the operation to be performed — so “n” needs to be a number in this case so that it can be multiplied). We do not actually want to perform the function on the letter “n” but rather, on a number that we will insert in the place of “n.” We can apply this function by setting “n” as a number, such as 2, in the example below. # Call the function with argument `n` u &lt;- myFirstFun(2) # Call `u` u Once we have changed “n” to a number, R then performs this operation and saves the result to a new variable “u.” We can then ask R to tell us what “u” is, and R returns or prints the results of the function, which in this case, is the number 4 (2*2). The image below shows the results we get if we attempt to run the function without changing the argument “n” to a number (giving us an error), and the results when we change “n” to the number “2” which assigns the result of the function (4) to “u,” or the number “3” which assigns the result of the function (now 9) to “u.” It is important to understand that “n” is an argument of the function “myFirstFun.” R does not consider “n” a variable, but it acts like a variable because it can change as you call the function into different contexts. To R, “u” and “myFirstFun” are variables because they are names to which values and other content are assigned. Here is another example of a function with one argument: Step 1: Build the function In the code below, we will build a function that checks the classes of all vectors in a dataframe. #build function with one argument (variable) check_class &lt;- function(data) { lapply(data, class) } Step 2: Call the function in one or more contexts. In the code below, we will call the function we built above and apply it to two different datasets. Just as we saw in the example above where we inserted the numbers 2 or 3 in place of “n,” we will insert the name of the datasets we want to use in place of the word “data” to call the new function we have built. Note: you will need to load the built-in R datasets “mtcars” and “iris” in order to test the code below.* #run check_class function on two different dataframes check_class(mtcars) check_class(iris) 8.6 A function can have more than one argument A function works similarly when it has two or more arguments. Let’s say we only want to look at the first vector or column in the dataframe “mtcars.” We would write a line of code that looks like this: #pull the values of the first column /vector in the dataframe &quot;mtcars&quot; mtcars[1] But if we wanted to create a function that looks at any column/vector in any dataframe, we could write a function that looks like this: #build function with two arguments (variable) one_column &lt;- function(data, x) { data[x] } Note: if we want to tell a user what kind of input we want to include, we could instead do something like function(dataset, column_position) or function(dataset, column_name). Once we have run the above function (telling R to save it to the global environment), we would then call this new function, which we have named one_column, and apply it to various dataframes, and telling R which column or vector in each dataframe we want to view. #run one_column function on two different dataframes one_column(mtcars, 1) one_column(iris, 2) #Packages A package is a set of functions that other users and developers have made that allow R users to perform various operations. As with many applications and software, some R packages are well crafted, documented, and updated frequently, while others are not. You will want to use your best judgment and choose packages that you think will help you in your work, but will remain stable and functional. Try adding the packages below: dplyr wakefield rlang Go to Tools &gt; Install Packages in RStudio, search for the functions, and then follow the steps to install them. Once you have installed them, you will then need to load the libraries into your R environment by using the following code: #load libraries library(dplyr) library(wakefield) library(rlang) Click here to find out more about dplyr Click here to find out more about wakefield Click here to find out more about rlang If you have installed the above packages and loaded their libraries, you can then create a function that uses the table you made in the earlier session, “Introduction to R,” to add five rows of data, add a logical vector with randomly assigned logical values, and save this as a new table. Your function might look something like this code below. The comment tags indicate what each line of the function will do. Note: you will need to load in your data for my.table with the initial 15 rows before proceeding with the next steps. #Step 1: Build a function that adds a logical vector with randomly assigned TRUE/FALSE values make_logical.vec &lt;- function(dataset, new.col) { #make logical vector with random values vector_1 &lt;- r_sample_logical(15, prob = NULL, name = &quot;new.vector&quot;) %&gt;% as.logical() #tell R to read input for the name of new.col so that we can assign this name to the vector/column colName = quo_name(new.col) #add our new vector, with the name we have specified, to the dataset dataset %&gt;% mutate(!!quo_name(colName) := vector_1) #create new } #Step 2: Call our new function ‘make_logical.vec’ and assign the results to the table ‘my.data.’ my.data &lt;- make_logical.vec(my.data, &quot;logical.vec&quot;) Note: If we call the function, setting the dataset to ‘my.data’ and the name of the new vector to ‘logical.vec,’ it will create the dataframe but will only print it for us in our console. If we want to actually save the new dataframe to update our existing dataframe, we need to reassign it to ‘my.data,’ so that the updated dataframe replaces the original dataframe. As we can see in the code above, a function can contain more than one variable, and can include several or many lines of code and perform many operations. The above example demonstrates this, and also shows that packages such as as the ones we have loaded here, while optional for working in R, can allow you to call many useful functions. 8.7 Using a package and function to graph data and export a .png We can install and load the ‘ggplot’ or ‘ggforce’ package to graph data from a dataframe and export the graph to a file. For example, below we can build a function that graphs the data from two columns/vectors, and then generates a .png file. We start by loading the package that contains the plotting functions we want to use: Note: if you have not installed ggforce already, you will want to do that now. #graph distance and time from our.data library(ggforce) Next, we could build a function that looks like this: # write function that graphs two variables from dataset graph_data &lt;- function(data, column1, column2, n) { distances &lt;- (data %&gt;% filter(column1 &lt;= &quot;n&quot;)) %&gt;% ggplot(aes(column2, column1)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) png(&quot;graph.png&quot;) print(graph) dev.off() } Lastly, we can call the above function, and apply it to the dataframe, my.table, to compare distance and time of travel. graph_data(my.data, my.data$distance.mi, my.data$time.min, 14.0) The above function generates a .png that looks like this: 8.8 Saving functions and calling them from another file You can save the functions you build to a separate file, and then load these as a source. For example, I might save my functions to an R script, called “functions.r.” I can then load these sources along with my packages into my R environment. Note: Although we loaded libraries as we went through this lesson, the best practice is to run your packages and source files at the very beginning of your new R script, as shown in the example that follows. library(dplyr) library(wakefield) library(rlang) library(ggforce) source(&quot;functions.r&quot;) The above code will allow you to call functions that are saved in these libraries and in the functions.r file. "],["file-input-and-output.html", "9 File Input and Output 9.1 Objectives 9.2 Basic Idea 9.3 File Formats 9.4 Filesystems and Paths 9.5 get and set working directory 9.6 saving and loading R data 9.7 Reading and Writing Text data 9.8 URLS as files", " 9 File Input and Output This lesson will cover some standard functions for reading and writing data in R. 9.1 Objectives getting and setting working directory save and load R objects to/from disk read and write tabular data read data from a url 9.2 Basic Idea As a data scientist, you will constantly be reading from and writing to files. Generally you are given some dataset that you need to analyze and report on. This means that you need to load the data into R, run some code, and finally save some outputs. This is all done with files. 9.3 File Formats When people talk about binary files, vs text files what they really mean is - is it human readable? A text file should have text data that a human can read with a text editor. A binary file has binary data that a human can’t really read, but the appropriate software can. 9.4 Filesystems and Paths At a high level, files are information stored on a computer. Each file has a name, and a unique file path. A filepath is the location of the file in the storage device. A file name as two parts: the name and the file extension. The file extension (everything after the .) is meant to indicate to the user (you) and the operating system what the file contains. This hint means you often times don’t need to open the file to know what kind of data it has. However, the extension is not enforced by anything, its just a useful suggestion. Paths can be relative or absolute. An absolute path is the full path through the filesystem to reach a file. A relative path is the path from some starting point to the file you want to reach. You can consider a relative path as something that needs to be combined with another path to reach a file. 9.5 get and set working directory Before we begin, we will get and set our working directory in R. You can think of the working directory as the part that gets combined with the relative path. getwd() will return the absolute file path of the working directory getwd() Call setwd to set the working directory to a path you specify as an input argument. setwd(&quot;~/Documents/file_io/&quot;) # notice the argument getwd() A really useful function in R is list.files(), which lists all the files at a given path. Listing the files should confirm for us that we are in the right place. list.files() 9.6 saving and loading R data 9.6.1 rds A simple way to save an R object directly to a file, such that it can be loaded into another R session is with the saveRDS function. saveRDS will write a single object to a specified file path. By default, it will save the object as a binary representation. This can be very useful for large objects, as the binary format will be significantly more space efficient. y = c(0,1,2,3,4) saveRDS(y, file=&quot;myvectors.rds&quot;) Confirm that it worked. list.files() The counterpart to saveRDS is readRDS. With readRDS, you can load in an rds file, which by definition contains a single R object, and assign it to a variable in your session. x = readRDS(&quot;myvector.rds&quot;) This will work for any R object. For example. saveRDS(mtcars, file=&quot;mtcars.rds&quot;) my_cars = readRDS(&quot;mtcars.rds&quot;) Saving and loading using readRDS is really powerful to save data. However, it does have a pretty significant drawback - its useless outside of R. For someone to explore the data, they would need to load R. 9.7 Reading and Writing Text data In addition to saveRDS and readRDS, R has functions for working with text files. These are commonly used for getting external data into R. And for exporting your data so that it can be used by other people. 9.7.1 tabular data Generally the data you work with in R will be tabular. Dataframes are an example of tabular data. 9.7.2 read and write table To write tabular data from a text file use the write.table function. Before running it, lets look at the documentation and understand the key arguments. ?write.table The important arguments are x, file, and sep. x is the dataframe you are saving. file is the name of the file you want to create and write to. sep is the field separator, also called delimiter. Notice that if file is left blank, then R will just print the results to the console, instead of into a file. Lets use this to explore what the sep argument does. small = head(my_cars) write.table(small) write.table(small, sep=&quot; &quot;) write.table(small, sep=&quot;.&quot;) write.table(small, sep=&quot;,&quot;) Lets write our data to a file. write.table(my_cars, file=&quot;cars.txt&quot;) list.files() # confirm it worked Now lets read the data back in. from_cars.txt = read.table(&quot;cars.txt&quot;) Always inspect your data to make sure everything worked colnames(from_cars.txt) dim(from_cars.txt) head(from_cars.txt) 9.7.3 CSV format A CSV (comma separated values) file is a text file that uses ‘,’ as the field separator. This is probably the most commonly used format for plain text tabular data. To write a csv in R use the write.csv function. This is equivalent to write.table(from_cars.txt, file=\"cars.csv\", sep=\",\") write.csv(from_cars.txt, file=&quot;cars.csv&quot;) from_cars.csv = read.csv(&quot;cars.csv&quot;) Again, double check that everything worked. head(from_cars.csv) colnames(from_cars.csv) dim(from_cars.csv) What went wrong here? In this case it was ambiguous if the first column was rownames or actual values. Lets fix it temp = from_cars.csv[, 2:12] rownames(temp) = from_cars.csv[,1] fixed = temp With these sorts of problems, you can generally fix them by using the appropriate arguments to the function calls of the read and write functions. Notice the argument in this function call. It specifies that the rownames can be read in from the first column of the tabular data in the file. from_cars.csv = read.csv(&quot;cars.csv&quot;, row.names =1) 9.7.4 Non tabular data There are functions in R for reading and writing text data that doesn’t represent tabular data. A common one is writeLines and readLines. texts = c(&quot;line one&quot;, &quot;line two&quot;) writeLines(texts, &quot;raw.txt&quot;) texts2 = readLines(&quot;raw.txt&quot;) 9.8 URLS as files Files can be transferred over the internet. URLs are a type of filepath, that denotes a filepath, and the computer that file is stored on. Many functions in R that involve reading and writing from files, can be given a url as the filepath argument. In that case, the file will be transferred over the internet, onto your computer, and then read into R. Here is an example of reading in a file from a url, using the readLines function. url = &quot;https://datalab.ucdavis.edu&quot; t = readLines(url) "],["strings-and-regular-expressions.html", "10 Strings and Regular Expressions 10.1 Printing Output 10.2 Escape Sequences 10.3 Character Encodings 10.4 The Tidyverse 10.5 The stringr Package 10.6 Regular Expressions", " 10 Strings and Regular Expressions After this lesson, you should be able to: Print strings with cat Read and write escape sequences and raw strings With the stringr package: Split strings on a pattern Replace parts of a string that match a pattern Extract parts of a string that match a pattern Read and write regular expressions, including: Anchors ^ and $ Character classes [] Quantifiers ?, *, and + Groups () 10.1 Printing Output The cat function prints a string in the R console. If you pass multiple arguments, they will be concatenated: cat(&quot;Hello&quot;) ## Hello cat(&quot;Hello&quot;, &quot;Nick&quot;) ## Hello Nick Pitfall 1: Printing a string is different from returning a string. The cat function only prints (and always returns NULL). For example: f = function() { cat(&quot;Hello&quot;) } x = f() ## Hello x ## NULL If you just want to concatenate some strings (but not necessarily print them), use paste instead of cat. The paste function returns a string. The str_c function in stringr (a package we’ll learn about later in this lesson) can also concatenate strings. Pitfall 2: Remember to print strings with the cat function, not the print function. The print function prints R’s representation of an object, the same as if you had entered the object in the console without calling print. For instance, print prints quotes around strings, whereas cat does not: print(&quot;Hello&quot;) ## [1] &quot;Hello&quot; cat(&quot;Hello&quot;) ## Hello 10.2 Escape Sequences In a string, an escape sequence or escape code consists of a backslash followed by one or more characters. Escape sequences make it possible to: Write quotes or backslashes within a string Write characters that don’t appear on your keyboard (for example, characters in a foreign language) For example, the escape sequence \\n corresponds to the newline character. Notice that the cat function translates \\n into a literal new line, whereas the print function doesn’t: x = &quot;Hello\\nNick&quot; cat(x) ## Hello ## Nick print(x) ## [1] &quot;Hello\\nNick&quot; As another example, suppose we want to put a literal quote in a string. We can either enclose the string in the other kind of quotes, or escape the quotes in the string: x = &#39;She said, &quot;Hi&quot;&#39; cat(x) ## She said, &quot;Hi&quot; y = &quot;She said, \\&quot;Hi\\&quot;&quot; cat(y) ## She said, &quot;Hi&quot; Since escape sequences begin with backslash, we also need to use an escape sequence to write a literal backslash. The escape sequence for a literal backslash is two backslashes: x = &quot;\\\\&quot; cat(x) ## \\ There’s a complete list of escape sequences for R in the ?Quotes help file. Other programming languages also use escape sequences, and many of them are the same as in R. 10.2.1 Raw Strings A raw string is a string where escape sequences are turned off. Raw strings are especially useful for writing regular expressions, which we’ll do later in this lesson. Raw strings begin with r\" and an opening delimiter (, [, or {. Raw strings end with a matching closing delimiter and quote. For example: x = r&quot;(quotes &quot; and backslashes \\)&quot; cat(x) ## quotes &quot; and backslashes \\ Raw strings were added to R in version 4.0 (April 2020), and won’t work correctly in older versions. 10.3 Character Encodings Computers store data as numbers. In order to store text on a computer, we have to agree on a character encoding, a system for mapping characters to numbers. For example, in ASCII, one of the most popular encodings in the United States, the character a maps to the number 97. Many different character encodings exist, and sharing text used to be an inconvenient process of asking or trying to guess the correct encoding. This was so inconvenient that in the 1980s, software engineers around the world united to create the Unicode standard. Unicode includes symbols for nearly all languages in use today, as well as emoji and many ancient languages (such as Egyptian hieroglyphs). Unicode maps characters to numbers, but unlike a character encoding, it doesn’t dictate how those numbers should be mapped to bytes (sequences of ones and zeroes). As a result, there are several different character encodings that support and are synonymous with Unicode. The most popular of these is UTF-8. In R, we can write Unicode characters with the escape sequence \\U followed by the number for the character in base 16. For instance, the number for a in Unicode is 97 (the same as in ASCII). In base 16, 97 is 61. So we can write an a as: x = &quot;\\U61&quot; # or &quot;\\u61&quot; x ## [1] &quot;a&quot; Unicode escape sequences are usually only used for characters that are not easy to type. For example, the cat emoji is number 1f408 (in base 16) in Unicode. So the string \"\\U1f408\" is the cat emoji. Note that being able to see printed Unicode characters also depends on whether the font your computer is using has a glyph (image representation) for that character. Many fonts are limited to a small number of languages. The NerdFont project patches fonts commonly used for programming so that they have better Unicode coverage. Using a font with good Unicode coverage is not essential, but it’s convenient if you expect to work with many different natural languages or love using emoji. 10.3.0.1 Character Encodings in Text Files Most of the time, R will handle character encodings for you automatically. However, if you ever read or write a text file (including CSV and other formats) and the text looks like gibberish, it might be an encoding problem. This is especially true on Windows, the only modern operating system that does not (yet) use UTF-8 as the default encoding. Encoding problems when reading a file can usually be fixed by passing the encoding to the function doing the reading. For instance, the code to read a UTF-8 encoded CSV file on Windows is: read.csv(&quot;my_data.csv&quot;, fileEncoding = &quot;UTF-8&quot;) Other reader functions may use a different parameter to set the encoding, so always check the documentation. On computers where the native language is not set to English, it can also help to set R’s native language to English with Sys.setlocale(locale = \"English\"). Encoding problems when writing a file are slightly more complicated to fix. See this blog post for thorough explanation. 10.4 The Tidyverse The Tidyverse is a popular collection of packages for doing data science in R. The packages are made by many of the same people that make RStudio. They provide alternatives to R’s built-in tools for: Manipulating strings (package stringr) Making visualizations (package ggplot2) Reading files (package readr) Manipulating data frames (packages dplyr, tidyr, tibble) And more Think of the Tidyverse as a different dialect of R. Sometimes the syntax is different, and sometimes ideas are easier or harder to express concisely. Whether to use base R or the Tidyverse is mostly subjective. As a result, the Tidyverse is somewhat polarizing in the R community. It’s useful to be literate in both, since both are popular. One advantage of the Tidyverse is that the packages are usually well-documented. For example, there are documentation websites and cheat sheets for most Tidyverse packages. 10.5 The stringr Package The rest of this lesson uses stringr, the Tidyverse package for string processing. R also has built-in functions for string processing. The main advantage of stringr is that all of the functions use a common set of parameters, so they’re easier to learn and remember. The first time you use stringr, you’ll have to install it with install.packages (the same as any other package). Then you can load the package with the library function: # install.packages(&quot;stringr&quot;) library(stringr) The typical syntax of a stringr function is: str_NAME(string, pattern, ...) Where: NAME describes what the function does string is the string to search within or transform pattern is the pattern to search for ... is additional, function-specific arguments For example, the str_detect function detects whether the pattern appears within the string: str_detect(&quot;hello&quot;, &quot;el&quot;) ## [1] TRUE str_detect(&quot;hello&quot;, &quot;ol&quot;) ## [1] FALSE Most of the stringr functions are vectorized: str_detect(c(&quot;hello&quot;, &quot;goodbye&quot;, &quot;lo&quot;), &quot;lo&quot;) ## [1] TRUE FALSE TRUE There are a lot of stringr functions. The remainder of this lesson focuses on three that are especially important, as well as some of their variants: str_split_fixed str_replace str_match You can find a complete list of stringr functions with examples in the documentation or cheat sheet. 10.5.1 Splitting Strings The str_split function splits the string at each position that matches the pattern. The characters that match are thrown away. For example, suppose we want to split a sentence into words. Since there’s a space between each word, we can use a space as the pattern: x = &quot;The students in this class are great!&quot; result = str_split(x, &quot; &quot;) result ## [[1]] ## [1] &quot;The&quot; &quot;students&quot; &quot;in&quot; &quot;this&quot; &quot;class&quot; &quot;are&quot; &quot;great!&quot; The str_split function always returns a list with one element for each input string. Here the list only has one element because x only has one element. We can get the first element with: result[[1]] ## [1] &quot;The&quot; &quot;students&quot; &quot;in&quot; &quot;this&quot; &quot;class&quot; &quot;are&quot; &quot;great!&quot; We have to use the double square bracket [[ operator here because x is a list (for a vector, we could use the single square bracket operator instead). Notice that in the printout for result, R gives us a hint that we should use [[ by printing [[1]]. To see why the function returns a list, consider what happens if we try to split two different sentences at once: x = c(x, &quot;Are you listening?&quot;) result = str_split(x, &quot; &quot;) result[[1]] ## [1] &quot;The&quot; &quot;students&quot; &quot;in&quot; &quot;this&quot; &quot;class&quot; &quot;are&quot; &quot;great!&quot; result[[2]] ## [1] &quot;Are&quot; &quot;you&quot; &quot;listening?&quot; Each sentence has a different number of words, so the vectors in the result have different lengths. So a list is the only way to store both. The str_split_fixed function is almost the same as str_split, but takes a third argument for the maximum number of splits to make. Because the number of splits is fixed, the function can return the result in a matrix instead of a list. For example: str_split_fixed(x, &quot; &quot;, 3) ## [,1] [,2] [,3] ## [1,] &quot;The&quot; &quot;students&quot; &quot;in this class are great!&quot; ## [2,] &quot;Are&quot; &quot;you&quot; &quot;listening?&quot; The str_split_fixed function is often more convenient than str_split because the nth piece of each input string is just the nth column of the result. For example, suppose we want to get the area code from some phone numbers: phones = c(&quot;717-555-3421&quot;, &quot;629-555-8902&quot;, &quot;903-555-6781&quot;) result = str_split_fixed(phones, &quot;-&quot;, 3) result[, 1] ## [1] &quot;717&quot; &quot;629&quot; &quot;903&quot; 10.5.2 Replacing Parts of Strings The str_replace function replaces the pattern the first time it appears in the string. The replacement goes in the third argument. For instance, suppose we want to change the word \"dog\" to \"cat\": x = c(&quot;dogs are great, dogs are fun&quot;, &quot;dogs are fluffy&quot;) str_replace(x, &quot;dog&quot;, &quot;cat&quot;) ## [1] &quot;cats are great, dogs are fun&quot; &quot;cats are fluffy&quot; The str_replace_all function replaces the pattern every time it appears in the string: str_replace_all(x, &quot;dog&quot;, &quot;cat&quot;) ## [1] &quot;cats are great, cats are fun&quot; &quot;cats are fluffy&quot; We can also use the str_replace and str_replace_all functions to delete part of a string by setting the replacement to the empty string \"\". For example, suppose we want to delete the comma: str_replace(x, &quot;,&quot;, &quot;&quot;) ## [1] &quot;dogs are great dogs are fun&quot; &quot;dogs are fluffy&quot; In general, stringr functions with the _all suffix affect all matches. Functions without _all only affect the first match. We’ll learn about str_match at the end of the next section. 10.6 Regular Expressions The stringr functions (including the ones we just learned) use a special language called regular expressions or regex for the pattern. The regular expressions language is also used in many other programming languages besides R. A regular expression can describe a complicated pattern in just a few characters, because some characters, called metacharacters, have special meanings. Letters and numbers are never metacharacters. They’re always literal. Here are a few examples of metacharacters (we’ll look at examples in the subsequent sections): Metacharacter Meaning . any single character (wildcard) \\ escape character (in both R and regex) ^ beginning of string $ end of string [ab] 'a' or 'b' [^ab] any character except 'a' or 'b' ? previous character appears 0 or 1 times * previous character appears 0 or more times + previous character appears 1 or more times () make a group More metacharacters are listed on the stringr cheatsheet, or in ?regex. The str_view function is especially helpful for testing regular expressions. It opens a browser window with the first match in the string highlighted. We’ll use it in the subsequent regex examples. The RegExr website is also helpful for testing regular expressions; it provides an interactive interface where you can write regular expressions and see where they match a string. 10.6.1 The Wildcard The regex wildcard character is . and matches any single character. For example: x = &quot;dog&quot; str_view(x, &quot;d.g&quot;) By default, regex searches from left to right: str_view(x, &quot;.&quot;) 10.6.2 Escape Sequences Like R, regular expressions can contain escape sequences that begin with a backslash. These are computed separately and after R escape sequences. The main use for escape sequences in regex is to turn a metacharacter into a literal character. For example, suppose we want to match a literal dot .. The regex for a literal dot is \\.. Since backslashes in R strings have to be escaped, the R string for this regex is \"\\\\.. Then the regex works: str_view(&quot;this.string&quot;, &quot;\\\\.&quot;) The double backslash can be confusing, and it gets worse if we want to match a literal backslash. We have to escape the backslash in the regex (because backslash is the regex escape character) and then also have to escape the backslashes in R (because backslash is also the R escape character). So to match a single literal backslash in R, the code is: str_view(&quot;this\\\\that&quot;, &quot;\\\\\\\\&quot;) Raw strings are helpful here, because they make the backslash literal in R strings (but still not in regex). We can use raw strings to write the above as: str_view(r&quot;(this\\that)&quot;, r&quot;(\\\\)&quot;) You can turn off regular expressions entirely in stringr with the fixed function: str_view(x, fixed(&quot;.&quot;)) It’s good to turn off regular expressions whenever you don’t need them, both to avoid mistakes and because they take longer to compute. 10.6.3 Anchors By default, a regex will match anywhere in the string. If you want to force a match at specific place, use an anchor. The beginning of string anchor is ^. It marks the beginning of the string, but doesn’t count as a character in the match. For example, suppose we want to match an a at the beginning of the string: x = c(&quot;abc&quot;, &quot;cab&quot;) str_view(x, &quot;a&quot;) str_view(x, &quot;^a&quot;) It doesn’t make sense to put characters before ^, since no characters can come before the beginning of the string. Likewise, the end of string anchor is $. It marks the end of the string, but doesn’t count as a character in the match. 10.6.4 Character Classes In regex, square brackets [ ] create a character class. A character class counts as one character, but that character can be any of the characters inside the square brackets. The square brackets themselves don’t count as characters in the match. For example, suppose we want to match a c followed by either a or t: x = c(&quot;ca&quot;, &quot;ct&quot;, &quot;cat&quot;, &quot;cta&quot;) str_view(x, &quot;c[ta]&quot;) You can use a dash - in a character class to create a range. For example, to match letters p through z: str_view(x, &quot;c[p-z]&quot;) Ranges also work with numbers and capital letters. To match a literal dash, place the dash at the end of the character class (instead of between two other characters), as in [abc-]. Most metacharacters are literal when inside a character class. For example, [.] matches a literal dot. A hat ^ at the beginning of the character class negates the class. So for example, [^abc] matches any one character except for a, b, or c: str_view(&quot;abcdef&quot;, &quot;[^abc]&quot;) 10.6.5 Quantifiers Quantifiers are metacharacters that affect how many times the preceeding character must appear in a match. The quantifier itself doesn’t count as a character in the match. For example, the ? quantifier means the preceeding character can appear 0 or 1 times. In other words, ? makes the preceeding character optional. For example: x = c(&quot;abc&quot;, &quot;ab&quot;, &quot;ac&quot;, &quot;abbc&quot;) str_view(x, &quot;ab?c&quot;) The * quantifier means the preceeding character can appear 0 or more times. In other words, * means the preceeding character can appear any number of times or not at all. str_view(x, &quot;ab*c&quot;) The + quantifier means the preceeding character must appear 1 or more times. Quantifiers are greedy, meaning they always match as many characters as possible. 10.6.6 Groups In regex, parentheses create a group. Groups can be affected by quantifiers, making it possible to repeat a pattern (rather than just a character). The parentheses themselves don’t count as characters in the match. For example: x = c(&quot;cats, dogs, and frogs&quot;, &quot;cats and frogs&quot;) str_view(x, &quot;cats(, dogs,)? and frogs&quot;) 10.6.7 Extracting Matches Groups are espcially useful with the stringr functions str_match and str_match_all. The str_match function extracts the overall match to the pattern, as well as the match to each group. So you can use str_match to split a string in more complicated ways than str_split, or to extract specifc pieces of a string. For example, suppose we want to split an email address: str_match(&quot;naulle@ucdavis.edu&quot;, &quot;([^@]+)@(.+)[.](.+)&quot;) ## [,1] [,2] [,3] [,4] ## [1,] &quot;naulle@ucdavis.edu&quot; &quot;naulle&quot; &quot;ucdavis&quot; &quot;edu&quot; "],["data-structures.html", "11 Data Structures 11.1 Tabular Data 11.2 Tree / Document Data Structures 11.3 Relational Databases 11.4 Non-Hierarchical Relational Data 11.5 Geospatial Data", " 11 Data Structures Merriam-Webster’s Dictionary defines data as: factual information (such as measurements or statistics) used as a basis for reasoning, discussion, or calculation information in digital form that can be transmitted or processed information output by a sensing device or organ that includes both useful and irrelevant or redundant information and must be processed to be meaningful Several key principals are introduced in the above definition: data is an intermediary step leading towards some form of analysis or or presentation, not typically an end in itself data comes in multiple formats, both digital and analogue data can be collected by both humans and machines not all data in a given dataset is necessarily meaningful, correct nor useful Data Scientists (as differentiated from statisticians or computer scientists, for example) are expert in understanding the nature of data itself and the steps necessary to assess the suitability of a given data set for answering specific research questions and the work required to properly prepare data for successful analysis. In the broadest terms, we call this process data forensics. The first step in the data forensics process is understanding the format(s) through which data are stored and transferred. 11.1 Tabular Data Tabular Data is the most ubiquitous form of data storage and the one most familiar to most users. Tabular data consists of organizing data in a table of rows and columns. Traditionally, each column in the table represents a Field of Variable and each row represents an observation or entity. For example, the table below shows a tabular organization of a subset of the mtcars dataset: Table 1.1: mpg cyl disp hp Mazda RX4 21.0 6 160.0 110 Mazda RX4 Wag 21.0 6 160.0 110 Datsun 710 22.8 4 108.0 93 Hornet 4 Drive 21.4 6 258.0 110 Hornet Sportabout 18.7 8 360.0 175 Valiant 18.1 6 225.0 105 Duster 360 14.3 8 360.0 245 Merc 240D 24.4 4 146.7 62 Merc 230 22.8 4 140.8 95 Merc 280 19.2 6 167.6 123 11.2 Tree / Document Data Structures Another popular form of data structure is the Tree structure, sometimes referred to as a Document based data structure. Tree data structures present data in a hierarchical tree-like structure in which all items related back to a single, root node. A “Family Tree” is a good example of tree structured data: The mtcars data from the above table can also be represented using a tree structure: The above image visually depicts the mtcars data as a tree, which works well for a human reader but is no parsable by the computer. There are a variety of ways to represent tree data as a computer file (or data stream) so that it can be read and parsed by the computer. In this class, we will cover two of the most popular formats: XML and JSON. 11.2.1 Structuring Data as XML XML is stands for Extensible Markup Language. Markup languages have been around since the 1960’s and were originally developed as a means to adding structured information to an existing unstructured text. In the days of analogue text preparation, professional editors typically used a blue or red pencil to make notes on typed manuscripts. The use of a specially collored pen or pencil for “marking up” documents, as the procedure was known in the industry, easily allowed subsequent readers to distinguish between editorial comment and formatting notes placed on typed manuscripts from the texts themselves. Computerized markup languages were developed as a means of allowing data specialists to markup a text in a manner that would allow the computer to distinguish between textual content and meta-information (information about the text) about the text when both types of information appear in the same file. XML is the most widely used form of markup today. In fact, nearly every webpage that you have ever viewed is actually an XML document that contains both content to be displayed and instructions for the computer on how to display that content embedded in the file using XML Tags, which are simply instructions contained with the special charcters “&lt;” and “&gt;.” For example, consider the following short email text: To: Tavi From: Jonna Subject: Meeting Date: Thursday, February 4, 2021 at 2:46 PM Don&#39;t forget about meeting with Sarah next week, 2pm in room 242. Thanks, Jonna This email contains quite a bit of structured email (sender, receiver, date/time, etc.), but there is no easy way for the computer easily extract this structure. We can solve this problem by using XML to embed information about the structure directly in the document as follows: &lt;head&gt; &lt;to&gt;Tavi&lt;/to&gt; &lt;from&gt;Jonna&lt;/from&gt; &lt;subject&gt;Meeting&lt;/subject&gt; &lt;datetime&gt; &lt;dayofweek&gt;Thursday&lt;/dayofweek&gt; &lt;month&gt;February&lt;/month&gt; &lt;day&gt;4&lt;/day&gt; &lt;year&gt;2021&lt;/year&gt; &lt;time&gt;2:46 PM&lt;/time&gt; &lt;/datetime&gt; &lt;/head&gt; &lt;body&gt; Don&#39;t forget about meeting with Sarah next week, 2pm in room 242. Thanks, &lt;signature&gt;Jonna&lt;/signuature&gt; &lt;/body&gt; By using XML, we are able to identify specific information in the email in a way that the computer is a capable of parsing. This allows us to use computational methods to easily extract information in bulk from many emails and it also allows us to program a computer program, such as an email client, to organize and properly display all of the parts of the email. The above XML example illustrates several important aspects of XML: All XML tags are enclosed in “&lt;” and “&gt;” symbols. There are 2 primary types of tags, opening tags, which designate the beginning character that is defined by the tag, and closing tags, which designate the end of the portion of the text to be associated with the opening tag. Closing tags are always indicated by slash character where tag is the name of the opening tag that is being closed. Tags be be embedded within each other in a tree-like structure. However, any tags opened within a tag must be closed before parent tag can be closed. For example, &lt;name&gt;&lt;first&gt;John&lt;/first&gt; &lt;last&gt;Doe&lt;/last&gt;&lt;/name&gt; is valid, but &lt;name&gt;&lt;first&gt;John&lt;/first&gt; &lt;last&gt;Doe&lt;/name&gt;&lt;/last&gt; is not valid. While XML was oringinally developed as a means of embedding meta information about a text directly in a text, it also quickly evolved into a stand-alone means of representing tree-structured data for exchange between computer systems. To this end, many computer applications use XML to store, share, and retrieve data. For exmaple, we can represent the data in our truncated mtcars dataset as XML as follows: &lt;cars&gt; &lt;make id=&quot;mazda&quot;&gt; &lt;model id=&quot;RX4&quot;&gt; &lt;mpg&gt;21.0&lt;/mpg&gt; &lt;cyl&gt;6&lt;/cyl&gt; &lt;disp&gt;160.0&lt;/disp&gt; &lt;hp&gt;110&lt;/hp&gt; &lt;/model&gt; &lt;model id=&quot;RX4 Wag&quot;&gt; &lt;mpg&gt;21.0&lt;/mpg&gt; &lt;cyl&gt;6&lt;/cyl&gt; &lt;disp&gt;160.0&lt;/disp&gt; &lt;hp&gt;110&lt;/hp&gt; &lt;/model&gt; &lt;/make&gt; &lt;make id=&quot;Datsun&quot;&gt; &lt;model id=&quot;710&quot;&gt; &lt;mpg&gt;22.8&lt;/mpg&gt; &lt;cyl&gt;4&lt;/cyl&gt; &lt;disp&gt;108.0&lt;/disp&gt; &lt;hp&gt;93&lt;/hp&gt; &lt;/model&gt; &lt;/make&gt; &lt;make id=&quot;Hornet&quot;&gt; &lt;model id=&quot;4 Drive&quot;&gt; &lt;mpg&gt;21.4&lt;/mpg&gt; &lt;cyl&gt;6&lt;/cyl&gt; &lt;disp&gt;258.0&lt;/disp&gt; &lt;hp&gt;110&lt;/hp&gt; &lt;/model&gt; &lt;model id=&quot;Sportabout&quot;&gt; &lt;mpg&gt;18.7&lt;/mpg&gt; &lt;cyl&gt;8&lt;/cyl&gt; &lt;disp&gt;360.0&lt;/disp&gt; &lt;hp&gt;175&lt;/hp&gt; &lt;/model&gt; &lt;/make&gt; &lt;make id=&quot;Valiant&quot;&gt; &lt;model id=&quot;valiant&quot;&gt; &lt;mpg&gt;18.1&lt;/mpg&gt; &lt;cyl&gt;6&lt;/cyl&gt; &lt;disp&gt;225.0&lt;/disp&gt; &lt;hp&gt;105&lt;/hp&gt; &lt;/model&gt; &lt;/make&gt; &lt;make id=&quot;Duster&quot;&gt; &lt;model id=&quot;360&quot;&gt; &lt;mpg&gt;14.3&lt;/mpg&gt; &lt;cyl&gt;8&lt;/cyl&gt; &lt;disp&gt;360.0&lt;/disp&gt; &lt;hp&gt;245&lt;/hp&gt; &lt;/model&gt; &lt;/make&gt; &lt;make id=&quot;Merc&quot;&gt; &lt;model id=&quot;240D&quot;&gt; &lt;mpg&gt;24.4&lt;/mpg&gt; &lt;cyl&gt;4&lt;/cyl&gt; &lt;disp&gt;146.7&lt;/disp&gt; &lt;hp&gt;62&lt;/hp&gt; &lt;/model&gt; &lt;model id=&quot;230&quot;&gt; &lt;mpg&gt;22.8&lt;/mpg&gt; &lt;cyl&gt;4&lt;/cyl&gt; &lt;disp&gt;140.8&lt;/disp&gt; &lt;hp&gt;95&lt;/hp&gt; &lt;/model&gt; &lt;model id=&quot;280&quot;&gt; &lt;mpg&gt;19.2&lt;/mpg&gt; &lt;cyl&gt;6&lt;/cyl&gt; &lt;disp&gt;167.6&lt;/disp&gt; &lt;hp&gt;123&lt;/hp&gt; &lt;/model&gt; &lt;/make&gt; &lt;/cars&gt; For an XML dataset to be technically valid, the tags used to markup the dataset must themselves be defined according to a schema, another XML document that defines all tags that can be used in marking up a dataset and the allowable tree structure of the markup (for example, which tags can be parents of which other tags, etc.). You do not need to understand, or even know, the schema being used to present data in order to read and parse an XML document. However, schemas are extremely useful (and often necessary) for building applications that perform advanced processing of XML documents, such as web browsers, emial clients, etc. For more information on XML and XML Schemas see the w3schools XML Tutorial at https://www.w3schools.com/xml/. 11.2.2 Structuring Data as JSON XML provides an excellent framework for encoding, saving, and transfering all kinds of data, and it was the dominant mode of transfering data across the internet for many years. However, XML has an Achilles’ Heel from the data transfer perspective: a lack of sparcity. If you look closely at the mtcars dataset XML example above, you will note that the markup accounts for more of the total characters in the document than the data itself. In a world where data is regularly being exchanged in real time across the network, the use of XML can result in the necessity to exchange a lot more data to accomplish the same task. This adds both time and cost to every data transaction. JavaScript Object Notation (JSON) was developed as a standard to address this problem and provides a sparse framework for representing data that introduces minimal, non-data elements into the overal data structure. JSON uses a key/value pair structure to represent data elements: &quot;model&quot;:&quot;RX4&quot; Individual data elements are then grouped to reflect more complex data structures: {&quot;model&quot;: {&quot;id&quot;: &quot;2&quot;, &quot;hp&quot;: &quot;120&quot;}} The example below shows the subsetted mtcars dataset represented as JSON. Note the use of the “[” character to indicated repeated elements in the data: { &quot;cars&quot;: [{ &quot;make&quot;: &quot;Mazda&quot;, &quot;model&quot;: [{ &quot;id&quot;: &quot;RX4&quot;, &quot;mpg&quot;: &quot;21.0&quot;, &quot;cyl&quot;: &quot;6&quot;, &quot;disp&quot;: &quot;160.0&quot;, &quot;hp&quot;: &quot;110&quot; }, { &quot;id&quot;: &quot;RX4 Wag&quot;, &quot;mpg&quot;: &quot;21.0&quot;, &quot;cyl&quot;: &quot;6&quot;, &quot;disp&quot;: &quot;160.0&quot;, &quot;hp&quot;: &quot;110&quot; } ] }, { &quot;make&quot;: &quot;Datsun&quot;, &quot;model&quot;: { &quot;id&quot;: &quot;710&quot;, &quot;mpg&quot;: &quot;22.8&quot;, &quot;cyl&quot;: &quot;4&quot;, &quot;disp&quot;: &quot;108.0&quot;, &quot;hp&quot;: &quot;93&quot; } }, { &quot;make&quot;: &quot;Hornet&quot;, &quot;model&quot;: [{ &quot;id&quot;: &quot;4 Drive&quot;, &quot;mpg&quot;: &quot;21.4&quot;, &quot;cyl&quot;: &quot;6&quot;, &quot;disp&quot;: &quot;258.0&quot;, &quot;hp&quot;: &quot;110&quot; }, { &quot;id&quot;: &quot;Sportabout&quot;, &quot;mpg&quot;: &quot;18.7&quot;, &quot;cyl&quot;: &quot;8&quot;, &quot;disp&quot;: &quot;360.0&quot;, &quot;hp&quot;: &quot;175&quot; } ] }, { &quot;make&quot;: &quot;Valiant&quot;, &quot;model&quot;: { &quot;id&quot;: &quot;valiant&quot;, &quot;mpg&quot;: &quot;18.1&quot;, &quot;cyl&quot;: &quot;6&quot;, &quot;disp&quot;: &quot;225.0&quot;, &quot;hp&quot;: &quot;105&quot; } }, { &quot;make&quot;: &quot;Duster&quot;, &quot;model&quot;: { &quot;id&quot;: &quot;360&quot;, &quot;mpg&quot;: &quot;14.3&quot;, &quot;cyl&quot;: &quot;8&quot;, &quot;disp&quot;: &quot;360.0&quot;, &quot;hp&quot;: &quot;245&quot; } }, { &quot;make&quot;: &quot;Merc&quot;, &quot;model&quot;: [{ &quot;id&quot;: &quot;240D&quot;, &quot;mpg&quot;: &quot;24.4&quot;, &quot;cyl&quot;: &quot;4&quot;, &quot;disp&quot;: &quot;146.7&quot;, &quot;hp&quot;: &quot;62&quot; }, { &quot;id&quot;: &quot;230&quot;, &quot;mpg&quot;: &quot;22.8&quot;, &quot;cyl&quot;: &quot;4&quot;, &quot;disp&quot;: &quot;140.8&quot;, &quot;hp&quot;: &quot;95&quot; }, { &quot;id&quot;: &quot;280&quot;, &quot;mpg&quot;: &quot;19.2&quot;, &quot;cyl&quot;: &quot;6&quot;, &quot;disp&quot;: &quot;167.6&quot;, &quot;hp&quot;: &quot;123&quot; } ] } ] } For information on the JSON format, see the Tutorialspoint JSON Tutorial at https://www.tutorialspoint.com/json/index.htm. You can also use the JSONLint Json Validator at https://jsonlint.com/ to check the syntax of any JSON representation. 11.3 Relational Databases Relational Databases, frequently referred to as Relational Database Management Systems (RDBMS), provide another way of structuring data. Unlike tabular, XML, and JSON data representations, RDBMS data is not easily human readable and specialized software is usually required to interact with data stored as relational data. Most programming environments (including r) provide specialized drivers for communicating with RDBMS in order to facilitate working with data stored in these systems. RDBMS have three primary purposes as a data storage format: To reduce duplication of data; To speed-up access and insertion of new data; To insure data integrity. Items 2 and 3 above are accomplished at the software level, by deploying strict checks on data input, complex data indexing systems, and implementing redundant, automated, backup systems, to name just a few of the functionalities offered by RDBMS. Item 1 above, reducing duplication of data, is accomplished by using a specific, “relational” data structure that encourages the use of controlled lists of data mapped to individual observations. Looking at our mtcars subset data, for example, we see that while there are ten observations, there are only 6 makes of cars. To represent this in RDBMS, we first create a “Table,” a named collection of data, that contains a unique list of car makes: Table 1.2: MAKE_TABLE id Make 1 Mazda 2 Datsun 3 Hornet 4 Valiant 5 Duster 6 Merc Once we have a table of unique lists, we then create and populate a table of our cars, associating each car with its appropriate make from the MAKE_TABLE table: Table 11.1: CARS_TABLE Make Model mpg cyl disp hp 1 RX4 21.0 6 160.0 110 1 RX4 Wag 21.0 6 160.0 110 2 710 22.8 4 108.0 93 3 4 Drive 21.4 6 258.0 110 3 Sportabout 18.7 8 360.0 175 4 Valiant 18.1 6 225.0 105 5 360 14.3 8 360.0 245 6 240D 24.4 4 146.7 62 6 230 22.8 4 140.8 95 6 280 19.2 6 167.6 123 In the above table, we only normalized the car Make field. In a fully normalized RDBMS data structure, we would also create a control table for the Model field in anticipation of the fact that we could have more than one observation for a given model. Fully normalized RBDMS data structures use control tables for all fields that contain string data. The image below shows a sample Entry Relationship Diagram (ERD) for a more complex dataset relating to course offerings and enrollments. Each line connecting two tables marks a field in a “join” table that uses the id field in a control table (known as a foreign key) to associate information in the control table with the records in the join table. 11.4 Non-Hierarchical Relational Data In the era of the social network, it is becoming increasingly necessary to represent relationships between entities that are not hierarchical. Unlike a family tree, the fact that you are connected to someone on Facebook or Instagram does not imply any type of hierarchical relationship. Such networks are typically represented using the Graph data structure: Graphs consists of collections of vertices or nodes, the entities being graphed, and edges, the relationships between nodes. Another important aspect of graph data is the concept of directionality. A directed graph indicates the direction of the relationship identified by the edge. We might, for example, wish to draw edges that indicate that one node was influenced by another node, in which case we could identify an “influence” edge and use directionality to indicate who influenced whom: Graph data can be stored and or transfered using any of the data formats discussed above or using specialized graph databases management software. 11.5 Geospatial Data Geospatial data represents a final type of data with it’s own, unique data structure. Geospatial data unique because it always realates directly to the physical world and, because it relies on world-wide standards which have been in development and communally accepted for hundreds of years. Because of its uniqueness as a data type, geospatial data will be covered as a stand-alone topic later in the course. "],["how-the-web-works.html", "12 How the Web Works 12.1 Client-Server Architecture 12.2 Understanding URLs", " 12 How the Web Works The discipline of Data Science was, in a large part, ushered into being by the increasing availability of information available on the World Wide Web or through other internet sources. Prior to the popularization of the internet as a publishing and communications platform, the majority of scientific research involved controlled studies in which researchers would collect their own data through various direct means of data collection (surveys, medical testing, etc.) in order to test a stated hypothesis. The vast amount of information available on the internet disrupted this centuries long dominance. Today, the dominant form of scientific research involves using data collected or produced by others for reasons having little or nothing to do with the research question being investigated by scholar. Users who post items about their favorite political candidate are not, for example, doing this so that sociologists can better under how politics function in America. However, their Tweets are being used in that and many other unforseen capacities. Because the internet provides such a rich trove of information for study, understanding how to effectively get, process, and prepare information from the internet for scientific research is a crucial skill for any data scientist. And in order to understand these workflows, the data scientist must first understand how the internet itself functions. 12.1 Client-Server Architecture The base architecture and functioning of the internet is quite simple: A content producer puts information on a computer called the server for others to retrieve; A user uses their local computer, called the client, to request the information from the sever; The server delivers the information to the client. Each of the above detailed steps is accomplished using a technically complex but conceptually simple set of computer protocols. The technical details are beyond the scope of this course. We are here concerned with their conceptual architecture. 12.1.1 Communication Between Clients and Servers Anytime a computer connects to any network, that computer is assigned a unique identifier known as an internet protocol (IP) address that uniquely identifies that computer on the network. IP addresses have the form x.x.x.x, where each x can be any integer from 0 to 255. For example, 169.237.102.141 is the current IP address of the computer that hosts the DataLab website. IP addresses are sometimes pre-designated for particular computers. A pre-designated IP address is known as static IP address. In other cases IP addresses are dynamically assigned from a range of available IP Address using a system known as the Dynamic Host Configuration Protocol (DHCP). Servers are typically assigned static IP addresses and clients are typically assigned dynamic IP addresses. As humans, we are used to accessing websites via a domain name (which we’ll discuss shortly), but you can also contact any server on the internet by simply typing the IP address into your browser address bar where you would normally enter the URL. For example, you can simply click on https://169.237.102.141 to access the DataLab website. (note: your browser may give you a security warning if you try to access a server directly using an IP address. For the link above, it is safe to proceed to the site.) 12.1.2 Domain Name Resolution IP addresses are the unique identifiers that make the internet work, but they are not very human friendly. To solve this problem, a system of domain name resolution was created. Under this system, internet service providers access a universal domain registry database that associates human readable domain names with machine readable IP addresses, and a secondary set of of internet connected servers known as domain name servers (DNS) provide a lookup service that translates domain names into IP addresses in the background. As the end-user, you enter and see only domain names, but the actual request process is a multi-step process in which domain names are translated to IP address in the background: A content produce puts information on a computer called the server for others to retrieve; A user uses their local computer, called the client, to request the information from the sever using a domain name using request software such as a web browser; The user’s client software first sends a request to a DNS server to retrieve the IP address of the server on the network associated with the entered domain name; The DNS server returns the associated IP address to the client; The client then makes the information request to the server using its retrieved IP address; The server delivers the information to the client. 12.1.3 Request Routing Our simple digram of the client server process shows only two computers. But when you connect to the internet you are not, of course, creating a direct connection to a single computer. Rather, you are connecting to vase network of literally millions of computers, what we have come to refer to as the cloud. In order to solve this problem, the internet backbone also deploys a routing system that directs requests and responses across the network to the appropriate servers and clients. When you connect to the WiFi network in your home, office, or the local coffee house, you are connecting to a router. That router receives all of your requests and, provided you are not requesting something directly from another computer that is connected to the same router, passes that request on to a larger routing network at the Internet Service Provider (ISP). When the ISP routers receive your request, they check to see if your requestion something from a computer that connected to their network. If it is, they deliver the request. If it is not, they pass the request on to another, regional routing network. And this routing process is repeated until your request if finally routed to the correct server. A content produce puts information on a computer called the server for others to retrieve; A user uses their local computer, called the client, to request the information from the sever using a domain name using request software such as a web browser; The user’s client software first sends a request to a DNS server to retrieve the IP address of the server on the network associated with the entered domain name; The DNS server returns the associated IP address to the client; The client sends the request to the local (in home, office, etc.) router; After check of IP addresses on local network, request is routed to the ISP’s routing system; The request is passed through the internet routing network until it reaches the routing system of the server’s ISP and, finally, the server itself. 12.1.4 The Server Response When a request is sent to a server across the internet, the request includes both the specific URL of the resource being request and also an hidden request header. The request header provides information to the server such as the IP address and the operating system of the client, the transfer protocol being used, and the software on the client that is making the request. The server uses this information to properly format it’s response and to route it back to the requesting client using the same IP routing process as described above. 12.1.5 Internet Transfer Protocols All of the information transferred between computers over the network is transfered as streams of binary data. In order to ensure data integrity, these streams are usually broken up into smaller packets of data which are transmitted independent of each other and then reassembled by the receiving computer once it has received all of the packets in the stream. The first packet returned (a header packet) typically delivers information about how many packets the client should expect to receive and about how they should be reassembled to recreate the original data stream. There are many different standards for how data streams are divided into packets. One standard might, for example, break the stream into a collection of 50 byte packets, while another might use 100 byte packages. These standards are called protocols. The two protocols that are familiar to most users are http and https, which define the hypertext transfer protocol and its sibling the hypertext transfer secure protocol. When you type a url like https://datalab.ucdavis.edu into your browser, you are instructing the browser to use the https protocol to exchange information. Because http and https are so common, most modern browsers do not require you to type the protocol name. They will simply insert the protocol for you in the background. 12.2 Understanding URLs URL is an acronym for Uniform Resource Locators. “Uniform” is a key term in this context. URLs are not arbitrary pointers to information. They are machine and human readable and parsable and contain a lot of information in them. All URLs are constructed using a standardized format. Consider the following URL: https://sfbaywildlife.info/species/common_birds.htm There are actually several distinct components to the above URL Table 1.1: protocol server path to file https:// sfbaywildlife.info /species/common_birds.htm We’ve already discussed Internet Protocols and domain names. The file path portion of the URL can also provide valuable information about the server. It reads exactly like a Unix file path on the command line. The path /species/common_birds.htm indicates that the file common_birds.htm is in the species directory on the server. 12.2.1 Dynamic Files In the above example, when you enter the URL https://sfbaywildlife.info/species/common_birds.htm, your browser requests the file at /species/common_birds.html on the server. The server simply finds the file and delivers it to your web browser. We call this a static web server because the server itself does not do any processing of files prior to delivery. It simply receives requests for files living on the server and then sends them to the client, whose browser renders the file for viewing. Many websites, however, use dynamic processing. Pages with file extensions such as .php or .jsp, for example, include computer code in them. When these pages are requested by the server, the server executes the code in the designated file and sends the output of that execution to the requesting client rather than the actual file. Many sites, such as online stores and blogs, use this functionality to connect their web pages to active databases that track inventory and orders, for example. 12.2.2 Query Strings Dynamic websites, such as e-commerce sites that are connected to databases, require a mechanism for users to submit information to the server for processing. This is accomplished through one of two HTTP commands: GET or POST. POST commands send submitted information to the server via a hidden HTTP header that is invisible to the end user. Scraping sites that require POST transactions is possible but can require significant sleuthing to determine the correct parameters and is beyond the scope of this course. GET requests, which are, happily for web scrapers more ubiquitous than POST requests, are much easier to understand. They are submitted via a query string that is simply appended to the request URL as in the following example: https://ebba.english.ucsb.edu/search_combined/?ft=dragon&amp;numkw=52 Here we see a Query String appended to the end of the actual URL: Table 1.2: protocol server path to file query string https:// ebba.english.ucsb.edu /search_combined/index.php ?ft=dragon&amp;numkw=52 Query strings always appear at the end of the URL and begin with the ? character followed by a series of key/value pairs separated by the &amp; character. In the above example we see that two parameters are submitted to the server via the query string as follows: ft=dragon numkw=52 The server will use these parameter values as input to perform a dynamic operation, in this case searching a database. "],["web-scraping.html", "13 Web Scraping 13.1 Getting Data from the Web 13.2 R’s XML Parsers 13.3 XPath 13.4 The Web Scraping Workflow 13.5 Case Study: CA Cities 13.6 Case Study: The CA Aggie 13.7 CSS Selectors", " 13 Web Scraping Scraping a web page means extracting information so that it can be used programmatically (for instance, in R). After this lesson, you should be able to: Explain and read hypertext markup language (HTML) View the HTML source of a web page Use Firefox or Chrome’s web developer tools to locate tags within a web page With the rvest package: Read HTML into R Extract HTML tables as data frames With the xml2 package: Use XPath to extract specific elements of a page 13.1 Getting Data from the Web Ways you can get data from the web, from most to least convenient: Direct download or “data dump” R or Python package (there are packages for many popular web APIs) Documented web API Undocumented web API Scraping 13.1.1 What’s in a Web Page? Modern web pages usually consist of many files: Hypertext markup language (HTML) for structure and formatting Cascading style sheets (CSS) for more formatting JavaScript (JS) for interactivity Images HTML is the only component that always has to be there. Since HTML is what gives a web page structure, it’s what we’ll focus on when scraping. HTML is closely related to eXtensible markup language (XML). Both languages use tags to mark structural elements of data. In HTML, the elements literally correspond to the elements of a web page: paragraphs, links, tables, and so on. Most tags come in pairs. The opening tag marks the beginning of an element and the closing tag marks the end. Opening tags are written &lt;NAME&gt;, where NAME is the name of the tag. Closing tags are written &lt;/NAME&gt;. A singleton tag is a tag that stands alone, rather than being part of a pair. Singleton tags are written &lt;NAME /&gt;. In HTML (but not XML) they can also be written &lt;NAME&gt;. Fortunately, HTML only has a few singleton tags, so they can be distinguished by name regardless of which way they’re written. For example, here’s some HTML that uses the em (emphasis, usually italic) and strong (usually bold) tags, as well as the singleton br (line break) tag: &lt;em&gt;&lt;strong&gt;This text&lt;/strong&gt; is emphasized.&lt;br /&gt;&lt;/em&gt; Not emphasized A pair of tags can contain other elements (paired or singleton tags), but not a lone opening or closing tag. This creates a strict, treelike hierarchy. Opening and singleton tags can have attributes that contain additional information. Attributes are name-value pairs written NAME=\"VALUE\" after the tag name. For instance, the HTML a (anchor) tag creates a link to the URL provided for the href attribute: &lt;a href=&quot;http://www.google.com/&quot; id=&quot;mytag&quot;&gt;My Search Engine&lt;/a&gt; In this case the tag also has a value set for the id attribute. Now let’s look at an example of HTML for a complete, albeit simple, web page: &lt;html&gt; &lt;head&gt; &lt;title&gt;This is the page title!&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;This is a header!&lt;/h1&gt; &lt;p&gt;This is a paragraph. &lt;a href=&quot;http://www.r-project.org/&quot;&gt;Here&#39;s a website!&lt;/a&gt; &lt;/p&gt; &lt;p id=&quot;hello&quot;&gt;This is another paragraph.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; In most web browsers, you can examine the HTML for a web page by right-clicking and choosing “View Page Source.” See here for a more detailed explanation of HTML, and here for a list of valid HTML elements. 13.2 R’s XML Parsers A parser converts structured data into familiar data structures. R has two popular packages for parsing XML (and HTML): The “XML” package The “xml2” package The XML package has more features. The xml2 package is more user-friendly, and as part of the Tidyverse, it’s relatively well-documented. This lesson focuses on xml2, since most of the additional features in the XML package are related to writing (rather than parsing) XML documents. The xml2 package is often used in conjunction with the “rvest” package, which provides support for CSS selectors (described later in this lesson) and automates scraping HTML tables. The first time you use these packages, you’ll have to install them: install.packages(&quot;xml2&quot;) install.packages(&quot;rvest&quot;) Let’s start by parsing the example of a complete web page from earlier. The xml2 function read_xml reads an XML document, and the rvest function read_html reads an HTML document. Both accept an XML/HTML string or a file path (including URLs): html = r&quot;( &lt;html&gt; &lt;head&gt; &lt;title&gt;This is the page title!&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;This is a header!&lt;/h1&gt; &lt;p&gt;This is a paragraph. &lt;a href=&quot;http://www.r-project.org/&quot;&gt;Here&#39;s a website!&lt;/a&gt; &lt;/p&gt; &lt;p id=&quot;hello&quot;&gt;This is another paragraph.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; )&quot; library(xml2) library(rvest) doc = read_html(html) doc ## {html_document} ## &lt;html&gt; ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... ## [2] &lt;body&gt;\\n &lt;h1&gt;This is a header!&lt;/h1&gt;\\n &lt;p&gt;This is a paragraph.\\n ... The xml_children function returns all of the immediate children of a given element. The top element of our document is the html tag, and its immediate children are the head and body tags: tags = xml_children(doc) The result from xml_children is a node set (xml_nodeset object). Think of a node set as a vector where the elements are tags rather than numbers or strings. Just like a vector, you can access individual elements with the indexing (square bracket [) operator: length(tags) ## [1] 2 head = tags[1] head ## {xml_nodeset (1)} ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... The xml_text function returns the text contained in a tag. Let’s get the text in the title tag, which is beneath the head tag. First we isolate the tag, then use xml_text: title = xml_children(head) xml_text(title) ## [1] &quot;&quot; &quot;This is the page title!&quot; Navigating through the tags by hand is tedious and easy to get wrong, but fortunately there’s a better way to find the tags we want. 13.3 XPath An XML document is a tree, similar to the file system on your computer: html ├── head │ └── title └── body ├── h1 ├── p └── p └── a When we wanted to find files, we wrote file paths. We can do something similar to find XML elements. XPath is a language for writing paths to elements in an XML document. XPath is not R-specific. At a glance, an XPath looks similar to a file path: XPath Description / root, or element separator . current tag .. parent tag * any tag (wildcard) The xml2 function xml_find_all finds all elements at given XPath: xml_find_all(doc, &quot;/html/body/p&quot;) ## {xml_nodeset (2)} ## [1] &lt;p&gt;This is a paragraph.\\n &lt;a href=&quot;http://www.r-project.org/&quot;&gt;Here&#39;s ... ## [2] &lt;p id=&quot;hello&quot;&gt;This is another paragraph.&lt;/p&gt; Unlike a file path, an XPath can identify multiple elements. If you only want a specific element, use indexing to get it from the result. XPath also has some features that are different from file paths. The // separator means “at any level beneath.” It’s a useful shortcut when you want to find a specific element but don’t care where it is. Let’s get all of the p elements at any level of the document: xml_find_all(doc, &quot;//p&quot;) ## {xml_nodeset (2)} ## [1] &lt;p&gt;This is a paragraph.\\n &lt;a href=&quot;http://www.r-project.org/&quot;&gt;Here&#39;s ... ## [2] &lt;p id=&quot;hello&quot;&gt;This is another paragraph.&lt;/p&gt; Let’s also get all a elements at any level beneath a p element: xml_find_all(doc, &quot;//p/a&quot;) ## {xml_nodeset (1)} ## [1] &lt;a href=&quot;http://www.r-project.org/&quot;&gt;Here&#39;s a website!&lt;/a&gt; The vertical bar | means “or.” You can use it to get two different sets of elements in one query. Let’s get all h1 or p tags: xml_find_all(doc, &quot;//h1|//p&quot;) ## {xml_nodeset (3)} ## [1] &lt;h1&gt;This is a header!&lt;/h1&gt; ## [2] &lt;p&gt;This is a paragraph.\\n &lt;a href=&quot;http://www.r-project.org/&quot;&gt;Here&#39;s ... ## [3] &lt;p id=&quot;hello&quot;&gt;This is another paragraph.&lt;/p&gt; 13.3.1 Predicates In XPath, the predicate operator [] gets elements at a position or matching a condition. Most conditions are about the attributes of the element. In the predicate operator, attributes are always prefixed with @. For example, suppose we want to find all tags where the id attribute is equal to \"hello\": xml_find_all(doc, &quot;//*[@id = &#39;hello&#39;]&quot;) ## {xml_nodeset (1)} ## [1] &lt;p id=&quot;hello&quot;&gt;This is another paragraph.&lt;/p&gt; Notice that the equality operator in XPath is =, not ==. Strings in XPath can be quoted with single or double quotes. You can combine multiple conditions in the predicate operator with and and or. There are also several XPath functions you can use in the predicate operator. These functions are not R functions, but rather built into XPath. Here are a few: XPath Description not() negation contains() check string x contains y text() get tag text substring() get a substring For instance, suppose we want to get elements that contain the word “paragraph”: xml_find_all(doc, &quot;//*[contains(text(), &#39;paragraph&#39;)]&quot;) ## {xml_nodeset (2)} ## [1] &lt;p&gt;This is a paragraph.\\n &lt;a href=&quot;http://www.r-project.org/&quot;&gt;Here&#39;s ... ## [2] &lt;p id=&quot;hello&quot;&gt;This is another paragraph.&lt;/p&gt; Finally, note that you can also use the predicate operator to get elements at a specific position. For example, to get the second p element anywhere in the document: xml_find_all(doc, &quot;//p[2]&quot;) ## {xml_nodeset (1)} ## [1] &lt;p id=&quot;hello&quot;&gt;This is another paragraph.&lt;/p&gt; Notice that this is the same as if we had used R to get the second element: xml_find_all(doc, &quot;//p&quot;)[2] ## {xml_nodeset (1)} ## [1] &lt;p id=&quot;hello&quot;&gt;This is another paragraph.&lt;/p&gt; Beware that although the XPath predicate operator resembles R’s indexing operator, the syntax is not always the same. We’ll learn more XPath in the examples. There’s a complete list of XPath functions on Wikipedia. 13.4 The Web Scraping Workflow Scraping a web page is part technology, part art. The goal is to find an XPath that’s concise but specific enough to identify only the elements you want. If you plan to scrape the web page again later or want to scrape a lot of similar web pages, the XPath also needs to be general enough that it still works even if there are small variations. Firefox and Chrome include “web developer tools” that are invaluable for planning a web scraping strategy. Press Ctrl + Shift + i (Cmd + Shift + i on OS X) in Firefox or Chrome to open the web developer tools. We can also use the web developer tools to interactively identify the element that corresponds to a specific part of a web page. Press Ctrl + Shift + c and then click on the part of the web page you want to identify. The best way to approach web scraping (and programming in general) is as an incremental, iterative process. Use the web developer tools to come up with a basic strategy, try it out in R, check which parts don’t work, and then repeat to adjust the strategy. Expect to go back and forth between your web browser and R several times when you’re scraping. Most scrapers follow the same four steps, regardless of the web page and the language of the scraper: Download pages with an HTTP request (usually GET) Parse pages to extract text Clean up extracted text with string methods or regex Save cleaned results In R, xml2’s read_xml function takes care of step 1 for you, although you can also use httr functions to make the request yourself. 13.4.1 Being Polite Making an HTTP request is not free! It has a real cost in CPU time and also cash. Server administrators will not appreciate it if you make too many requests or make requests too quickly. So: If you’re making multiple requests, slow them down by using R’s Sys.sleep function to make R do nothing for a moment. Aim for no more than 20-30 requests per second, unless you’re using an API that says more are okay. Avoid requesting the same page twice. One way to do this is by caching (saving) the results of the requests you make. You can do this manually, or use a package that does it automatically, like the httpcache package. Failing to be polite can get you banned from websites! Also check the website’s terms of service to make sure scraping is not explicitly forbidden. 13.5 Case Study: CA Cities Wikipedia has many pages that are just tables of data. For example, there’s this list of cities and towns in California. Let’s scrape the table to get a data frame. Step 1 is to download the page: wiki_url = &quot;https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_California&quot; wiki_doc = read_html(wiki_url) Step 2 is to extract the table element from the page. We can use Firefox or Chrome’s web developer tools to identify the table. HTML tables usually use the table tag. Let’s see if it’s the only table in the page: tables = xml_find_all(wiki_doc, &quot;//table&quot;) tables ## {xml_nodeset (4)} ## [1] &lt;table class=&quot;wikitable&quot;&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th scope=&quot;row&quot; style=&quot;background ... ## [2] &lt;table class=&quot;wikitable plainrowheaders sortable&quot;&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th scop ... ## [3] &lt;table class=&quot;nowraplinks hlist mw-collapsible autocollapse navbox-inner&quot; ... ## [4] &lt;table class=&quot;nowraplinks mw-collapsible autocollapse navbox-inner&quot; style ... The page has 4 tables. We can either make our XPath more specific, or use indexing to get the table we want. Refining the XPath makes our scraper more robust, but indexing is easier. For the sake of learning, let’s refine the XPath. Going back to the browser, we can see that the table includes \"wikitable\" and \"sortable\" in its class attribute. So let’s search for these among the table elements: tab = xml_find_all(tables, &quot;//*[contains(@class, &#39;sortable&#39;)]&quot;) tab ## {xml_nodeset (1)} ## [1] &lt;table class=&quot;wikitable plainrowheaders sortable&quot;&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th scop ... Now we get just one table! Here we used a second XPath applied only to the results from the first, but we also could’ve done this all with one XPath: //table[contains(@class, 'sortable')]. The next part of extracting the data is to extract the value from each individual cell in the table. HTML tables have a strict layout order, with tags to indicate rows and cells. We could extract each cell by hand and then reassemble them into a data frame, but the rvest function html_table can do it for us automatically: cities = html_table(tab, fill = TRUE) cities = cities[[1]] head(cities) ## Name Type County Population (2010)[1][8][9] Land area[1] ## 1 Name Type County Population (2010)[1][8][9] sq mi ## 2 Adelanto City San Bernardino 31,765 56.01 ## 3 Agoura Hills City Los Angeles 20,330 7.79 ## 4 Alameda City Alameda 73,812 10.61 ## 5 Albany City Alameda 18,539 1.79 ## 6 Alhambra City Los Angeles 83,089 7.63 ## Land area[1] Incorporated[7] ## 1 Land area[1] km2 ## 2 145.1 December 22, 1970 ## 3 20.2 December 8, 1982 ## 4 27.5 April 19, 1854 ## 5 4.6 September 22, 1908 ## 6 19.8 July 11, 1903 The fill = TRUE argument ensures that empty cells are filled with NA. We’ve successfully imported the data from the web page into R, so we’re done with step 2. 13.5.1 Data Cleaning Step 3 is to clean up the data frame. The column names contain symbols, the first row is part of the header, and the column types are not correct. # Fix column names. names(cities) = c(&quot;city&quot;, &quot;type&quot;, &quot;county&quot;, &quot;population&quot;, &quot;mi2&quot;, &quot;km2&quot;, &quot;date&quot;) # Remove fake first row. cities = cities[-1, ] # Reset row names. rownames(cities) = NULL How can we clean up the date column? The as.Date function converts a string into a date R understands. The idea is to match the date string to a format string where the components of the date are indicated by codes that start with %. For example, %m stands for the month as a two-digit number. You can read about the different date format codes in ?strptime. Here’s the code to convert the dates in the data frame: dates = as.Date(cities$date, &quot;%B %m, %Y&quot;) cities$date = dates We can also convert the population to a number: class(cities$population) ## [1] &quot;character&quot; # Remove commas and footnotes (e.g., [1]) before conversion library(stringr) pop = str_replace_all(cities$population, &quot;,&quot;, &quot;&quot;) pop = str_replace_all(pop, &quot;\\\\[[0-9]+\\\\]&quot;, &quot;&quot;) pop = as.numeric(pop) # Check for missing values, which can mean conversion failed any(is.na(pop)) ## [1] FALSE cities$population = pop 13.6 Case Study: The CA Aggie Suppose we want to scrape The California Aggie. In particular, we want to get all the links to news articles on the features page https://theaggie.org/features/. This could be one part of a larger scraping project where we go on to scrape individual articles. First, let’s download the features page so we can extract the links: url = &quot;https://theaggie.org/features/&quot; doc = read_html(url) We know that links are in a tags, but we only want links to articles. Looking at the features page with the web developer tools, the links to feature articles are all inside of a section tag. So let’s get the section tag: xml_find_all(doc, &quot;//section&quot;) # OR html_nodes(doc, &quot;section&quot;) ## {xml_nodeset (1)} ## [1] &lt;section id=&quot;blog-grid&quot;&gt;&lt;div class=&quot;blog-grid-container&quot;&gt;\\n\\n\\t\\t\\t\\t\\t&lt;d ... Just to be safe, let’s also use the id attribute, which is \"blog-grid\". Usually the id of an element is unique, so this ensures that we get the right section even if later the web developer for The Aggie adds other section tags. We can also add in a part about getting links now: section = xml_find_all(doc, &quot;//section[@id = &#39;blog-grid&#39;]&quot;) # OR html_nodes(doc, &quot;section#blog-grid&quot;) links = xml_find_all(section, &quot;//a&quot;) # OR html_nodes(section, &quot;a&quot;) length(links) ## [1] 267 That gives us 267 links, but there are only 15 articles on the page, so something’s still not right. Inspecting the page again, there are actually three links to each article: on the image, on the title, and on “Continue Reading.” Let’s focus on the title link. There are a couple different ways we can identify the title link: Always inside an h2 tag Always has title attribute that starts with “Permanent” Generally it’s more robust to rely on tags (structure) than to rely on attributes (other than id and class). So let’s use the h2 tag here: links = xml_find_all(section, &quot;//h2/a&quot;) # OR html_nodes(section, &quot;h2 &gt; a&quot;) length(links) ## [1] 15 Now we’ve got the 15 links, so let’s get the URLs from the href attribute. feature_urls = xml_attr(links, &quot;href&quot;) The other article listings (Sports, Science, etc) on The Aggie have a similar structure, so we can potentially reuse our code to scrape those. So let’s turn our code into a function. The input will be a downloaded page, and the output will be the article links. parse_article_links = function(page) { section = xml_find_all(page, &quot;//section[@id = &#39;blog-grid&#39;]&quot;) links = xml_find_all(section, &quot;//h2/a&quot;) xml_attr(links, &quot;href&quot;) } We can test this out on the Sports page. First we download the page: sports = read_html(&quot;https://theaggie.org/sports&quot;) Then we call the function on the document: sports_urls = head(parse_article_links(sports)) head(sports_urls) ## [1] &quot;https://theaggie.org/2021/02/19/pickleball-the-rising-sport/&quot; ## [2] &quot;https://theaggie.org/2021/02/19/2021-olympic-games-are-in-limbo/&quot; ## [3] &quot;https://theaggie.org/2021/02/12/wild-year-for-football-culminates-in-super-bowl/&quot; ## [4] &quot;https://theaggie.org/2021/02/12/american-sports-sees-a-lack-of-diversity-in-team-coach-composition/&quot; ## [5] &quot;https://theaggie.org/2021/02/10/preview-of-the-uefa-champions-league-knockout-stage/&quot; ## [6] &quot;https://theaggie.org/2021/02/05/out-of-the-bubble-trouble/&quot; It looks like the function works even on other pages! We can also set up the function to extract the link to the next page, in case we want to scrape multiple pages of links. The link to the next page of features (an arrow at the bottom) is an a tag with class next. Let’s see if that’s specific enough to isolate the tag: next_page = xml_find_all(doc, &quot;//a[contains(@class, &#39;next&#39;)]&quot;) # OR html_nodes(doc, &quot;a.next&quot;) It looks like it is. We use contains here rather than = because it is common for the class attribute to have many parts. It only has one here, but using contains makes our code robust against changes in the future. We can now modify our parser function to return the link to the next page: parse_article_links = function(page) { # Get article URLs section = xml_find_all(page, &quot;//section[@id = &#39;blog-grid&#39;]&quot;) links = xml_find_all(section, &quot;//h2/a&quot;) urls = xml_attr(links, &quot;href&quot;) # Get next page URL next_page = xml_find_all(page, &quot;//a[contains(@class, &#39;next&#39;)]&quot;) next_url = xml_attr(next_page, &quot;href&quot;) # Using a list allows us to return two objects list(urls = urls, next_url = next_url) } Since our function gets URL for the next page, what happens on the last page? Looking at the last page in the browser, there is no link to the next page. Let’s see what our scraper function does: last_page = read_html(&quot;https://theaggie.org/features/page/180/&quot;) parse_article_links(last_page) ## $urls ## [1] &quot;https://theaggie.org/2008/03/14/daily-calendar/&quot; ## [2] &quot;https://theaggie.org/2008/03/14/facial-hair-takes-root-at-uc-davis/&quot; ## [3] &quot;https://theaggie.org/2008/03/13/daily-calendar/&quot; ## [4] &quot;https://theaggie.org/2008/03/12/corrections/&quot; ## [5] &quot;https://theaggie.org/2008/03/12/daily-calendar/&quot; ## [6] &quot;https://theaggie.org/2008/03/12/five-years-in-iraq-part-two/&quot; ## [7] &quot;https://theaggie.org/2008/03/11/daily-calendar/&quot; ## [8] &quot;https://theaggie.org/2008/03/11/deals-around-davis/&quot; ## [9] &quot;https://theaggie.org/2008/03/11/five-years-in-iraq-part-one/&quot; ## ## $next_url ## character(0) We get an empty character vector as the URL for the next page. This is because the xml_find_all function returns an empty node set for the next page URL, so there aren’t any href fields for xml_attr to extract. It’s convenient that the xml2 functions behave this way, but we could also add an if-statement to the function to check (and possibly return NA as the next URL in this case). Then the code becomes: parse_article_links = function(page) { # Get article URLs section = xml_find_all(page, &quot;//section[@id = &#39;blog-grid&#39;]&quot;) links = xml_find_all(section, &quot;//h2/a&quot;) urls = xml_attr(links, &quot;href&quot;) # Get next page URL next_page = xml_find_all(page, &quot;//a[contains(@class, &#39;next&#39;)]&quot;) if (length(next_page) == 0) { next_url = NA } else { next_url = xml_attr(next_page, &quot;href&quot;) } # Using a list allows us to return two objects list(urls = urls, next_url = next_url) } Now our function should work well even on the last page. If we want to scrape links to all of the articles in the features section, we can use our function in a loop: # NOTE: This code is likely to take a while to run, and is meant more for # reading than for you to run and try out. url = &quot;https://theaggie.org/features/&quot; article_urls = list() i = 1 # On the last page, the next URL will be `NA`. while (!is.na(url)) { # Download and parse the page. page = read_html(url) result = parse_article_links(page) # Save the article URLs in the `article_urls` list. The variable `i` is the # page number. article_urls[[i]] = result$url i = i + 1 # Set the URL to the next URL. url = result$next_url # Sleep for 1/30th of a second so that we never make more than 30 requests # per second. Sys.sleep(1/30) } Now we’ve got the basis for a simple scraper. 13.7 CSS Selectors Cascading style sheets (CSS) is a language used to control the formatting of an XML or HTML document. CSS selectors are the CSS way to write paths to elements. CSS selectors are more concise than XPath, so many people prefer them. Even if you prefer CSS selectors, it’s good to know XPath because CSS selectors are less flexible. Here’s the basic syntax of CSS selectors: CSS Description a tags a a &gt; b tags b directly beneath a a b tags b anywhere beneath a a, b tags a or b #hi tags with attribute id=\"hi\" .hi tags with attribute class that contains \"hi\" [foo=\"hi\"] tags with attribute foo=\"hi\" [foo*=\"hi\"] tags with attribute foo that contains \"hi\" If you want to learn more, CSS Diner is an interactive tutorial that covers the entire CSS selector language. In Firefox, you can get CSS selectors from the web developer tool. Right-click the tag you want a selector for and choose “Copy Unique Selector.” Beware that the selectors Firefox generates are often too specific to be useful for anything beyond the simplest web scrapers. The rvest package uses CSS selectors by default. Behind the scenes, the package translates these into XPath and passes them to xml2. Here are a few examples of CSS selectors, using rvest’s html_nodes function: html = r&quot;( &lt;html&gt; &lt;head&gt; &lt;title&gt;This is the page title!&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;This is a header!&lt;/h1&gt; &lt;p&gt;This is a paragraph. &lt;a href=&quot;http://www.r-project.org/&quot;&gt;Here&#39;s a website!&lt;/a&gt; &lt;/p&gt; &lt;p id=&quot;hello&quot;&gt;This is another paragraph.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; )&quot; doc = read_html(html) # Get all p elements html_nodes(doc, &quot;p&quot;) ## {xml_nodeset (2)} ## [1] &lt;p&gt;This is a paragraph.\\n &lt;a href=&quot;http://www.r-project.org/&quot;&gt;Here&#39;s ... ## [2] &lt;p id=&quot;hello&quot;&gt;This is another paragraph.&lt;/p&gt; # Get all links html_nodes(doc, &quot;a&quot;) ## {xml_nodeset (1)} ## [1] &lt;a href=&quot;http://www.r-project.org/&quot;&gt;Here&#39;s a website!&lt;/a&gt; # Get all tags with id=&quot;hello&quot; html_nodes(doc, &quot;#hello&quot;) ## {xml_nodeset (1)} ## [1] &lt;p id=&quot;hello&quot;&gt;This is another paragraph.&lt;/p&gt; "],["optical-character-recognition.html", "14 Optical Character Recognition 14.1 Loading Page Images 14.2 Running OCR 14.3 Accuracy 14.4 Unreadable Text", " 14 Optical Character Recognition Much of the data we’ve used in the course thus far has been born-digital. That is, we’ve used data that originates from a digital source and does not exist elsewhere in some other form. Think back, for example, to the lecture on strings in R: your homework required you to type text directly into RStudio, manipulate it, and print it to screen. But millions, even billions, of data-rich documents do not originate from digital sources. The United States Census, for example, dates back to 1790; we still have these records and could go study them to get a sense of what the population was like hundreds of years ago. Likewise, printing and publishing far precedes the advent of computers; much of the literary record is still bound up between the covers books or stowed away in archives. Computers, however, can’t read the way we read, so if we wanted to use digital methods to analyze such materials, we’d need to convert them into a computationally tractable form. How do we do so? One way would be to transcribe documents by hand, either by typing out plaintext versions with word processing software or by using other data entry methods like keypunching to record the information those documents contain. Amazon’s Mechanical Turk service is an example of this kind of data entry. It’s also worth noting that, for much of the history of computing, data entry was highly gendered and considered to be “dumb,” secretarial work that young women would perform. Much of the divisions between “cool” coding and computational grunt work that, in a broad, cultural sense, continue to inform how we think about programming, and indeed who gets to program, stem from such perceptions. In spite of (or perhaps because of) such perceptions, huge amounts of data owe their existence to manual data entry. That said, the process itself is expensive, time consuming, error-prone, and, well, dull. Optical character recognition, or OCR, is an attempt to offload the work of digitization onto computers. Speaking in a general sense, this process ingests images of print pages (such as those available on Google Books or HathiTrust), applies various preprocessing procedures to those images to make them a bit easier to read, and then scans through them, trying to match the features it finds with a “vocabulary” of text elements it keeps as a point of reference. When it makes a match, OCR records a character and enters it into a text buffer (a temporary data store). Oftentimes this buffer also includes formatting data for spaces, new lines, paragraphs, and so on. When OCR is finished, it outputs its matches as a data object, which you can then further manipulate or analyze using other code. 14.1 Loading Page Images OCR “reads” by tracking pixel variations across page images. This means every page you want to digitize must be converted into an image format. For the purposes of introducing you to OCR, we won’t go through the process of creating these images from scratch; instead, we’ll be using ready-made examples. The most common page image formats you’ll encounter are pdf and png. They’re lightweight, portable, and usually retain the image quality OCR software needs to find text. The pdftools package is good for working with these files: # install.packages(&quot;pdftools&quot;) library(pdftools) ## Using poppler version 0.73.0 Once you’ve downloaded/installed it, you can load a pdf into RStudio from your computer by entering its filesystem location as a string and assigning that string to a variable, like so: pdf &lt;- &quot;./pdf_sample.pdf&quot; Note that we haven’t used a special load function, like read.csv() or readRDS(). pdftools will grab this file from its location and load it properly when you run a process on it. (You can also just write the string out in whatever function you want to call, but we’ll keep our pdf location in a variable for the sake of clarity.) The same method works with web addresses. We’ll be using web material. First, write out an address and assign it to a variable. pdf &lt;- &quot;https://datalab.ucdavis.edu/adventures-in-datascience/pdf_sample.pdf&quot; Some pdf files will have text data already encoded into them. This is especially the case if someone made a file with word processing software (like when you write a paper in Word and email a pdf to your TA or professor). You can check whether a pdf has text data with pdf_text(). Assign this function’s output to a variable and print it to screen with cat(), like so: text_data &lt;- pdf_text(pdf) cat(text_data) ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. See how the RStudio terminal recreates the original formatting from the pdf. If you were to use print() on the text output, you’d see all the line breaks and spaces pdf_text() created to match its output with the file. This re-creation would be even more apparent if you were to save the output to a new file with write(). Doing so would produce a close, plaintext approximation of the original pdf. You can also process multi-page pdf files with pdf_text(), with more or less success. It can transcribe whole books and will keep them in a single text buffer, which you can then assign to a variable or save to a file. Keep in mind, however, that if your pdf files have headers, footers, page numbers, chapter breaks, or other such paratextual information, pdf_text() will pick this up and include it in its output. A later lecture in the course will discuss how to go about dealing with this extra data. If, when you run pdf_text(), you find that your file already contains text data, you’re set! There’s no need to perform OCR and you can immediately start working with your data. However, if you run the function and find that it outputs a blank character string, you’ll need to OCR it. The next section shows you how. 14.2 Running OCR First, you’ll need to download/install another package, tesseract, which complements pdftools. The latter only loads/reads pdfs, whereas tesseract actually performs OCR. Download/install tesseract: # install.packages(&quot;tesseract&quot;) library(tesseract) And assign a new pdf to a new variable: new_pdf &lt;- &quot;https://jeroen.github.io/images/ocrscan.pdf&quot; To run OCR on this pdf, use the following: ocr_output &lt;- ocr(new_pdf) ## Converting page 1 to fileb17260387640ocrscan_1.png... done! Print the output to screen with cat() and see if the process worked: cat(ocr_output) ## | SAPORS LANE - BOOLE - DORSET - BH25 8 ER ## TELEPHONE BOOLE (945 13) 51617 - TELEX 123456 ## ## Our Ref. 350/PJC/EAC 18th January, 1972. ## Dr. P.N. Cundall, ## Mining Surveys Ltd., ## Holroyd Road, ## Reading, ## Berks. ## Dear Pete, ## ## Permit me to introduce you to the facility of facsimile ## transmission. ## ## In facsimile a photocell is caused to perform a raster scan over ## ## the subject copy. The variations of print density on the document ## cause the photocell to generate an analogous electrical video signal. ## This signal is used to modulate a carrier, which is transmitted to a ## remote destination over a radio or cable communications link. ## ## At the remote terminal, demodulation reconstructs the video ## signal, which is used to modulate the density of print produced by a ## printing device. This device is scanning in a raster scan synchronised ## with that at the transmitting terminal. As a result, a facsimile ## copy of the subject document is produced. ## ## Probably you have uses for this facility in your organisation. ## ## Yours sincerely, ## 44, f ## P.J. CROSS ## Group Leader - Facsimile Research ## Registered in England: No. 2038 ## No. 1 Registered Office: GO Vicara Lane, Ilford. Eseex. Voila! You’ve just digitized text. The formatting is a little off, but things look good overall. And most importantly, it looks like everything has been transcribed correctly. As you ran this process, you might’ve noticed that a new png file briefly appeared on your computer. This is because tesseract converts pdfs to png under the hood as part of its pre-processing work and then silently deletes that png when it outputs its matches. If you have a collection of pdf files that you’d like to OCR, it can sometimes be faster and less memory intensive to convert them all to png first. You can perform this conversion like so: png &lt;- pdf_convert(new_pdf, format=&quot;png&quot;, filenames=&quot;./img/png_example.png&quot;) ## Converting page 1 to ./img/png_example.png... done! In addition to making a png object in your RStudio environment, pdf_convert() will also save that file directly to your working directory. Imagine, for example, how you might put a vector of pdf files through a for loop and save them to a directory, where they can be stored until you’re ready to OCR them. pdfs &lt;- c(&quot;list.pdf&quot;, &quot;of.pdf&quot;, &quot;files.pdf&quot;, &quot;to.pdf&quot;, &quot;convert.pdf&quot;) outfiles &lt;- c(&quot;list.png&quot;, &quot;of.png&quot;, &quot;files.png&quot;, &quot;to.png&quot;, &quot;convert.png&quot;) for (i in 1:length(pdfs)) { pdf_convert(pdfs[i], format=&quot;png&quot;, filenames=outfiles[i]) } ocr() can work with a number of different image types. It takes pngs in the same way as it takes pdfs: png_ocr_output &lt;- ocr(png) 14.3 Accuracy If you cat() the output from the above png file, you might notice that the text is messier than it was when we used pdf_text_ocr(). cat(png_ocr_output) ## THE SLEREXE COMPANY LIMITED ## SAPORS LANE . BOOLE - DORSET . BH25 8ER ## ‘aurrmowe noun (945 18) SU6I7 - run 123486 ## Our Ref. 350/PIC/EAC 18eh January, 1972. ## De. P.M. Cundall, ## Mining Surveys Ltd., ## Holroyd Road, ## Reading, ## Berks. ## Dear Pete, ## ## Permit ne to introduce you to the facility of facsinile ## transmission. ## ## In facsimile @ photocell is caused to perform a raster scan over ## the subject copy, The variations of print density on the docunent ## cause the photocell to generate an analogous electrical video signal. ## ‘This signal is used to modulate a carrier, which is transmitted to a ## remote destination over @ radio or cable Communications Link. ## ## At che remote tersinal, demodulation reconstructs the video ## signal, which is used to modulate the density of print produced by a ## printing device. This device is scanning in a raster scan synchronised ## ith chac at the transmitting terminal. As a result, a facsimile ## copy of the subject docusent is produced. ## ## Probably you have uses for this facility in your organisation. ## ## Yours sincerely, ## PJ. CROSS ## Group Leader ~ Facsimile Research This doesn’t have to do with the png file format per se but rather with the way we created our file. If you open it, you’ll see that it’s quite blurry, which has made it harder for ocr() to match the text it represents: This blurriness is because pdf_convert() defaults its conversions to 72 dpi, or dots per inch. Dpi is a measure of image resolution (originally from inkjet printing), which describes the amount of pixels your computer uses to create images. More pixels means higher image resolution, though this comes with a trade off: images with a high dpi are also bigger and take up more space on your computer. Usually, a dpi of 150 is sufficient for most OCR jobs, especially if your documents were printed with technologies like typewriters, dot matrix printers, and so on and if they feature fairly legible typefaces (Times New Roman, for example). A dpi of 300, however, is ideal. You can set the dpi in pdf_convert() by adding a dpi argument in the function: hi_res_png &lt;- pdf_convert(new_pdf, format=&quot;png&quot;, dpi=150, filenames=&quot;./img/hi_res_png_example.png&quot;) ## Converting page 1 to ./img/hi_res_png_example.png... done! Another function, ocr_data()outputs a data frame that contains all of the words tesseract found when it scanned through your image, along with a column of confidence scores. These scores, which range from 0-100, provide valuable information about how well the OCR process has performed, which in turn may tell you whether you need to modify your pdf or png files further before OCRing them (more on this below). Generally, you can trust scores of 93 and above. To get confidence scores for an OCR job, call ocr_data() and subset the confidence column, like so: ocr_data &lt;- ocr_data(hi_res_png) confidence_scores &lt;- ocr_data$confidence print(confidence_scores) ## [1] 92.326187 93.291992 92.390686 91.292068 90.040321 93.285721 89.960922 ## [8] 43.996544 43.996544 82.938339 93.216827 92.747429 76.725304 92.798645 ## [15] 92.798645 94.139832 96.169128 96.553284 91.130814 91.130814 95.643044 ## [22] 95.643044 95.283821 91.192238 89.044609 87.802193 96.233360 93.276566 ## [29] 92.992844 95.952171 96.681137 96.284302 95.152283 95.977455 96.308533 ## [36] 95.692993 95.692993 95.987144 96.534164 96.897186 95.809143 96.897415 ## [43] 96.315361 96.617821 96.508087 94.390121 95.978737 96.522850 95.214600 ## [50] 95.642807 96.655434 96.282211 96.282211 96.361366 95.597168 95.981224 ## [57] 96.211266 95.877457 8.440552 96.515381 95.978645 95.955009 95.685791 ## [64] 96.777512 96.954750 96.224609 96.495888 96.877327 96.955742 96.350052 ## [71] 96.784325 96.784325 96.262688 96.271156 96.284409 96.690887 95.752106 ## [78] 96.577225 96.782013 95.615807 96.313568 96.293747 96.120255 96.120255 ## [85] 96.640419 95.863403 95.929253 96.522018 96.398582 96.446907 95.803619 ## [92] 95.803619 95.204765 96.370934 96.169952 95.475868 95.475868 96.492149 ## [99] 96.108894 96.828568 96.650681 94.557724 94.658371 94.658371 96.770195 ## [106] 96.225060 96.670525 96.354980 96.906342 96.303818 96.014244 96.497437 ## [113] 96.302750 95.792542 95.792542 95.861221 96.304672 96.203583 96.420517 ## [120] 96.625023 96.626266 96.882683 91.595039 96.384834 95.978401 95.988167 ## [127] 96.459465 96.473129 96.613380 96.464584 96.782417 96.463692 96.355392 ## [134] 96.262489 95.908119 96.471008 95.633606 96.694962 96.391228 94.782692 ## [141] 95.936348 96.104546 96.003822 95.396400 96.081680 96.895088 96.446304 ## [148] 95.649055 96.735336 96.161446 96.006187 96.169670 96.660339 96.984940 ## [155] 96.720856 96.825615 96.421089 96.421089 96.121315 96.758072 96.483368 ## [162] 96.435646 96.783485 96.674332 43.258137 81.451591 50.273232 96.291702 ## [169] 96.013054 90.972252 90.843719 96.506401 96.685356 95.623764 94.061028 ## [176] 90.181320 96.139618 92.959770 94.400261 85.483757 70.593071 96.380119 ## [183] 66.150887 18.099655 78.519165 83.035973 31.735725 The mean is a good indicator of the overall OCR quality: confidence_mean &lt;- mean(confidence_scores) print(confidence_mean) ## [1] 92.51906 Looks pretty good, though there were a few low scores that dragged the score down a bit. Let’s look at the median: confidence_median &lt;- median(confidence_scores) print(confidence_median) ## [1] 96.12025 We can work with that! If we want to check our output a bit more closely, we can do two things. First, we can look directly at ocr_data and compare, row by row, a given word and its confidence score. print(ocr_data) ## word confidence bbox ## 1 SAPORS 92.326187 422,194,497,208 ## 2 LANE 93.291992 508,194,560,208 ## 3 - 92.390686 570,203,575,205 ## 4 BOOLE 91.292068 585,193,651,208 ## 5 - 90.040321 661,202,666,205 ## 6 DORSET 93.285721 676,193,755,208 ## 7 - 89.960922 764,203,769,205 ## 8 BH 43.996544 780,193,808,208 ## 9 25 43.996544 807,189,831,217 ## 10 8ER 82.938339 842,193,883,208 ## 11 TELEPHONE 93.216827 449,232,534,243 ## 12 BOOLE 92.747429 544,232,589,243 ## 13 (94513) 76.725304 600,229,664,246 ## 14 51617 92.798645 675,229,719,244 ## 15 - 92.798645 730,237,735,240 ## 16 TELEX 94.139832 746,232,792,243 ## 17 123456 96.169128 804,228,857,244 ## 18 Our 96.553284 211,392,246,408 ## 19 Ref. 91.130814 261,391,306,407 ## 20 350/PJC/EAC 91.130814 325,389,459,409 ## 21 18th 95.643044 863,389,910,405 ## 22 January, 95.643044 924,389,1020,408 ## 23 1972. 95.283821 1038,388,1095,405 ## 24 Dr. 91.192238 212,492,244,508 ## 25 P.N. 89.044609 262,492,307,508 ## 26 Cundall, 87.802193 325,491,419,510 ## 27 Mining 96.233360 212,516,286,538 ## 28 Surveys 93.276566 301,518,383,536 ## 29 Ltd., 92.992844 398,516,457,535 ## 30 Holroyd 95.952171 212,543,298,562 ## 31 Road, 96.681137 313,542,369,561 ## 32 Reading, 96.284302 213,566,308,587 ## 33 Berks. 95.152283 213,593,282,608 ## 34 Dear 95.977455 213,668,261,683 ## 35 Pete, 96.308533 275,668,333,686 ## 36 Permit 95.692993 276,715,348,732 ## 37 me 95.692993 362,721,386,732 ## 38 to 95.987144 402,719,423,732 ## 39 introduce 96.534164 439,715,548,731 ## 40 you 96.897186 562,720,599,735 ## 41 to 95.809143 614,718,636,731 ## 42 the 96.897415 652,716,685,731 ## 43 facility 96.315361 702,714,799,735 ## 44 of 96.617821 813,715,836,731 ## 45 facsimile 96.508087 852,713,961,731 ## 46 transmission. 94.390121 215,740,371,757 ## 47 In 95.978737 278,793,300,807 ## 48 facsimile 96.522850 316,790,424,807 ## 49 a 95.214600 439,796,450,806 ## 50 photocell 95.642807 463,791,573,811 ## 51 is 96.655434 589,790,611,806 ## 52 caused 96.282211 627,791,700,807 ## 53 to 96.282211 714,793,736,806 ## 54 perform 96.361366 751,790,839,810 ## 55 a 95.597168 852,794,862,806 ## 56 raster 95.981224 876,793,950,806 ## 57 scan 96.211266 965,794,1013,806 ## 58 over 95.877457 1026,793,1075,805 ## 59 : 8.440552 16,824,18,826 ## 60 the 96.515381 215,817,249,832 ## 61 subject 95.978645 265,815,349,836 ## 62 copy. 95.955009 364,820,421,835 ## 63 The 95.685791 451,816,486,831 ## 64 variations 96.777512 501,814,623,831 ## 65 of 96.954750 639,815,661,831 ## 66 print 96.224609 675,814,736,835 ## 67 density 96.495888 752,814,837,834 ## 68 on 96.877327 851,819,875,830 ## 69 the 96.955742 890,814,924,830 ## 70 document 96.350052 939,814,1037,830 ## 71 cause 96.784325 215,846,275,857 ## 72 the 96.784325 290,841,324,857 ## 73 photocell 96.262688 338,841,449,861 ## 74 to 96.271156 465,844,487,857 ## 75 generate 96.284409 502,844,599,860 ## 76 an 96.690887 614,845,637,857 ## 77 analogous 95.752106 652,841,761,861 ## 78 electrical 96.577225 777,839,898,856 ## 79 video 96.782013 914,838,975,856 ## 80 signal. 95.615807 989,838,1071,859 ## 81 This 96.313568 215,865,261,881 ## 82 signal 96.293747 278,865,349,886 ## 83 is 96.120255 365,865,386,881 ## 84 used 96.120255 402,866,450,882 ## 85 to 96.640419 465,868,487,881 ## 86 modulate 95.863403 501,865,599,881 ## 87 a 95.929253 615,870,626,881 ## 88 carrier, 96.522018 639,865,735,884 ## 89 which 96.398582 750,864,812,881 ## 90 is 96.446907 828,864,849,881 ## 91 transmitted 95.803619 865,863,1000,881 ## 92 to 95.803619 1015,867,1037,880 ## 93 a 95.204765 1052,868,1063,879 ## 94 remote 96.370934 215,894,287,907 ## 95 destination 96.169952 302,890,438,907 ## 96 over 95.475868 451,895,500,906 ## 97 a 95.475868 514,895,525,906 ## 98 radio 96.492149 539,889,599,906 ## 99 or 96.108894 614,895,637,906 ## 100 cable 96.828568 653,891,712,906 ## 101 communications 96.650681 727,888,899,906 ## 102 link. 94.557724 915,888,972,905 ## 103 At 94.658371 277,941,300,956 ## 104 the 94.658371 316,941,350,956 ## 105 remote 96.770195 365,943,437,956 ## 106 terminal, 96.225060 453,939,560,962 ## 107 demodulation 96.670525 577,939,725,956 ## 108 reconstructs 96.354980 740,942,886,956 ## 109 the 96.906342 903,940,937,956 ## 110 video 96.303818 951,938,1013,955 ## 111 signal, 96.014244 215,964,297,986 ## 112 which 96.497437 314,964,375,981 ## 113 is 96.302750 390,964,411,981 ## 114 used 95.792542 427,965,475,981 ## 115 to 95.792542 490,968,512,981 ## 116 modulate 95.861221 526,965,625,981 ## 117 the 96.304672 641,965,675,981 ## 118 density 96.203583 690,964,775,984 ## 119 of 96.420517 789,964,812,980 ## 120 print 96.625023 826,963,887,984 ## 121 produced 96.626266 901,964,1001,984 ## 122 by 96.882683 1013,964,1038,983 ## 123 a 91.595039 1052,968,1063,979 ## 124 printing 96.384834 215,989,314,1010 ## 125 device. 95.978401 327,989,410,1006 ## 126 This 95.988167 440,989,486,1006 ## 127 device 96.459465 502,989,575,1006 ## 128 is 96.473129 590,989,611,1006 ## 129 scanning 96.613380 628,989,726,1010 ## 130 in 96.464584 741,989,763,1005 ## 131 a 96.782417 777,994,788,1005 ## 132 raster 96.463692 802,992,876,1005 ## 133 scan 96.355392 890,994,938,1005 ## 134 synchronised 96.262489 953,988,1101,1009 ## 135 with 95.908119 213,1015,263,1031 ## 136 that 96.471008 278,1015,325,1031 ## 137 at 95.633606 340,1018,362,1031 ## 138 the 96.694962 379,1015,413,1031 ## 139 transmitting 96.391228 428,1014,576,1035 ## 140 terminal. 94.782692 591,1014,697,1031 ## 141 As 95.936348 727,1015,749,1031 ## 142 a 96.104546 765,1019,775,1030 ## 143 result, 96.003822 789,1015,873,1034 ## 144 a 95.396400 890,1019,901,1030 ## 145 facsimile 96.081680 915,1012,1026,1030 ## 146 copy 96.895088 215,1045,263,1060 ## 147 of 96.446304 278,1040,300,1056 ## 148 the 95.649055 317,1041,350,1056 ## 149 subject 96.735336 365,1039,450,1060 ## 150 document 96.161446 465,1040,562,1056 ## 151 is 96.006187 578,1039,599,1055 ## 152 produced. 96.169670 614,1040,722,1060 ## 153 Probably 96.660339 278,1091,375,1110 ## 154 you 96.984940 389,1095,427,1110 ## 155 have 96.720856 439,1091,488,1106 ## 156 uses 96.825615 503,1094,550,1106 ## 157 for 96.421089 566,1090,601,1106 ## 158 this 96.421089 616,1089,662,1106 ## 159 facility 96.121315 678,1088,775,1110 ## 160 in 96.758072 792,1088,814,1105 ## 161 your 96.483368 827,1094,876,1110 ## 162 organisation. 96.435646 890,1087,1049,1110 ## 163 Yours 96.783485 678,1141,737,1156 ## 164 sincerely, 96.674332 753,1139,874,1160 ## 165 Td, 43.258137 688,1198,778,1276 ## 166 / 81.451591 808,1209,823,1266 ## 167 P.J. 50.273232 678,1304,724,1319 ## 168 CROSS 96.291702 742,1304,802,1319 ## 169 Group 96.013054 678,1329,739,1347 ## 170 Leader 90.972252 753,1328,827,1344 ## 171 - 90.843719 840,1334,852,1338 ## 172 Facsimile 96.506401 867,1326,978,1344 ## 173 Research 96.685356 992,1327,1092,1344 ## 174 Registered 95.623764 521,1574,600,1587 ## 175 in 94.061028 607,1574,621,1584 ## 176 England: 90.181320 629,1573,695,1586 ## 177 No. 96.139618 725,1574,749,1584 ## 178 2038 92.959770 756,1574,793,1584 ## 179 No. 94.400261 72,1597,99,1610 ## 180 1 85.483757 110,1597,114,1609 ## 181 Registered 70.593071 457,1592,535,1605 ## 182 Office: 96.380119 545,1592,595,1603 ## 183 @O 66.150887 627,1592,644,1602 ## 184 Vicars 18.099655 651,1592,700,1602 ## 185 Lane, 78.519165 723,1592,763,1605 ## 186 Ilford. 83.035973 771,1592,817,1602 ## 187 Essex, 31.735725 825,1592,873,1603 That’s a lot of information though. Something a little more sparse might be better. We can use base R’s table() function to count the number of times unique words appear in the OCR data. We do this with the word column in our ocr_data variable from above: ocr_vocabulary &lt;- table(ocr_data$word) ocr_vocabulary &lt;- as.data.frame(ocr_vocabulary) Let’s look at the first 30 words: head(ocr_vocabulary, 30) ## Var1 Freq ## 1 - 5 ## 2 : 1 ## 3 (94513) 1 ## 4 @O 1 ## 5 / 1 ## 6 1 1 ## 7 123456 1 ## 8 18th 1 ## 9 1972. 1 ## 10 2038 1 ## 11 25 1 ## 12 350/PJC/EAC 1 ## 13 51617 1 ## 14 8ER 1 ## 15 a 9 ## 16 an 1 ## 17 analogous 1 ## 18 As 1 ## 19 at 1 ## 20 At 1 ## 21 Berks. 1 ## 22 BH 1 ## 23 BOOLE 2 ## 24 by 1 ## 25 cable 1 ## 26 carrier, 1 ## 27 cause 1 ## 28 caused 1 ## 29 communications 1 ## 30 copy 1 This representation makes it easy to spot errors like discrepancies in spelling. We could correct those either manually or with string matching. One way to further examine this table is to look for words that only appear once or twice in the output; among such entries you’ll often find misspellings. The table does, however, have its limitations. Looking at this data can quickly become overwhelming if you send in too much text. Additionally, notice that punctuation “sticks” to words and that uppercase and lowercase variants of words are counted separately, rather than together. These quirks are fine, useful even, if we’re just spot-checking for errors, but we’d need to further clean this data if we wanted to use it in computational text analysis. A later lecture will discuss other methods that we can use to clean text. When working in a data-forensic mode with page images, it’s a good idea to pull a few files at random and run them through ocr_data() to see what you’re working with. OCR accuracy is often wholly reliant on the quality of the page images, and most of the work that goes into digitizing text involves properly preparing those images for OCR. Adjustments include making sure images are converted to black and white, increasing image contrast and brightness, increasing dpi, and rotating images so that their text is more or less horizontal. tesseract performs some of these tasks itself, but you can also do them ahead of time and often you’ll have more control over quality this way. The tesseract documentation goes into detail about what you can do to improve accuracy before even opening RStudio; we can’t cover this in depth, but keep the resource in mind as you work with this type of material. And remember: the only way to completely trust your accuracy is to go through the OCR output yourself. It’s a very common thing to have to make small tweaks to output. In this sense, we haven’t quite left the era of hand transcription. 14.4 Unreadable Text All that said, these various strategies for improving accuracy will only get you so far if your page images are composed in a way OCR just can’t read. OCR systems contain a lot of in-built assumptions about what “normal” text is, and they are incredibly brittle when they encounter text that diverges from that norm. Early systems, for example, required documents to be printed with special, machine-readable typefaces; texts that contained anything other than this design couldn’t be read. Now, OCR is much better at handling a variety of text styling, but systems still struggle with old print materials like blackletter. Running: ballad &lt;- &quot;https://ebba.english.ucsb.edu/images/cache/hunt_1_18305_2448x2448.jpg&quot; ballad_out &lt;- ocr(ballad) Produces: cat(ballad_out) ## CAditeription of Qoptons falcehoo ) ## nf Doge thyze, and of bis fatall farewell, ## ache fatal fineof Sratteurs lees : ## \\ 4Bp gulice due, deferuyng foe. . : ## Flate (alas) the great tntruth = Whe Crane wolvetipedpto the funne, ort, bis futtrpug long (be fare) 7 ## Mf Wrattours, hotw ttfped % heavdit once of olde: _ TH pll pay bis foes atlak: : : ## WU do lit toknolv, hal herex-aae And with the kyng ofbpresdinitrine his metcpe moued once alway, : ## Wotv late allegeance fed. wp Fame, J beardittolve ~~ He hall them quight out caté j ## @ 3f Riucrsrage againt the Sea. Qnodotwac ihe wolve not fal fre ne, WH ith fentence int fo2 their bntruth, 7 ## And {well with fondeine rayne s Wut higher pid nidmoury ? And boeakpng of his tupll: : ## Wolw gla ave they to fall agapne, il patt her veach (faith woe repo2te) Che fruits of their fedictous feds, E ## Anodtrace their wwonted traine? Shame madea backe recour She barnes of earth thail fpll. ## B) 3f fire by force wolde forge the fall 3 touch no Armes herein at all, Wheterfoules God wot foze clogd w crime ## Df any fumptuoufe place, sButthelvafable wpe: _ and their potteritic ## A) 3f water flods byd him leaueof, Wi hote mo2all fence doth reper MWelpotted fore with theirabule, ## dis flames be wyll difgrace. DF clpmers bye the guyle. And ttand by their folie. ## 3f Goo command the wyrdes to ceale, Who buyloes a boule of many &gt; heir linpngs left their namea thame, ## ‘ Ibis blattes are lapo full low: andlatth not ground yorker heir dedes tith popfon (ped: ; ## H 3f God command the eas to calmes 4But doth ertortethe groundt ig, Sbeirdeathesatvagefor wantofgrace ## i hey tyll notrageo2 flow. _ Wis builopng can not dure, SUbeir honours quite ts dead, 4 ## A all thinges at Gods commandemet be, @€ Who fekes furmifing to dip heir leth tofedethe kytes andcrowes ## f afbetheirflateregarde: : a Kuler fentby GOD: aCheir armes a maze fo2 men; ie ## H Audnoman lines whole veftinie 4s fubiec {ure, deuoide of grace. Sbeir guerdon as eramples are Bs ## wy hint ts bupeeparde. he caufe of bis olune rod. Co Dafh dolte Dunces den, ke ## ) iSut when a mait fo2lakes thethip, - A byave that Wyll bernett defple Whol vp pour fnouts pou fluggih forte Q ## s ° Androlwles in wallowing wanes; ‘By right ould lofe a wyng: Poumumming malkpng route : : ## And of bis valuntarte wyil, Qua thents thee no fying fouls €rtoll pour erclantations vp, ## i Wis one god bap depraues + {But How as other thyng. 4Baals chapletnes,champions ffoute. &amp; ## HM olw hal be bopetolcape the gulfe 2 Anobe that lofeth all at games, Make fate fo2 pardons, papitts boaue, ii ## : Wow Mhal be thinke todeale 2 2 {pendes infotule ercefle: Foz traitours indulgence; o ## A iolw thal his fanfic baing him found Andhopesbybhapsto bealebisharme, . fend out fome purgatorte (craps, ; ## i Zo Safties tore with faple 2 Mutt deivke of Beare diftrefte. Home wWulls with peter pence. FE ## : otw (hall his fraight tn fine fuccede 2 Go fpeake of b2pdles to reftrapne D fwarine of D20nes, how dare pe fol Bi ## 4 Alas what thall be gapne 2 aLhis wylfull waptward cretpe ¢ With labourpng Wes contend if ## # Uibatfeare by foes do makebimquake bey care not foz the bake of God, ou fought forhonte fromthe hiues, ; ## j Wow ofte fubieceto payne 2 Do princes, men bntrue. Wut gall pou foundinend. F ## ] ioin fundzie tintes in Dangers den Co cuntrye, caufers of much woe, hele walpes do watk, their lings beout F ## : 4s th2owne theman onivpfe 2 Sp eotal freeones, afall: Whee {pight wyll not auaple ; ## F) bo climes withouten holde on bye, ano Gtpeir one eftates, altpng, BWhele Peacocks proudearenakedlette fF ## z sBeware, ¥ hint adutse. a others, arpeas gall. Of their oifplayed tayple. ‘ ## S Alifuchas trut to falfe contracs, D Looe, how long thele Liserds lurkt, hele Lurkype cocks tu cullour red, , ## | D2 fecret harmes confpire? GuvG O D&gt; bow great a whple _ Solong banelurkt alofe : é ## f iBefure, with Portonstbey Haltafte Were they in Handwith fetgnedbarts Whe Beare (althongh but foty of fote) 4 ## 7 A right deferucd hire. ACbeir cuntrye to defple 2 Hath pluc his wynages by peofe. : ## &amp; Whey can not lwike for better (pede, Holv did they frame theirfurniture? . Whe Done her borotyenlighthath lol, fe ## o Soa death fo2luch to fells Iolv fit they made thet toles ¢ She wapnedas we fee : ; ## F @odgvant the tuftice of the wozlde Wow Symon fought our engipt) Wrote Who hoped by bap of othersharmes, E ## ie qput by the papnes of hell. Mo hapng to Romaine (coles. © full Bone once to be. B ## &amp; F02(uchapentiuerale it is, Holy Simon Magus playd his parte, SCbe Lyon (uffrea long the wuil, 3 ## : hat Cnglith harts did dare ain Wabilon balwde pidrage: Wis noble mynd to trpe: i ## @ io pale the boundes of duties latuc, Iolv walan bulles begon to bell, Wntpll the wWull was rageyng tod, A ## i @2 of theircuntrie care. How Judas fought bis wage. And from bis take dia hye. vi ## # Andmercie bath fo long releat Iolw Jannes and Jamb2es od abpde acben time ft twas to bid him tap : ## | Dffendours (God doth knoi) aie bount of boatneficke acs, perforce, bis hoanes to cut + u ## 4 Gnd bountic of our curteous Nuene oww Dathan, Chore, Abiram femd Andmake him leaue bis rageing tanes 4 ## ‘ ao long hath fpared ber foe. Xo path our Moyles facs. Au fcilence to be put. ° ## By But God, whole grace infpiresherbarte, Wow womaine marchant feta fret Andall the calues of Wafan kynd i ## t WA pil not abyoe the fpight iHfs pardons beaue a fale, Are weaned from their with ; if ## | Of Rebels rage, who rampeto reach Jor alwapes fomeagaint the ruth Whe Wircan Wigers tamed now, i‘ ## : From ber, her title quight. WMlolde d2eame afencelestale. __ Lemathon eates no fith. i ## A Although the flowe in pitifull seale, Gods bicar from bis god receaued 4Bebholde befoze pour balefullepes i ## L nd loueth to fucke no biwd : Gbekeyestolofcandbynds She purchace of pour parte, a ## fH 3oet Goda caueat iyll her lend sBaals chapleinthoght bo fire wolk” ie Suruep pour forefnefozrowful fight 4 ## a © appeale thole Wipers made, Such was bis pagan mynd. With fighes of bubble barte, 4 ## B) aman that fees bts bouteon fire, Godiorre howhits thetertthete ts Lament thelackeot pour alies : ## ol WH pli (eke to quench the fame: That faith fuch men tall he _ Religious rebells all: 2 ## i) Gls from the fpople fomeparteconucy, Au their religion hot nozcolne ABewwepe that pli fuccelle of yours, : ## i Cis (eke the heate to tame, Of much baviette. Come curfe pour fodcine fall. “i ## Ht who feea penthoule wether beate, And Cundey (oats of fects farts And when pe haue pour guilesoutfought # ## : Ano beares a botttroule ipndes - iuifion thall appeare ¢ , And all pour craft app2oued, 3 ## iM sButhedefull faletie of himfelte, Againk thefather, fonnethe —7UL&gt; ppeccauimus thall be pour fong ; ## : WA pil force him fuccour fynde 2 Gaink mother, saughter +k our ground tpozke is remoued, : ## abe pitifull pactent Pelican, 4s {t not come to palfe trot pra? Andlokehow Poztons(pedtheivwills &amp; ## 4 er blwd although He teas aged, battards (urethep bes _ Euen fo thetr fet all baue, bs ## i ject inyll he femeher dateto end, Cabo our gad mother Nueneo - : po better let thenthope to gayne a; ## : } @Da2tare her poung be fpen. &amp; lt ied see a “a $ut gallowes without graue. ## Ll she Cagle fpngesbher pong onesdowne Canod is bengeance long retat See : ## | What fight a at erate? eatbere bis oy feruants tele cFINIS. pee olan. a ## i) winpertec folwles the aeadly bates, Aniurioute fights of godlede men, ey : ## ‘ And rightly Cuch mitble, ‘ Moho turne as doth a whele 2 A Q eo CE OH iE ## £ ‘ ie ## a Ampzinted at London by Alerander Lacte, foz Henrie Ayskehams, divellpng at the fiqne 4 ## HC of the blacke Move, at the midole sprozth doze of Paules church. ¢ i ## i &#39; i (Note by the way that this example used a jpg file. ocr() can handle those too.) Even though every page is an “image” to OCR, OCR struggles with imagistic or unconventional page layouts, as well as inset graphics. Add to that sub-par scans of archival documents, as in the newspaper page below, and the output will contain way more errors than correct matches. newspaper &lt;- &quot;https://chroniclingamerica.loc.gov/data/batches/mimtptc_inkster_ver01/data/sn88063294/00340589130/1945120201/0599.pdf&quot; newspaper_ocr &lt;- ocr(newspaper) ## Converting page 1 to filef5983ab50c2a0599_1.png... done! Beautifully messy output results: cat(newspaper_ocr) ## ai . a ## 5 IE EI ## UNIVERSAL KEYBOARD i DVORAK KEYBOARD ## ne in o fu “ % Z x f= 7 ## ‘ . pA ‘ eee . . \\ “a fr J s a . ## . : , : og oF Sn ’ &lt; f &#39; » | mii } a a} ) ## &#39; © /\\ e MF gp O ES nn i ; ; \\2/\\4 &gt; 5 | ## | Ly FP eT a ## oh a 7 &lt; Pa “ : : E BNED Bye ee “4 = ete 2 2 s ms a m3 = tan&quot; ## ane SO a A os a &gt;) ny P\\GGYC YR + e222 ## . 1 f A P i . ae 5 iat sc dae, Sa on a ot ai , ,} Lr AT \\J CJ (JTtila ## Si a MR aie tS eR” Mi OS ## | ? S fis 7 eee. ae ie ate NS »)( * ) +702 ## : &gt; ~ ; sat 6 F S &gt; 2 me te oat hs F oy . ‘ ’ j 4 , } ## a er ae See 8 ake apa wei ## ff yee seer Ng a ‘ ## , fy Seg 33 . oe . Oy A ae . Base *) as ear , o*% ## 7 Jt grte&#39;y® Ne i . oe . &quot; “ 4 4 nd ; : . ’ vi ’ J , = ~ , ## ’ Be Sa i . SEH, . ## ff See ‘= Ps sf , ed Bad fl N . 4 ## a MA . ’ 22 , ## (Rr, ; uC ## . rai / ¢: a m * , a ## &quot; y . 4 . -— ~~ ae ## i ° : a PA — 3 : eee 7 mer | . 3 ## ; ; neg a 7 a i ~ a ; # ## : q 5 3 : sale LSIGOE emt ‘ ## : ee » &lt;3 ‘ a ‘ ## ; 7 Base ; \\ , _ 4 . ; ## ee SS ty * , : ## : i ka ee Poy Les r . »* 7 ## 5S a ead _ ’ ; tA ## ! Bea ; &lt; - 4 ; ## ? Pe ee ta i 2B. ## es - + 2 Bes 186. iy | : &#39; ## os 4 &quot; . . . peas AY Be or &#39; ## ‘ ss , bgt AAI 7. ## } ; re er &amp; aes ¢ ; ## | fo | a anti | Oa . Ss ## Py . 3 . ne i. | | | ## ; . € “ &gt; a a J ) ## “@ a ~_ . MA jj j , ## J . . oo b tt - + ; ; | ## ¥ pe ‘ OMe? ) | ## - =&lt; | ~~ . 4 Pan Pe ° . ee Pr ee oe © abe “« ~—— meheeaethae @ ‘ ## $ &lt;4 a6 : a, a m — &#39; e . x 7 | ## : \\ NG - gy oe - 2 | 2 ## h =&gt;. ” ‘“ ) . ## &#39; &#39; ’ ; ™~, ~~ es i ro ys vv a ‘ { 20 : ## &#39; ~~ aS ee , | &quot; - \\ . ## : : . ## L , ‘~S —s icin j &gt; 2 } ## = | nee ‘ner Ne ## ——————————eV—V—ake eo ntl sant * 5 é. mm rr i en mmr ## ~s = . . . ; ## e “ , Standard Typewriter Key ## : “¢ board (Left) Is the Same ## &amp; &amp; Now as When Christopher ## Sholes Invented the First ## s ————- ites &#39; = ## ) kt Typewriter (Center) in 1475, ## ; — : But It May Give Way to ## ; q i the New Dvorak Keyboard ## a a ~~ (Right). Figures Indicate ## ““% : Comparative Loads for Each ## —— Row, Hand and Finger. ## ’ . t ie v « ## r ae ” ## + ae » 2 ## | » to Tt] hedule for treamlini &#39; I post : : reek typewrite! ve ## SO aS Sa aie . : ## vorld: the typewriter keyboa t present is a ar. Commander Dvorak ## . : ; ~ ; 7 on @ is ## mn) ti¢ 2c 2 rie \\ ie built f ‘ : i.? &quot; ¥ { - 3 turned his Arts ion ## Sal jped tke adtot-ef-other things, ty Tt? &#39; ‘ . to those who lost ## } rt il usc in t} { ™~ ivy Ts 1 { 7 Noy , 3 an arm auring the ## ljuction a nuch a : | . as wark Working ## n fox jumps over the ut ¥ closely with Coionel ## il good m« in &#39; I , { &#39; ts Robert 5S a ## f offort. en ee noted newspapel ## 7 st ‘ ‘ ‘ ‘ man who i ’ hi . ## nical t li | | » =f right arr is a re ## : ne ’ ’ ult of i Ww und at ## . a Ohrdruf, Germany, ## the commander de ## ‘ &#39; i vi loped a one-har s ## ; | : m4 U typewriter. In fact ## ‘ tha | ; he invented two ot ## , them, one for th ## eft hand and or ## , ted for the rignt ## | 5 There is no rea ## ; | 4 iT) wrt y in arn, ## , DULtAe—At mputee cant typ ## ot th . is fast as the aver ## ‘ : \\ ’ ioe person * he ¢*&gt; ## bout it q plained. “On thes ## = Nee ee cnich make up 9 ## rt m . , pe &#39; a! (a POS. ? . . Cale ## i 5 Ee, a Hy Ag” a -ae Pa 7 per cent of all typ ## am 3 | eee . eS ae ae ety Oe «4 . ing —21 letters and ## t} : coy ,eer se ’ oe, | , thie period ane ## ‘ ,* | ‘ . &gt;} ’ re - 4 . ‘ ## , Bera &#39; i: ene 8 . Trias Att »&#39; ithin ## &#39; the load \\ 2 a i isv reach of the ## oft eaker, |} |: that t — han Th, ## ™ &#39; : &#39; i i ## : | fit) ri &lt;x . oa ata , e thy, &#39; five l¢ Pere ## : e 4 . sete ‘\\)y ’ it every rir 4 ; — ## / : in f to . c.. ence . naking Up Lie per ## » | 4} ‘ F ‘ tir : oe - ## | Set isis cuit acini Dl oe ee, Re on ent of typing are ## sere are overworked while ne i, ee X. - reached with son ## | ‘7 ; ~ 4 . . ee “ os sf ## ers } &#39;- ala ’ — a _—™ - i 2 en h &#39; ulty, but with ## (‘omn rier Dvorak. ee &#39; , trie hand still ! ## rwmer director of re | as he ele ice) &quot; home position.— ## irch of the University § i ta RI fi sp ‘ om — ae By using the sp ## : . . en 7 Sot fi a2 Pa £4 bas eh a nnn on Soe i : , 7 ’ - &#39; ## f&gt;) VA asnineton and now a on Oe % 6 EWE: Giga , a ce, 5 Su gy er . bal Keybo il ## ga : . — no ” &quot;s * or Re ## naval et] f Cy expert, a ae i ra mSstirmates trat } ## , rked out cif) ‘ tirely ee ages . a rie ty} word ; ## iifferent arrangement ; sae ute will not hb ## piacing the impo! tant inusual for Ol ## kave on the “hom row. On the Dvorak Keyboard, ind typists ## : ; : : _- ## one on which the fin Which She Is Demonstrating, Lenore Fenton Knowing tha ## ‘fers rest MacLain, “the Lastest Secretary in the World,” Is Able to Type 182 Net Words a Minute terans who ga&#39; ## , : &#39; - . ## “The differe &gt; y . Up an aft for the ## \\ +3. } ry? &#39; he ’ Poe &#39; \\ | ; &#39; | f e © crert lor , ; ## ; : fir rtiry ; ‘ t} ‘ t? &#39; &#39; fellov | i! t! ## from 1 } i day put ol 0 &#39; it ne said ## &#39; the new kevbhboard &#39; it wont be easy to co ## the lard J yoard, only ; | pra thie ind Di ## , i without : Lf ; &#39; 7 7 ; enang eetu ## ’ : | } ° ’ ; ry : | ; }\\ :) ri¢ ’ { | : if I 1% it ib ## ent thie be laded alti ## ) , ‘ —_ ## ) } ’ ‘) ; ; &#39; . . rie | &#39; ty ## , . : : &quot; , c&quot; ## aa &#39; . ! nder ad nded I&#39;ll tell ## . 5 -F | Vv] &#39; , , ru ## &#39; &#39; | i : | \\ : ’ ## 2 December 2. IN to 7388 LMERIC AUN WEED Y ## “ One strategy you might use to work with sources like this is to crop out everything you don’t want to OCR. This would be especially effective if, for example, you had a newspaper column that always appeared in the top left-hand corner of the page. You could preprocess your page images so that they only showed that part of the newspaper and left out any ads, images, or extra text. Doing so would likely increase the quality of your OCR output. Such a strategy can be achieved outside of R with software ranging from Adobe Photoshop or the open-source GIMP to Apple’s Automator workflows. Within R, packages like tabulizer and magick enable this. You won’t, however, be required to use these tools in the course, though we may have a chance to demonstrate some of them during lecture. There are several other scenarios where OCR might not be able to read text. Two final (and major) ones are worth highlighting. First, for a long time OCR support for non-alphabetic writing systems was all but nonexistent. New datasets have been released in recent years that mostly rectify these absences, but sometimes support remains spotty and your milage may vary. Second, OCR continues to struggle with handwriting. While it is possible to train unsupervised learning processes on datasets of handwriting and get good results, as of yet there is no general purpose method for OCRing handwritten texts. The various ways people write just don’t conform to the standardized methods of printing that enable computers to recognize text in images. If, someday, you figure out a solution for this, you’ll have solved one of the most challenging problems in computer vision and pattern recognition to date! "],["data-visualization.html", "15 Data Visualization 15.1 Factors 15.2 R Graphics Overview 15.3 The Grammar of Graphics 15.4 Designing a Visualization", " 15 Data Visualization After this lesson, you should be able to: Explain the difference between strings and factors Convert strings to factors Explain the grammar of graphics With the ggplot2 package: Make various kinds of plots Save plots Choose an appropriate kind of plot based on the data 15.1 Factors A feature is categorical if it measures a qualitative category. For example, the genres rock, blues, alternative, folk, pop are categories. R uses the class factor to represent categorical data. Visualizations and statistical models sometimes treat factors differently than other data types, so it’s important to make sure you have the right data type. If you’re ever unsure, remember that you can check the class of an object with the class function. When you load a data set, R usually can’t tell which features are categorical. That means identifying and converting the categorical features is up to you. For beginners, it can be difficult to understand whether a feature is categorical or not. The key is to think about whether you want to use the feature to divide the data into groups. For example, if we want to know how many songs are in the rock genre, we first need to divide the songs by genre, and then count the number of songs in each group (or at least the rock group). As a second example, months recorded as numbers can be categorical or not, depending on how you want to use them. You might want to treat them as categorical (for example, to compute max rainfall in each month) or you might want to treat them as numbers (for example, to compute the number of months time between two events). The bottom line is that you have to think about what you’ll be doing in the analysis. In some cases, you might treat a feature as categorical only for part of the analysis. Let’s think about which features are categorical in the favorite places data set from an earlier lesson. There’s a copy of the data in an RDS file on Canvas. We can load the data with: favs = readRDS(&quot;data/fav_places.rds&quot;) str(favs) ## &#39;data.frame&#39;: 15 obs. of 7 variables: ## $ raw_description: chr &quot;Mandro - Shaved Ice!&quot; &quot;Cruess Hall (the design building)&quot; &quot;The ARC&quot; &quot;Probably the CoHo. I like the croissants.&quot; ... ## $ location : chr &quot;Mandro&quot; &quot;Cruess Hall&quot; &quot;ARC&quot; &quot;CoHo&quot; ... ## $ address : chr &quot;Mandro Teahouse, 1260 Lake Blvd, Davis, CA 95616&quot; &quot;Cruess Hall, 375 California Ave, Davis, CA 95616&quot; &quot;UC Davis Activities and Recreation Center, 760 Orchard Rd, Davis, CA 95616&quot; &quot;UC Davis Activities and Recreation Center, 760 Orchard Rd, Davis, CA 95616&quot; ... ## $ bike_min : num 16 3 4 2 3 1 4 3 0 7 ... ## $ walk_min : num 56 11 14 5 9 3 14 9 0 22 ... ## $ distance_mi : num 3.1 0.6 0.8 0.2 0.5 0.2 0.7 0.5 0 1.2 ... ## $ major : chr &quot;Nutrition&quot; &quot;Psychology&quot; &quot;Biology&quot; &quot;Political Science&quot; ... The numeric columns in this data set (bike_min, walk_min, and distance_mi) are all quantitative, so they’re not categorical. That leaves the character columns. The raw_description and address columns mostly have unique entries. This is a sign that they wouldn’t make good categorical features, because if we use a column of unique entries to make groups, each group will only have one element! So we can rule these out as categorical. They are textual data. That leaves us with the major and location columns. It’s easy to imagine grouping the survey responses by major, we can say major is categorical. For location, it’s not as clear. The locations are: table(favs$location) ## ## Arboretum ARC CoHo Cruess Hall Disneyland ## 2 2 1 1 1 ## iTea Mandro MU PES Shields Library ## 1 1 1 1 1 ## Tennis Courts Walker Hall West Village ## 1 1 1 So there are a couple of locations that aren’t unique, but most of them are. For this data set, we probably shouldn’t treat location as categorical because the groups wouldn’t be interesting. However, if we had more survey responses (or more at each location) it might make sense to do so. Let’s convert the major column to a factor. To do this, use the factor function: factor(favs$major) ## [1] Nutrition Psychology Biology ## [4] Political Science Sociology Sustainable Agriculture ## [7] Economics Political Science Undeclared ## [10] Psychology Undeclared Economics ## [13] Political Science English Economics ## 9 Levels: Biology Economics English Nutrition Political Science ... Undeclared favs$major = factor(favs$major) Notice that factors are printed differently than strings. The categories of a factor are called levels. You can list the levels with the levels function: levels(favs$major) ## [1] &quot;Biology&quot; &quot;Economics&quot; ## [3] &quot;English&quot; &quot;Nutrition&quot; ## [5] &quot;Political Science&quot; &quot;Psychology&quot; ## [7] &quot;Sociology&quot; &quot;Sustainable Agriculture&quot; ## [9] &quot;Undeclared&quot; Factors remember all possible levels even if you take a subset: favs$major[c(1, 2)] ## [1] Nutrition Psychology ## 9 Levels: Biology Economics English Nutrition Political Science ... Undeclared This is another way factors are different from strings. Factors “remember” all possible levels even if they aren’t present. This ensures that if you plot a factor, the missing levels will still be represented on the plot. You can make a factor forget levels that aren’t present with the droplevels function: first3 = favs$major[1:3] droplevels(first3) ## [1] Nutrition Psychology Biology ## Levels: Biology Nutrition Psychology 15.2 R Graphics Overview There are three popular systems for creating visualizations in R: The base R functions (primarily the plot function) The lattice package The ggplot2 package These three systems are not interoperable! Consequently, it’s best to choose one to use exclusively. Compared to base R, both lattice and ggplot2 are better at handling grouped data and generally require less code to create a nice-looking visualization. The ggplot2 package is so popular that there are now knockoff packages for other data-science-oriented programming languages like Python and Julia. The package is also part of the Tidyverse. Because of these advantages, we’ll use ggplot2 for visualizations in this and all future lessons. 15.3 The Grammar of Graphics ggplot2 has detailed documentation and also a cheatsheet. The “gg” in ggplot2 stands for grammar of graphics. The idea of a grammar of graphics is that visualizations can be built up in layers. In ggplot2, the three layers every plot must have are: Data Geometry Aesthetics There are also several optional layers. Here are a few: Layer Description scales Title, label, and axis value settings facets Side-by-side plots guides Axis and legend position settings annotations Shapes that are not mapped to data coordinates Coordinate systems (Cartesian, logarithmic, polar) 15.3.1 Making a Plot As an example, let’s plot the favorite places survey data. First, we need to load ggplot2. As always, if this is your first time using the package, you’ll have to install it. Then you can load the package: # install.packages(&quot;ggplot2&quot;) library(ggplot2) What kind of plot should we make? It depends on what data we want the plot to show. Let’s make a plot that shows the distance in miles (from the library) against the walking time in minutes for each place in the data. Both the distance and the walking time are recorded as numbers. A scatter plot is a good choice for displaying two numeric features. Later we’ll learn about other options, but for now we’ll make a scatter plot. Layer 1: Data The data layer determines the data set used to make the plot. ggplot and most other Tidyverse packages are designed for working with tidy data frames. Tidy means: Each observation has its own row. Each feature has its own column. Each value has its own cell. Tidy data sets are convenient in general. A later lesson will cover how to make an untidy data set tidy. Until then, we’ll take it for granted that the data sets we work with are tidy. To set up the data layer, call the ggplot function on a data frame: ggplot(favs) This returns a blank plot. We still need to add a few more layers. Layer 2: Geometry The geometry layer determines the shape or appearance of the visual elements of the plot. In other words, the geometry layer determines what kind of plot to make: one with points, lines, boxes, or something else. There are many different geometries available in ggplot2. The package provides a function for each geometry, always prefixed with geom_. To add a geometry layer to the plot, choose the geom_ function you want and add it to the plot with the + operator: ggplot(favs) + geom_point() ## Error: geom_point requires the following missing aesthetics: x and y This returns an error message that we’re missing aesthetics x and y. We’ll learn more about aesthetics in the next section, but this error message is especially helpful: it tells us exactly what we’re missing. When you use a geometry you’re unfamiliar with, it can be helpful to run the code for just the data and geometry layer like this, to see exactly which aesthetics need to be set. As we’ll see later, it’s possible to add multiple geometries to a plot. Layer 3: Aesthetics The aesthetic layer determines the relationship between the data and the geometry. Use the aesthetic layer to map features in the data to aesthetics (visual elements) of the geometry. The aes function creates an aesthetic layer. The syntax is: aes(AESTHETIC = FEATURE, ...) The names of the aesthetics depend on the geometry, but some common ones are x, y, color, fill, shape, and size. There is more information about and examples of aesthetic names in the documentation. For example, if we want to put the distance_mi feature on the x-axis, the aesthetic layer should be: aes(x = distance_mi) In the aes function, column names are never quoted. Unlike most layers, the aesthetic layer is not added to the plot with the + operator. Instead, you can pass the aesthetic layer as the second argument to the ggplot function: ggplot(favs, aes(x = distance_mi, y = walk_min)) + geom_point() If you want to set an aesthetic to a constant value, rather than one that’s data dependent, do so outside of the aesthetic layer. For instance, suppose we want to make the points blue: ggplot(favs, aes(x = distance_mi, y = walk_min)) + geom_point(color = &quot;blue&quot;) If you set an aesthetic to a constant value inside of the aesthetic layer, the results you get might not be what you expect: ggplot(favs, aes(x = distance_mi, y = walk_min, color = &quot;blue&quot;)) + geom_point() This plot also demonstrates the importance of visualization. We can see that one of the survey response distances is around 500 miles, which is probably erroneous. Before we continue, let’s remove all responses that are more than 100 miles from campus: favs = favs[favs$distance_mi &lt;= 100, ] Per-geometry Aesthetics When you pass an aesthetic layer to the ggplot function, it applies to the entire plot. You can also set an aesthetic layer individually for each geometry, by passing the layer as the first argument in the geom_ function: ggplot(favs) + geom_point(aes(x = distance_mi, y = walk_min)) This is really only useful when you have multiple geometries. As an example, let’s color-code the points by major: ggplot(favs, aes(x = distance_mi, y = walk_min, color = major)) + geom_point() Now let’s also add labels to each point. To do this, we need to add another geometry: ggplot(favs, aes(x = distance_mi, y = walk_min, color = major, label = major)) + geom_point() + geom_text(size = 2) Where we put the aesthetics matters: ggplot(favs, aes(x = distance_mi, y = walk_min, label = major)) + geom_point() + geom_text(aes(color = major), size = 2) Layer 4: Scales The scales layer controls the title, axis labels, and axis scales of the plot. Most of the functions in the scales layer are prefixed with scale_, but not all of them. The labs function is especially important, because it’s used to set the title and axis labels: ggplot(favs, aes(x = distance_mi, y = walk_min)) + geom_point() + labs(title = &quot;Walking Distance vs Miles&quot;, x = &quot;Distance (mi)&quot;, y = &quot;Walking Distance (min)&quot;) 15.3.2 Saving Plots In ggplot2, use the ggsave function to save the most recent plot you created: ggsave(&quot;scatter.png&quot;) The file format is selected automatically based on the extension. Common formats are PNG and PDF. The Plot Device You can also save a plot with one of R’s “plot device” functions. The steps are: Call a plot device function: png, jpeg, pdf, bmp, tiff, or svg. Run your code to make the plot. Call dev.off to indicate that you’re done plotting. This strategy works with any of R’s graphics systems (not just ggplot2). Here’s an example: # Run these lines in the console, not the notebook! jpeg(&quot;scatter.jpeg&quot;) ggplot(favs, aes(x = distance_mi, y = walk_min)) + geom_point() dev.off() 15.3.3 Example: Bar Plot Let’s say we want to plot the number of responses for each major. A bar plot is an appropriate way to represent this visually. The geometry for a bar plot is geom_bar. Since bar plots are mainly used to display frequencies, the geom_bar function automatically computes frequencies when given mapped to a categorical feature. So we can write: ggplot(favs, aes(x = major)) + geom_bar() + labs(title = &quot;Majors in Responses&quot;, x = &quot;Major&quot;, y = &quot;Count&quot;) To prevent geom_bar from computing frequencies automatically, set stat = \"identity\". This is mainly useful if you want to plot quantities you’ve computed manually on the y-axis. 15.4 Designing a Visualization What plot is appropriate? Variable Versus Plot categorical bar, dot categorical categorical bar, dot, mosaic numerical box, density, histogram numerical categorical box, density, ridge numerical numerical line, scatter, smooth scatter If you want to add a: 3rd numerical variable, use it to change point/line sizes. 3rd categorical variable, use it to change point/line styles. 4th categorical variable, use side-by-side plots. Also: Always add a title and axis labels. These should be in plain English, not variable names! Specify units after the axis label if the axis has units. For instance, “Height (ft).” Don’t forget that many people are colorblind! Also, plots are often printed in black and white. Use point and line styles to distinguish groups; color is optional. Add a legend whenever you’ve used more than one point or line style. Always write a few sentences explaining what the plot reveals. Don’t describe the plot, because the reader can just look at it. Instead, explain what they can learn from the plot and point out important details that are easily overlooked. Sometimes points get plotted on top of each other. This is called overplotting. Plots with a lot of overplotting can be hard to read and can even misrepresent the data by hiding how many points are present. Use a two-dimensional density plot or jitter the points to deal with overplotting. For side-by-side plots, use the same axis scales for both plots so that comparing them is not deceptive. "],["network-analysis.html", "16 Network Analysis 16.1 Overview 16.2 What is Network Analysis 16.3 Network Data 16.4 Graph Level Properties 16.5 Node Level Properties 16.6 Network Workflow 16.7 Network Tools 16.8 References", " 16 Network Analysis 16.1 Overview This reader will introduce the theoretical and logistical underpinnings of network analysis. It will define what networks are, their limitations, and their use cases. It will then cover some of the most commonly used network measures, what they mean, and how to generate them. 16.1.1 While you wait Make sure you have installed all the packages you will need: install.packages(c(\"statnet\", \"visNetwork\", \"learnr\")) Download the data and class code-along from Canvas Play around with the Oracle of Bacon 16.1.2 Learning Objectives By the end of this class meeting, students should be able to: Understand what a network is. Understand what is and is not relational data. Understand the shortcomings and limitations of network analysis. Evaluate a network dataset and interpret the generated metrics. 16.1.3 Roadmap What is Social Network Analysis (SNA) Examples of networks in research SNA Data Network (graph) level properties Individual (node) level properties Network Tools Guided Homework Start/Question Time 16.2 What is Network Analysis You are all most likely familiar now with tabular data; rows and columns containing information. It looks like this: Person Name Age Widgets J 30 1 Y 21 3 G 32 4 Z 48 8 While this is a tidy way to store data, it artificially atomizes or separates many of the things we are interested in as researchers, social or otherwise. Network analysis is a tool to work with relational data, a.k.a. information about how entities are connected with each other. For example, the diagram below shows the same data as the table above, with the added benefit of showing how these individuals are connected to each other. Hover over the people to reveal the data about them. Rather than looking only at attributes of specific data points, we are looking at the connections between data. In network analysis, data points are called nodes or vertices, and the connections between them are called edges or ties. Vertices can be anything—people, places, words, concepts—they are usually mapped into rows in a data frame. Edges contain any information on how these things connect or are related to each other. These components create a network or graph, defined as “finite set or sets of actors and the relation or relations defined on them” (Wasserman and Faust 1994). 16.2.1 Networks in research - Social Sciences One of the first instances of social network analysis was originally published in 1932 as part of Jacob Moreno’s Who Shall Survive (1953). This study used the friendship networks of girls within a reform school to show that the ties between them were a stronger predictor of runaways than any attribute of the girls themselves. Since then, networks have been used widely in the social sciences, but only really picked up as the tools to understand SNA became more available. 16.2.2 Networks in research - Neuroscience Neuroscientists use networks to study the brain, given their ready application to neurons and pathways. Bassett and Sporns (2017) provide an overview of how to translate neuroscience problems into network ones, and the tools available to study them. 16.2.3 Networks in research - Chemistry Chemistry was quick to see the applications of networks. As early at 1985 papers were published detailing the potential networks provided in terms of understanding and finding new ways to measure and understand the bonds between atoms and molecules (Balaban 1985). 16.2.4 Networks in research - The Internet The internet is a network! Beyond the various social network sites, servers themselves act as nodes and the information flows between them along edges. Google used this property in the first version of their search engine, which used the network metric of PageRank to determine which sites to show at the top of search results (Page 2001). 16.2.5 Networks in research - Infrastructure Fand and Mostafavi (2019) showed how you can use social media network data to find where infrastructure is failing during disasters, such as hurricane Harvey in 2017. Their system promises a method to monitor physical infrastructure like roads, bridges, and barriers like more easily monitored infrastructure like the electrical grid. 16.2.6 Networks in research - Security Network analysis has also been used for offensive purposes. One of the most prominent uses is mapping crime or terror networks (Krebs 2002), though it is fraught with ethical concerns. There are specific tools made for this purpose, such as the keyplayer package (An and Liu 2016), which helps find what nodes in a network would fragment them the most if removed. 16.3 Network Data Networks are based on relational data. This means the core data requirement is that we have some measure of how nodes are connected. The two most common network data formats are the edgelist and adjacency matrix. Either of these will work for nearly any network purpose, and it is easy to convert between them. You will also need an attributes file, which gives information about the nodes being connected. 16.3.1 Edgelist An edgelist is a two-column dataframe with a from and to column. Each row represents one edge or tie, with the possibility of adding in more information. Here is an example of a basic edgelist. Let’s load in the example data you downloaded and look at some of it. # load in the data toy_edgelist = read.csv(&quot;./data/toy_edgelist.csv&quot;, header = TRUE, stringsAsFactors = FALSE) # show the first 10 rows kable(head(toy_edgelist, n = 10)) to from b33f00bd1109e1ae3ffa757d0aef0a25942f2ba3 zuko 19d5b2694036f6fab966564c1c44bc74330f22c2 zuko 9483b16c4904908115f4538525e37f776f4596d4 zuko f8452649773eb7e024bfa59c395afa0c302d1928 zuko eea677240a425ed7ccdeff69feb2d377a5542599 zuko 9dbcce359070c879f20843e19564aee545f80d2d zuko 749e81272630eb4755e4a7bca10fe3e3524d77ce zuko toph zuko 5737a840aa867025dcb506f24cb5546f16b4d777 zuko 028f5d1f351d38cd6553ab4674b19725d5ea3d3c zuko 16.3.2 Adjacency Matrix The same data can also be displayed in a table format. The information is the same, but it is presented in a way more usable by our code to create measures we care out. In this format, every node has both a row and column. If there is an edge between two nodes, a 1 is placed in the intersection of their row and column. 028f5d1f351d38cd6553ab4674b19725d5ea3d3c 19d5b2694036f6fab966564c1c44bc74330f22c2 5737a840aa867025dcb506f24cb5546f16b4d777 749e81272630eb4755e4a7bca10fe3e3524d77ce 9483b16c4904908115f4538525e37f776f4596d4 9dbcce359070c879f20843e19564aee545f80d2d b33f00bd1109e1ae3ffa757d0aef0a25942f2ba3 eea677240a425ed7ccdeff69feb2d377a5542599 f8452649773eb7e024bfa59c395afa0c302d1928 toph zuko 028f5d1f351d38cd6553ab4674b19725d5ea3d3c 0 0 0 0 0 0 0 0 0 0 1 19d5b2694036f6fab966564c1c44bc74330f22c2 0 0 0 0 0 0 0 0 0 0 1 5737a840aa867025dcb506f24cb5546f16b4d777 0 0 0 0 0 0 0 0 0 0 1 749e81272630eb4755e4a7bca10fe3e3524d77ce 0 0 0 0 0 0 0 0 0 0 1 9483b16c4904908115f4538525e37f776f4596d4 0 0 0 0 0 0 0 0 0 0 1 9dbcce359070c879f20843e19564aee545f80d2d 0 0 0 0 0 0 0 0 0 0 1 b33f00bd1109e1ae3ffa757d0aef0a25942f2ba3 0 0 0 0 0 0 0 0 0 0 1 eea677240a425ed7ccdeff69feb2d377a5542599 0 0 0 0 0 0 0 0 0 0 1 f8452649773eb7e024bfa59c395afa0c302d1928 0 0 0 0 0 0 0 0 0 0 1 toph 0 0 0 0 0 0 0 0 0 0 1 zuko 0 0 0 0 0 0 0 0 0 0 0 16.3.3 Edge Weights Edges can also have weights, meaning some edges are valued more than others. In an edgelist, you can add a third “weight” column, entering higher numbers to denote a more important connection. In an adjacency matrix, you can put numbers other than 1 in the intersection to denote more important connections. For our example, we’ll stick with un-weighted connections for now. 16.3.4 Attributes Each network also typically has an attributes table, which looks just like typical tabular data, with each row belonging to a specific node in our network. Let’s load in and look at the sample attributes file. # load in data toy_attributes = read.csv(&quot;./data/toy_attributes.csv&quot;, header = TRUE, stringsAsFactors = FALSE) # show top of attributes table kable(head(toy_attributes, n = 10)) id year color zuko 2 purple nezuko 2 purple winnie the pooh 2 blue toph 2 purple chicken joe 2 green the rat from ratatouille 5 blue spider-man 3 blue yamaguchi tadashi 1 purple jude sweetwine 2 purple lord future 2 blue 16.3.5 Create an Example Network Before we start exploring specific measures, we’ll create a toy network to use as an example. Let’s start by loading in some packages. statnet is one of the major network packages in R. It allows you to compute many of the most common network measures, and run simulations called Exponential Random Graph Models. We’ll stick with the basics for now! # Run this to load statnet, if you need to install it, do so now. library(statnet) Now that we have our tools loaded, let’s create out first network. We’ll use the data you loaded in before. This toy network will be used as a visual for learning the measurements below. We are going to turn the attributes file and edgelist into a statnet network object. A network object is a special kind of list in R. It is formatted in a way that the other statnet functions expect. While you could edit it like a normal list, it is highly recommended you use the other statnet functions to manipulate this object to make sure you don’t break any of the data expectations. We’ll use the network function to create our network object. Before we create it, we will sort our attributes file alphabetically. This is super important, as the network object will automatically sort things itself. If we do not sort our attributes dataframe to match, all of our measures later will be misaligned! # sort your attributes frame alphabetically. Super important! toy_attributes = toy_attributes[order(toy_attributes$id), ] # make network! # we will cover the `directed = FALSE` argument soon. toy_network = network(toy_edgelist, directed = FALSE) Before we move on, let’s add a net_id column to our attributes dataframe. This will let us easily check what the network object IDs are for our nodes. # add ID column toy_attributes$net_id = 1:nrow(toy_attributes) We can inspect our new network by calling the summary function on it. Don’t worry too much about the output yet. summary(toy_network) ## Network attributes: ## vertices = 96 ## directed = FALSE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## bipartite = FALSE ## total edges = 90 ## missing edges = 0 ## non-missing edges = 90 ## density = 0.01973684 ## ## Vertex attributes: ## vertex.names: ## character valued attribute ## 96 valid vertex names ## ## No edge attributes ## ## Network edgelist matrix: ## [,1] [,2] ## [1,] 56 96 ## [2,] 5 96 ## [3,] 49 96 ## [4,] 82 96 ## [5,] 76 96 ## [6,] 51 96 ## [7,] 41 96 ## [8,] 93 96 ## [9,] 27 96 ## [10,] 1 96 ## [11,] 25 88 ## [12,] 22 88 ## [13,] 4 88 ## [14,] 16 88 ## [15,] 64 88 ## [16,] 17 94 ## [17,] 73 94 ## [18,] 6 94 ## [19,] 80 94 ## [20,] 66 94 ## [21,] 75 94 ## [22,] 70 94 ## [23,] 61 94 ## [24,] 95 94 ## [25,] 96 94 ## [26,] 96 93 ## [27,] 46 93 ## [28,] 65 93 ## [29,] 30 93 ## [30,] 39 93 ## [31,] 36 93 ## [32,] 79 69 ## [33,] 7 69 ## [34,] 11 69 ## [35,] 10 69 ## [36,] 43 69 ## [37,] 33 69 ## [38,] 58 69 ## [39,] 83 69 ## [40,] 20 92 ## [41,] 37 92 ## [42,] 57 92 ## [43,] 74 92 ## [44,] 71 92 ## [45,] 18 92 ## [46,] 2 92 ## [47,] 63 92 ## [48,] 50 92 ## [49,] 48 91 ## [50,] 60 91 ## [51,] 34 91 ## [52,] 26 91 ## [53,] 13 91 ## [54,] 38 91 ## [55,] 84 91 ## [56,] 12 95 ## [57,] 28 95 ## [58,] 9 95 ## [59,] 59 95 ## [60,] 29 95 ## [61,] 94 95 ## [62,] 73 95 ## [63,] 52 95 ## [64,] 62 95 ## [65,] 19 95 ## [66,] 31 86 ## [67,] 78 86 ## [68,] 15 86 ## [69,] 23 86 ## [70,] 73 86 ## [71,] 35 87 ## [72,] 32 87 ## [73,] 54 87 ## [74,] 40 87 ## [75,] 77 87 ## [76,] 24 89 ## [77,] 72 89 ## [78,] 81 89 ## [79,] 45 89 ## [80,] 3 89 ## [81,] 14 85 ## [82,] 67 85 ## [83,] 68 85 ## [84,] 8 85 ## [85,] 55 85 ## [86,] 21 90 ## [87,] 44 90 ## [88,] 47 90 ## [89,] 53 90 ## [90,] 42 90 Then we’ll add the node attributes to the network object. If you run summary again you should see the values from our toy_attributes have been added. # add each attribute to network. # do this by looking at every column, then adding it to the network for(col_name in colnames(toy_attributes)) { toy_network = set.vertex.attribute(x = toy_network, attrname = col_name, value=toy_attributes[,col_name]) } Let’s see what out network looks like! plot(toy_network) There we are. The default plotting in statnet is ugly. For the sake of our eyes, and for exploring some of the measure we create, we’ll use the visNetwork package to visualize our networks. It will make the code a bit more cumbersome, but it will be worth it. From now on, we will need to use the edges and attributes dataframes for plotting. This means we will often need to run commands twice, once for the network and once for the dataframes. When you are working with networks for research, you would usually do everything you need on your network, than create a dataframe from it all at once. We will need to deal with a bit of redundancy to take things one step at a time. Let’s try plotting again with visNetwork, using the dataframes. We’ll give the visNetwork function our edgelist and attributes dataframe. We’ll also tell it to plot the names from our attributes dataframe so we can see them when we hover over the nodes in the plot. # add pop-up tooltips with names # visNetwork uses the &quot;title&quot; column to create pop-up boxes toy_attributes$title = toy_attributes$id # plot! visNetwork(nodes = toy_attributes, edges = toy_edgelist) %&gt;% visInteraction(zoomView = FALSE) Nice. 16.3.6 Components Most often when working with networks you want to limit your analysis to one cluster or component, typically the largest one in your network. If segments of your network aren’t connected, you can’t answer many of the relational questions network analysis is good for! Let’s limit our network to the largest component: # find what nodes are part of the largest component toy_network%v%&quot;lc&quot; = component.largest(toy_network) # delete those nodes that are not ## in the network toy_network = delete.vertices(toy_network, which(toy_network%v%&quot;lc&quot; == FALSE)) ## in our dataframes toy_attributes = toy_attributes[toy_attributes$id %in% as.character(toy_network%v%&quot;id&quot;),] toy_edgelist = toy_edgelist[which(toy_edgelist$to %in% toy_attributes$id | toy_edgelist$from %in% toy_attributes$id),] # plot! visNetwork(nodes = toy_attributes, edges = toy_edgelist) %&gt;% visInteraction(zoomView = FALSE) 16.3.7 Limitations of Network Data Before we move on we should take a moment to talk about some the the caveats when using network data. While powerful, network analysis is particularly picky when in comes to data requirements. I’ll cover the two biggest ones below. You should always keep these in mind when using or interpreting network tools. 16.3.7.1 Missing Data Network analysis is very vulnerable to missing data. A simple way to understand why is to make a small adjustment to our network. I’ve highlighted one node in green. This node is structurally vital to the network; without it, the shape of the network as a whole will change. If we remove this node, the network changes in a major way! Imagine these nodes are people, and that missing node is the one person you forget to survey, or was sick the day data was collected. This could massively change the outcome of your analyses. There is some advanced research going on to detect and replace missing data like this if you have enough context, but it is not something to rely on. 16.3.7.2 Network Boundaries Network analysis is all about looking at the relationships between entities. However, following all connections an entity has can quickly spiral out of hand. For example, if you wanted to map your own social network, where would you start? You would include yourself, then your friends and family, but what about after that? Your friends and family have friends and family, as do their friends and family, as do their … and so on. If you are looking at human networks, every human will be included if you look far enough, so how do you decide when to stop? There is no easy answer. If you are looking at a pre-defined group (e.g. this class), you can set the boundaries to include everyone in this class and the connection between them. However, that doesn’t really capture the social networks of people in this class as most people will have friends elsewhere. Another common method is setting an arbitrary number of “steps” or connections from a target population. If we were interested in a 2-step network from an individual, we would collect all of their relevant connections, and then ask all the people they nominated about their connections. Some sort of justification will be needed as to why you picked the number of steps that you did. 16.3.8 Projected Networks Often, you will not have individual level network data, but you will have data on group membership. For example, if you wanted to map the social networks of student, but don’t know who they actually hang around with, you may be able to use class rosters to build an approximate network. This is call a bipartite network, two-mode, or projected network. You can see an example below. In this figure there are two kinds of nodes, students and classes. You can “collapse” this into a student network by assuming every student connected to a class is connected to each other. The same is true with classes, such that classes are related to each other if a single student is enrolled in both. This assumption may not always be correct, and you need to take care if you are going to make it in your research. If a class has 300 students, it is most likely not correct to assume every student knows every other student in that class. For reference, this is what out projected class network looks like: 16.4 Graph Level Properties Now that we know what networks are and have some examples of how they are used and the data required, let’s get into actually analyzing them. There are a number of measures we can compute to understand the structure of a network as a whole. We will go over some basic network level ones here. These are single measures or attributes used to describe the entire network, and can be used to compare one network against another. Directed or Un-directed Density Centralization 16.4.1 Directed or Un-directed Networks can either be directed or un-directed. A directed network treats the edges between nodes as having a specific direction of flow, while an un-directed network considers all edges to be mutual. An example of each is presented below. Both edgelist and adjacency matrix datasets are inherently directed. For edgelists, the sender is often the first column, and the receiver is the second. For adjacency matrices the rows are considered senders and columns are receivers. Directionality is often specified when the network objects are created. When we created our toy network, we specified directed = FALSE to simplify things. If you want a directed network, the default is directed = TRUE for statnet networks. A directed network tracks which node is the source and which node is the receiver for an edge. Take for example the follow mechanic on Twitter. User A can follow User B, creating a directed edge from A to B, but B does not have to follow A in return. This can be useful when trying to understand the flows of resources that are finite such as money or goods. # visNetwork uses a column called &quot;arrows&quot; to show directionality in its plots. # For our edgelist, we&#39;ll just say every row is &quot;to&quot; for now toy_edgelist$arrows = &quot;to&quot; # this will show us what our network would look like if it was directed. visNetwork(toy_attributes, toy_edgelist, main = &quot;Directed&quot;) %&gt;% visInteraction(zoomView = FALSE) An un-directed network treats all ties as mutual, such that A and B are both involved equally in a tie. An example is the friend mechanic on Facebook. Once a friendship is established, both users are considered equal in the tie. This can be helpful when you do not have information on what node initiates a tie, or when events happen equally to a group of nodes, such as all nodes being connected through co-membership in a group. # lets drop the arrow column for now since our network is un-directed. toy_edgelist = toy_edgelist[,c(&quot;from&quot;, &quot;to&quot;)] # plot visNetwork(toy_attributes, toy_edgelist, main = &quot;Un-directed&quot;) %&gt;% visInteraction(zoomView = FALSE) Which of these will be useful to you will likely change from project to project. However, it is vital to understand what kind of network you are working with, as many network calculations we will talk about later change their behavior based on if the network is directed or not. 16.4.2 Density Density is the first real graph level metric that helps you understand what is particular about the network you are looking at. The density of a network is a numerical score showing how many ties exist in a network, given the max possible in that network. Mathematically that is \\(\\frac{Actual Edges}{Possible Edges}\\), where actual edges is the number of edges in the network, and possible edges is the number of edges if every single node in the network was connected to every other node. Networks that are more densely connected are considered to be more cohesive and robust. This means that the removal of any specific edge or node will not have a great effect of the network as a whole. It also typically means that any one node in the network will be more likely to have access to whatever resources are in the network, as there are more potential connections in the network to search for resources. To calculate the density of a network, we use the network.density() function. You can also see it if you use summary() on your network object. Below is our toy network and a less dense version to try and visualize the difference. Density is all about how many edges exist in the network. Notice that there are the same number of nodes in both of these networks. 16.4.3 Centralization Freeman Centralization (usually just called centralization) gives a sense of the shape of the network, namely how node level measures are distributed in a network. We’ll discuss node level measures next, but for now it is only important to understand that node level measures are numeric scores assigned to specific nodes rather than the network as a whole. This means that each node may have a different value. Consider the two networks below. The first “star” network would be considered highly centralized, as one node connects to all the others, while the rest of the nodes have no connections to each other. This star network would have a edge centralization score of 1, as 100% of the ties are connected with one node. The loop network would have a score of 0, as every node is equally connected to each other. Centralization is a measure of how unevenly node level metrics are distributed in a network. This is helpful when trying to understand if some nodes in the network have a larger influence, or are is some way more important than others. 16.5 Node Level Properties Node level measures are numeric representations of a node’s position and importance in a network. There are several common node level measures, and we will go over some of them here. Each measure tries to quantify a different aspect of a node’s position in the network so we can make an argument about why that specific node or class of nodes is important in some way. We will go over: Degree Geodesic distance Betweenness centrality Eigenvector centrality Most node level measures are only helpful within the context of the network they were generated for. This is because the measures are created in part using network level measures like density. This means it is alright to compare one node to another within the same network, but toy should node compare the node level measures between networks. 16.5.1 Degree Degree counts how many edges are connected to a node. You can count incoming, outgoing, or total (Freeman) degree. Incoming and outgoing degree only matter in directed networks. In un-directed networks, only total edges are applicable. Degree gives a very rough measure of how popular or central a node is in the network. If a node has more ties, it may indicate that node as being more central or important the network as a whole. Degree is a raw count of the number of edges a node has, this makes the interpretation of degree highly dependent on the size of the network. In a small network with only 25 total edges, having 10 of them would be significant. In a larger network with 250 total edges, 10 edges could be less impressive. Degree should thus be interpreted in the cortex of other nodes in the network. Let’s scale the node sizes of our toy network based on their total degree numbers. We’ll get degree counts for each of our nodes using the degree() function. We can save that into our dataframe and network for use later. For now I am naming columns to work specifically with visNetwork, we’ll make a proper dataframe for analyses later using data we saved in the network object. In our visualization, you can click on any node to highlight only the edges connected to that node. # find the degree of each node and save in the network # we will use the special `%v%` operator when assigning values to a network. `%v%` works like `$` for dataframes, allowing you to ask for specific values in the network # in this case `%v%` stands for vertex, and you can use `%e%` if you want to work with edges. # so let&#39;s get the degree counts, and assign them to the &quot;degree&quot; variable in our network object toy_network%v%&quot;degree&quot; = degree(toy_network) # visNetwork uses the &quot;value&quot; column to determine node size, so let&#39;s put it there as well for now. # we&#39;ll square the values just to make them more distinct toy_attributes$value = degree(toy_network)^2 # plot! visNetwork(toy_attributes, toy_edgelist, main = &quot;Degree Example&quot;) %&gt;% visInteraction(zoomView = FALSE) 16.5.2 Geodesic Distance Geodesic Distance is “the length of the shortest path via the edges or binary connections between nodes” (Kadushin 2012). In other words, if we treat the network as a map we can move along, with the nodes being stopping places and the edges being paths, the geodesic is the shortest possible path we can use to walk between two nodes. Nodes that on average have a shorter geodesic distance between all the other nodes in the network are considered to have have greater access to the resources in a network. This is because a node with a low average geodesic distance can theoretically “reach” the other nodes with less effort because it does not need to travel as far. To find the mean geodesic distance for each node in the network we will first need to find the geodesic distance from each node to every other node, then take the mean. Not super difficult, but there isn’t a single function to do it for us. First we will use the geodist() function to get all the geodesics. # get all the geodesics # I use the $gdist so we only get geodesics not counts geodist(toy_network)$gdist ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] ## [1,] 0 2 3 4 4 5 3 4 5 2 4 4 3 ## [2,] 2 0 3 4 4 5 3 4 5 2 4 4 3 ## [3,] 3 3 0 3 3 4 2 3 4 3 3 3 4 ## [4,] 4 4 3 0 2 4 3 2 4 4 2 2 5 ## [5,] 4 4 3 2 0 4 3 2 4 4 2 2 5 ## [6,] 5 5 4 4 4 0 4 4 2 5 4 4 6 ## [7,] 3 3 2 3 3 4 0 3 4 3 3 3 4 ## [8,] 4 4 3 2 2 4 3 0 4 4 2 2 5 ## [9,] 5 5 4 4 4 2 4 4 0 5 4 4 6 ## [10,] 2 2 3 4 4 5 3 4 5 0 4 4 3 ## [11,] 4 4 3 2 2 4 3 2 4 4 0 2 5 ## [12,] 4 4 3 2 2 4 3 2 4 4 2 0 5 ## [13,] 3 3 4 5 5 6 4 5 6 3 5 5 0 ## [14,] 5 5 4 4 4 2 4 4 2 5 4 4 6 ## [15,] 3 3 4 5 5 6 4 5 6 3 5 5 2 ## [16,] 3 3 4 5 5 6 4 5 6 3 5 5 2 ## [17,] 2 2 3 4 4 5 3 4 5 2 4 4 3 ## [18,] 3 3 4 5 5 6 4 5 6 3 5 5 2 ## [19,] 2 2 3 4 4 5 3 4 5 2 4 4 3 ## [20,] 2 2 3 4 4 5 3 4 5 2 4 4 3 ## [21,] 4 4 3 2 2 4 3 2 4 4 2 2 5 ## [22,] 2 2 3 4 4 5 3 4 5 2 4 4 3 ## [23,] 4 4 3 2 2 4 3 2 4 4 2 2 5 ## [24,] 3 3 2 3 3 4 2 3 4 3 3 3 4 ## [25,] 4 4 3 2 2 4 3 2 4 4 2 2 5 ## [26,] 3 3 4 5 5 6 4 5 6 3 5 5 2 ## [27,] 3 3 2 3 3 4 2 3 4 3 3 3 4 ## [28,] 3 3 2 3 3 4 2 3 4 3 3 3 4 ## [29,] 3 3 2 2 2 2 2 2 2 3 2 2 4 ## [30,] 3 3 2 3 3 4 2 3 4 3 3 3 4 ## [31,] 2 2 3 4 4 5 3 4 5 2 4 4 3 ## [32,] 5 5 4 4 4 2 4 4 2 5 4 4 6 ## [33,] 3 3 2 3 3 4 2 3 4 3 3 3 4 ## [34,] 2 2 3 4 4 5 3 4 5 2 4 4 3 ## [35,] 4 4 3 3 3 1 3 3 1 4 3 3 5 ## [36,] 2 2 3 4 4 5 3 4 5 2 4 4 1 ## [37,] 2 2 1 2 2 3 1 2 3 2 2 2 3 ## [38,] 3 3 2 1 1 3 2 1 3 3 1 1 4 ## [39,] 1 1 2 3 3 4 2 3 4 1 3 3 2 ## [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] ## [1,] 5 3 3 2 3 2 2 4 2 4 3 4 ## [2,] 5 3 3 2 3 2 2 4 2 4 3 4 ## [3,] 4 4 4 3 4 3 3 3 3 3 2 3 ## [4,] 4 5 5 4 5 4 4 2 4 2 3 2 ## [5,] 4 5 5 4 5 4 4 2 4 2 3 2 ## [6,] 2 6 6 5 6 5 5 4 5 4 4 4 ## [7,] 4 4 4 3 4 3 3 3 3 3 2 3 ## [8,] 4 5 5 4 5 4 4 2 4 2 3 2 ## [9,] 2 6 6 5 6 5 5 4 5 4 4 4 ## [10,] 5 3 3 2 3 2 2 4 2 4 3 4 ## [11,] 4 5 5 4 5 4 4 2 4 2 3 2 ## [12,] 4 5 5 4 5 4 4 2 4 2 3 2 ## [13,] 6 2 2 3 2 3 3 5 3 5 4 5 ## [14,] 0 6 6 5 6 5 5 4 5 4 4 4 ## [15,] 6 0 2 3 2 3 3 5 3 5 4 5 ## [16,] 6 2 0 3 2 3 3 5 3 5 4 5 ## [17,] 5 3 3 0 3 2 2 4 2 4 3 4 ## [18,] 6 2 2 3 0 3 3 5 3 5 4 5 ## [19,] 5 3 3 2 3 0 2 4 2 4 3 4 ## [20,] 5 3 3 2 3 2 0 4 2 4 3 4 ## [21,] 4 5 5 4 5 4 4 0 4 2 3 2 ## [22,] 5 3 3 2 3 2 2 4 0 4 3 4 ## [23,] 4 5 5 4 5 4 4 2 4 0 3 2 ## [24,] 4 4 4 3 4 3 3 3 3 3 0 3 ## [25,] 4 5 5 4 5 4 4 2 4 2 3 0 ## [26,] 6 2 2 3 2 3 3 5 3 5 4 5 ## [27,] 4 4 4 3 4 3 3 3 3 3 2 3 ## [28,] 4 4 4 3 4 3 3 3 3 3 2 3 ## [29,] 2 4 4 3 4 3 3 2 3 2 2 2 ## [30,] 4 4 4 3 4 3 3 3 3 3 2 3 ## [31,] 5 3 3 2 3 2 2 4 2 4 3 4 ## [32,] 2 6 6 5 6 5 5 4 5 4 4 4 ## [33,] 4 4 4 3 4 3 3 3 3 3 2 3 ## [34,] 5 3 3 2 3 2 2 4 2 4 3 4 ## [35,] 1 5 5 4 5 4 4 3 4 3 3 3 ## [36,] 5 1 1 2 1 2 2 4 2 4 3 4 ## [37,] 3 3 3 2 3 2 2 2 2 2 1 2 ## [38,] 3 4 4 3 4 3 3 1 3 1 2 1 ## [39,] 4 2 2 1 2 1 1 3 1 3 2 3 ## [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] ## [1,] 3 3 3 3 3 2 5 3 2 4 2 2 ## [2,] 3 3 3 3 3 2 5 3 2 4 2 2 ## [3,] 4 2 2 2 2 3 4 2 3 3 3 1 ## [4,] 5 3 3 2 3 4 4 3 4 3 4 2 ## [5,] 5 3 3 2 3 4 4 3 4 3 4 2 ## [6,] 6 4 4 2 4 5 2 4 5 1 5 3 ## [7,] 4 2 2 2 2 3 4 2 3 3 3 1 ## [8,] 5 3 3 2 3 4 4 3 4 3 4 2 ## [9,] 6 4 4 2 4 5 2 4 5 1 5 3 ## [10,] 3 3 3 3 3 2 5 3 2 4 2 2 ## [11,] 5 3 3 2 3 4 4 3 4 3 4 2 ## [12,] 5 3 3 2 3 4 4 3 4 3 4 2 ## [13,] 2 4 4 4 4 3 6 4 3 5 1 3 ## [14,] 6 4 4 2 4 5 2 4 5 1 5 3 ## [15,] 2 4 4 4 4 3 6 4 3 5 1 3 ## [16,] 2 4 4 4 4 3 6 4 3 5 1 3 ## [17,] 3 3 3 3 3 2 5 3 2 4 2 2 ## [18,] 2 4 4 4 4 3 6 4 3 5 1 3 ## [19,] 3 3 3 3 3 2 5 3 2 4 2 2 ## [20,] 3 3 3 3 3 2 5 3 2 4 2 2 ## [21,] 5 3 3 2 3 4 4 3 4 3 4 2 ## [22,] 3 3 3 3 3 2 5 3 2 4 2 2 ## [23,] 5 3 3 2 3 4 4 3 4 3 4 2 ## [24,] 4 2 2 2 2 3 4 2 3 3 3 1 ## [25,] 5 3 3 2 3 4 4 3 4 3 4 2 ## [26,] 0 4 4 4 4 3 6 4 3 5 1 3 ## [27,] 4 0 2 2 2 3 4 2 3 3 3 1 ## [28,] 4 2 0 2 2 3 4 2 3 3 3 1 ## [29,] 4 2 2 0 2 3 2 2 3 1 3 1 ## [30,] 4 2 2 2 0 3 4 2 3 3 3 1 ## [31,] 3 3 3 3 3 0 5 3 2 4 2 2 ## [32,] 6 4 4 2 4 5 0 4 5 1 5 3 ## [33,] 4 2 2 2 2 3 4 0 3 3 3 1 ## [34,] 3 3 3 3 3 2 5 3 0 4 2 2 ## [35,] 5 3 3 1 3 4 1 3 4 0 4 2 ## [36,] 1 3 3 3 3 2 5 3 2 4 0 2 ## [37,] 3 1 1 1 1 2 3 1 2 2 2 0 ## [38,] 4 2 2 1 2 3 3 2 3 2 3 1 ## [39,] 2 2 2 2 2 1 4 2 1 3 1 1 ## [,38] [,39] ## [1,] 3 1 ## [2,] 3 1 ## [3,] 2 2 ## [4,] 1 3 ## [5,] 1 3 ## [6,] 3 4 ## [7,] 2 2 ## [8,] 1 3 ## [9,] 3 4 ## [10,] 3 1 ## [11,] 1 3 ## [12,] 1 3 ## [13,] 4 2 ## [14,] 3 4 ## [15,] 4 2 ## [16,] 4 2 ## [17,] 3 1 ## [18,] 4 2 ## [19,] 3 1 ## [20,] 3 1 ## [21,] 1 3 ## [22,] 3 1 ## [23,] 1 3 ## [24,] 2 2 ## [25,] 1 3 ## [26,] 4 2 ## [27,] 2 2 ## [28,] 2 2 ## [29,] 1 2 ## [30,] 2 2 ## [31,] 3 1 ## [32,] 3 4 ## [33,] 2 2 ## [34,] 3 1 ## [35,] 2 3 ## [36,] 3 1 ## [37,] 1 1 ## [38,] 0 2 ## [39,] 2 0 This output is just like an adjacency matrix, with row and columns being the network node IDs (net_id in our attributes dataframe). Next we would want sum all the columns for each row (so adding up all the geodesics for a node), and divide by the total number of nodes it can have an edge with to get the average geodesic distance for that node. This gives us the average geodesic distance for each node! # colsums gives us the sum of all columns for a row # we subtract one from the denominator becasue a node cannot have a geodesic distance with itself colSums(geodist(toy_network)$gdist) / (nrow(as.sociomatrix(toy_network)) - 1) ## [1] 3.131579 3.131579 2.947368 3.342105 3.342105 4.184211 2.947368 3.342105 ## [9] 4.184211 3.131579 3.342105 3.342105 3.842105 4.184211 3.842105 3.842105 ## [17] 3.131579 3.842105 3.131579 3.131579 3.342105 3.131579 3.342105 2.947368 ## [25] 3.342105 3.842105 2.947368 2.947368 2.447368 2.947368 3.131579 4.184211 ## [33] 2.947368 3.131579 3.210526 2.868421 1.973684 2.368421 2.157895 Lets add thus to our network and plot it. I will also add some color and labels so it is easier to see what this measure means. The red node has the longest average geodesic distance, and would need to travel through the whole network to reach the nodes on the opposite side. Meanwhile, the blue node has the smallest average geodesic distance because it is located near the middle of the network. # add mean geodesic distance to network object toy_network%v%&quot;mean_distance&quot; = (colSums(geodist(toy_network)$gdist))/(nrow(as.sociomatrix(toy_network)) - 1) # set all node colors in visNetwork to grey as default toy_attributes$color = c(&quot;grey&quot;) # add label as geodesic distance, rounding to 3 digits toy_attributes$label = round(toy_network%v%&quot;mean_distance&quot;, 3) # replace min average geodesic with blue, max with red toy_attributes$color[which(toy_network%v%&quot;mean_distance&quot; == max(toy_network%v%&quot;mean_distance&quot;))] = &quot;red&quot; toy_attributes$color[which(toy_network%v%&quot;mean_distance&quot; == min(toy_network%v%&quot;mean_distance&quot;))] = &quot;blue&quot; # make sure edges are grey too toy_edgelist$color = &quot;grey&quot; # plot visNetwork(toy_attributes, toy_edgelist, main = &quot;Geodesic Example&quot;) %&gt;% visInteraction(zoomView = FALSE) Note that while there is a correlation between degree counts (node size) and mean geodesic distance, one does not cause the other. This is our first instance of how network structure, not node attributes, can inform us about the nodes in a network. Essentially, looking at the network as a whole can tell us things about the people in it that is lost if we look only at individuals. 16.5.3 Centrality Centrality scores encompass a wide range of measures computed at the node level. Each tries to understand the importance of a single node within the structure of a network by looking at the nodes connection patterns to other nodes. Any centrality measure can be used to create a network level centralization score like we discussed above. We will go through some of the common centrality measures here, but know there are several more we will not cover. 16.5.3.1 Betweenness Centrality Betweenness Centrality is one of the most common centrality measures. It tries to calculate the extent to which a node acts as a gatekeeper or broker in the network. A broker bridges two otherwise disconnected segments in a network. If there are two parts of a network that would otherwise be broken apart if a node was removed, they would have a high betweenness centrality. The fragmenting of a network is not a prerequisite however, simply acting as an effective “shortcut” in a network can also raise a node’s betweenness centrality. Betweenness is calculated using geodesic distances, and gives a higher score to nodes that lie on more geodesic paths. The next network plot shows the size of nodes as their degree, with a label showing their betweenness centrality score. Centrality score are usually normalized such that their scores all sum to 1. This way, you can easily compare nodes within the network (but not between networks), and understand how nodes relate to each other structurally. It is possible for a node to have a 0 betweenness score if no geodesic distances pass through them. Like last time I’ve colored the nodes so that the node with the highest betweenness centrality is red, while the lowest is blue. Compared to distance however, it is considered advantageous to have a high betweenness centrality, as this means that nodes acts as a gatekeeper in the network, which can be a powerful position. Contrast this with having a low mean distance, which meant you were closer to all other nodes. # add eigenvector centrality to network object as &quot;norm_betweenness&quot; # we also tell it we are treating our data as un-directed (&quot;graph&quot;), rather than the default directed (&quot;digraph&quot;), same with `cmode = &quot;undirected&quot;` # we also say we want a normalized (0-1, sum to 1) score using `rescale = TRUE` toy_network%v%&quot;norm_betweenness&quot; = betweenness(dat = toy_network, gmode = &quot;graph&quot;, rescale = TRUE, cmode = &quot;undirected&quot;) # add label as geodesic distance, rounding to 3 digits toy_attributes$label = round(toy_network%v%&quot;norm_betweenness&quot;, 3) # reset all nodes to grey toy_attributes$color = c(&quot;grey&quot;) # replace min average geodesic with blue, max with red toy_attributes$color[which(toy_network%v%&quot;norm_betweenness&quot; == max(toy_network%v%&quot;norm_betweenness&quot;))] = &quot;red&quot; toy_attributes$color[which(toy_network%v%&quot;norm_betweenness&quot; == min(toy_network%v%&quot;norm_betweenness&quot;))] = &quot;blue&quot; # plot visNetwork(toy_attributes, toy_edgelist, main = &quot;Betweenness Centrality Example&quot;) %&gt;% visInteraction(zoomView = FALSE) 16.5.3.2 Eigenvector Centrality Eigenvector Centrality is commonly known as a measure of “popular friends.” Rather than looking at the network position of a node, it looks at the network positions of nodes connected to it. Nodes with a high eigenvector score will be connected to nodes more prominent in the network. Nodes with low degree can have high eigenvector scores if they are connected to important nodes. In real life networks this can be interpreted as being close to influential others in a network. I’ve colored the nodes so that the node with the highest eigenvector centrality is red, while the lowest is blue. It is considered advantageous to have a high eigenvector centrality, as this means you are well connected to other popular nodes. # add eigenvector centrality to network object as &quot;evc&quot; # we also tell it we are treating our data as un-directed (&quot;graph&quot;), rather than the default directed (&quot;digraph&quot;) # we also say we want a normalized (0-1, sum to 1) score using `rescale = TRUE` toy_network%v%&quot;evc&quot; = evcent(toy_network, gmode = &quot;graph&quot;, rescale = TRUE) # add label as geodesic distance, rounding to 3 digits toy_attributes$label = round(toy_network%v%&quot;evc&quot;, 3) # reset all nodes to grey toy_attributes$color = c(&quot;grey&quot;) # replace min average geodesic with blue, max with red toy_attributes$color[which(toy_network%v%&quot;evc&quot; == max(toy_network%v%&quot;evc&quot;))] = &quot;red&quot; toy_attributes$color[which(toy_network%v%&quot;evc&quot; == min(toy_network%v%&quot;evc&quot;))] = &quot;blue&quot; # plot visNetwork(toy_attributes, toy_edgelist, main = &quot;Eigenvector Centrality Example&quot;) %&gt;% visInteraction(zoomView = FALSE) 16.6 Network Workflow We have been taking our analyses one step at a time and plotting them. This is useful for learning, but slightly annoying in practice. Below I’ve aggregated how you would actually run analyses in practice so you can refer to it later. # data load toy_edgelist = read.csv(&quot;./data/toy_edgelist.csv&quot;, header = TRUE, stringsAsFactors = FALSE) toy_attributes = read.csv(&quot;./data/toy_attributes.csv&quot;, header = TRUE, stringsAsFactors = FALSE) # make a network ## sort your attributes frame alphabetically. Super important! toy_attributes = toy_attributes[order(toy_attributes$id), ] ## make network! toy_network_total = network(toy_edgelist, directed = FALSE) # largest component ## find what nodes are part of the largest component toy_network_total%v%&quot;lc&quot; = component.largest(toy_network_total) ## delete those nodes that are not ### in the network toy_network = delete.vertices(toy_network_total, which(toy_network_total%v%&quot;lc&quot; == FALSE)) ### in our dataframes toy_attributes = toy_attributes[toy_attributes$id %in% as.character(toy_network_total%v%&quot;vertex.names&quot;),] toy_edgelist = toy_edgelist[which(toy_edgelist$to %in% toy_attributes$id | toy_edgelist$from %in% toy_attributes$id),] # generate measures ## degree toy_network_total%v%&quot;degree&quot; = degree(toy_network_total) ## mean geodesic toy_network_total%v%&quot;mean_distance&quot; = (colSums(geodist(toy_network_total)$gdist)) / (nrow(as.sociomatrix(toy_network_total)) - 1) ## normalized betweenness toy_network_total%v%&quot;norm_betweenness&quot; = betweenness(dat = toy_network_total, gmode = &quot;graph&quot;, rescale = TRUE, cmode = &quot;undirected&quot;) ## eigenvector toy_network_total%v%&quot;evc&quot; = evcent(toy_network_total, gmode = &quot;graph&quot;, rescale = TRUE) # add back to attributes dataframe ## degree toy_attributes$degree = toy_network_total%v%&quot;degree&quot; ## mean geodesic toy_attributes$mean_distance = toy_network_total%v%&quot;mean_distance&quot; ## normalized betweenness toy_attributes$norm_betweenness = toy_network_total%v%&quot;norm_betweenness&quot; ## eigenvector toy_attributes$evc = toy_network_total%v%&quot;evc&quot; Finally, we can then look at the network measures for our nodes. This dataframe can be used for plotting or further analyses. kable(head(toy_attributes)) id year color degree mean_distance norm_betweenness evc 22 028f5d1f351d38cd6553ab4674b19725d5ea3d3c NA NA 2 3.131579 0 0.0185655 15 19d5b2694036f6fab966564c1c44bc74330f22c2 NA NA 2 3.131579 0 0.0185655 30 1ae1327030b801f0046278d197378603b51de4b7 NA NA 2 2.947368 0 0.0250892 67 258f00e649e6a452acb67cb9297c88820c05e2a7 NA NA 2 3.342105 0 0.0223477 65 2e9fed34d6b2d42052850b46aeaa97f9fe6542dc NA NA 2 3.342105 0 0.0223477 75 3220545023e21c80db2a4d4e10b3eb4217b90605 NA NA 2 4.184210 0 0.0030798 16.7 Network Tools There are several ways to interact with network data in R. Thus far we have been using a combination of statnet for analysis and visNetwork for visualization. Here we’ll gloss over some other tools and what they are used for. Rather than a comprehensive tutorial, this section is just meant to introduce you to what tools are out there so you can investigate them further if you have a need fro them. 16.7.1 Network Models 16.7.1.1 statnet statnet is one of the two largest network packages in R. It allows you to create network objects and generate the network measures we’ve been looking at this far. statnet’s claim to fame however is it’s ability to run network simulations, called exponential random graph models (ERGMs). These models allow you to keep some aspect of a network constant and generate random networks that fit your specifications. This can help you highlight one structural aspect of a network and prove that, all else being random, it is important. To learn more about ERGMs, see (Robins, Pattison, et al. 2007; Robins, Snijders, et al. 2007). Learn more on the statnet website. 16.7.1.2 igraph igraph is the other big network package in R. It has more network measures than statnet, but less robust simulation capabilities. While the same network concepts you’ve learned with statnet will help you understand all networks, the code syntax for igraph is different, so you can’t use the two tools interchangeably. Notably, some functions are named the same in statnet and igraph, so it is advised not to load both at the same time. Learn more on the igraph website. 16.7.1.3 intergraph intergraph is a utility package in R to convert between statnet and igraph network objects. This means you can prepare your data in your package of choice, then convert your network and use what tools you need from the other package. 16.7.1.4 tidygraph tidygraph is a relatively new tool in R, built to use tidyverse syntax. It can do many of the same basic network analyses of the two older packages, but lacks the variety of igraph or the simulation capability of statnet. Learn more on the tidygraph website. 16.7.2 Network Visualization 16.7.2.1 Built-in While we avoided it today, all network packages have built in visualization capabilities that can look nice if you work on it. The advantage of these is that you can use the network objects themselves to pull attributes from the networks for your plots (e.g. pull degree centrality for node size). 16.7.2.2 visNetwork visNetwork can make some nice interactive network visualizations with relatively simple code. This is great for learning and for exploring networks interactively. However, it does have significant downsides. For one, you have to keep separate dataframes for your edges and attributes as it cannon run directly on network objects. Most importantly it cannot produce static network images! You will most likely need more static plots than interactive ones. If you can only dig deeply into one tool, this one may not be the best to specialize in. 16.7.2.3 ggraph ggraph uses ggplot syntax to plot networks. There are several packages that do this in various stages of development. To my understanding, ggraph is the most recent incarnation still under active development. Learn more of the ggraph website. 16.8 References "],["statistics.html", "17 Statistics 17.1 Introduction 17.2 Uses of simulation 17.3 Mathematical statistics 17.4 Statistical inference 17.5 Regression 17.6 Model selection 17.7 Cross-validation 17.8 References", " 17 Statistics 17.1 Introduction It is useful to begin with some concrete examples of statistical questions to motivate the material that we’ll cover in the workshop. This will also help confirm that your R environment is working. These data files come from the fosdata package, which you can optionally install to your computer in order to get access to all the associated metadata in the help files. I will show loading the data from the workshop website. # optional: install the fosdata and mosaicModels packages using remotes::install_github install.packages( &quot;remotes&quot; ) remotes::install_github( &quot;speegled/fosdata&quot; ) remotes::install_github( &quot;ProjectMOSAIC/mosaicModel&quot; ) library( &quot;ggformula&quot; ) library( &quot;mosaic&quot; ) library( &quot;mosaicModel&quot; ) # load the datasets mice_pot = read.csv( url( &quot;https://raw.githubusercontent.com/ucdavisdatalab/adventures_in_data_science/master/data/mice_pot.csv&quot; )) barnacles = read.csv( url( &quot;https://raw.githubusercontent.com/ucdavisdatalab/adventures_in_data_science/master/data/barnacles.csv&quot; )) Births78 = read.csv( url(&quot;https://raw.githubusercontent.com/ucdavisdatalab/adventures_in_data_science/master/data/births.csv&quot;)) smoking = read.csv( url(&quot;https://raw.githubusercontent.com/ucdavisdatalab/adventures_in_data_science/master/data/smoking.csv&quot;)) adipose = read.csv( url(&quot;https://raw.githubusercontent.com/ucdavisdatalab/adventures_in_data_science/master/data/adipose.csv&quot;)) 17.1.1 mice_pot dataset The mice_pot dataset contains data on an experiment that dosed four groups of mice with different levels of THC. There was a low, medium, and high dosage group, as well as a control group that got no THC. The mice were were then observed for a while and their total movement was quantified as a percentage of the baseline group mean. Some statistical questions that might arise here are: were there differences in the typical amount of movement between mice of different groups? What was the average amount of movement by mice in the medium dose group? Both of these questions can be approached in ways that relate to the particular sample, which is kind of interesting descriptions of the data. For instance, if you install the dplyr package in your R environment, we can make those calculations right now. with( mice_pot, by(percent_of_act, group, mean)) ## group: 0.3 ## [1] 97.3225 ## ------------------------------------------------------------ ## group: 1 ## [1] 99.05235 ## ------------------------------------------------------------ ## group: 3 ## [1] 70.66787 ## ------------------------------------------------------------ ## group: VEH ## [1] 100 The means aren’t identical - there are clearly differences between all of the groups? Yes, in terms of the sample. But if you want to generalize your conclusion to cover what would happen to other mice that weren’t in the study, then you need to think about the population. In this case, that’s the population of all the mice that could have been dosed with THC. Because we don’t see we don’t see data from mice that weren’t part of the study, we rely on statistical inference to reach conclusions about the population. How is that possible? Well, some theorems from mathematical statistics can tell us about the distribution of the sample, relative to the population 17.1.2 barnacles dataset This dataset was collected by counting the barnacles in 88 grid squares on the Flower Garden Banks coral reef in the Gulf of Mexico. The counts were normalized to barnacles per square meter. Some questions that you might approach with statistical methods are, what is the average number of barnacles per square meter, and is it greater than 300? mean( barnacles$per_m ) ## [1] 332.0186 From that calculation, we see that the mean is 332 barnacles per square meter, which is greater than 300. But again, the first calculation has told us only about the mean of the particular locations that were sampled. Wouldn’t it be better to answer the questions by reference to the number of barnaces per square meter of reef, rather than square meter of measurement? Here, the population is then entire area of the Flower Garden Banks reef. Again, we will be able to answer the questions relative to the entire reef by working out the sample mean’s distribution, relative to the population. 17.1.3 Sample and population I’ve made reference to samples and populations, and they are fundamental concepts in statistics. A sample is a pretty easy thing to understand - it is the data, the hard numbers that go into your calculations. The population is trickier. It’s the units to which you are able to generalize your conclusions. For the barnacle data, in order for the population to be the entire Flower Garden Banks reef, the sample must have been carefully selected to ensure it was representative of the entire reef (for instance, randomly sampling locations so that any location on the reef might have been selected). For the mice on THC, the population is all the mice that might have been selected for use in the experiment. How big that population is depends on how the mice were selected for the experiment. Randomly selecting the experimental units from a group is a common way of ensuring that the results can generalize to that whole group. A non-random sample tends to mean that the population to which you can generalize is quite limited. What sort of population do you think we could generalize about if we recorded the age of everyone in this class? 17.2 Uses of simulation The study of statistics started in the 1800s but only barely. The field’s modern version was almost entirely developed during the first half of the 20th century - notably a time when data and processing power were in short supply. Today, that’s not so much the case and if you did the assigned reading then you saw that statisticians are very much still grappling with how to teach statistics in light of the developments in computational power over the past 40 years. Traditionally, statisticians are very concerned with assessing the normality of a sample, because the conclusions you get from traditional statistical methods depend on a sample coming from a normal distribution. Nowadays, there are a lot of clever methods that can avoid the need to assume normality. I’m going to show you some of those methods, because they help avoid getting into mathematical statistics. If you want to know more, one of the assigned readings was the introduction to a book that would be a great reference for self-guided study. We will use simulation-based methods extensively today. This is the density curve of a standard normal distribution: # set z to -4, 4 z = seq(-4, 4, length.out = 1000) # plot the standard normal density function with some annotations plot( x=z, y=dnorm(z), bty=&#39;n&#39;, type=&#39;l&#39;, main=&quot;standard normal density&quot;) # annotate the density function with the 5% probability mass tails polygon(x=c(z[z&lt;=qnorm(0.025)], qnorm(0.025), min(z)), y=c(dnorm(z[z&lt;=qnorm(0.025)]), 0, 0), col=grey(0.8)) polygon(x=c(z[z&gt;=qnorm(0.975)], max(z), qnorm(0.975)), y=c(dnorm(z[z&gt;=qnorm(0.975)]), 0, 0), col=grey(0.8)) And this is a histogram of samples taken from that same distribution: # sample 20 numbers from a standard normal and draw the histogram x = rnorm(20) round( sort(x), 2) ## [1] -1.42 -1.32 -1.09 -0.82 -0.80 -0.26 -0.19 0.09 0.14 0.28 0.28 0.41 ## [13] 0.48 0.50 0.53 0.79 1.06 1.34 1.71 2.27 hist(x) Do the numbers seem to come from the high-density part of the Normal density curve? Are there any that don’t? It isn’t surprising if some of your x samples are not particularly close to zero. One out of twenty (that’s five percent) samples from a standard Normal population are greater than two or less than negative two, on average. That’s “on average” over the population. Your sample may be different. Here is the density of the exponential distribution: # draw the desity of an Exponential distribution t = seq(-1, 5, length.out=1000) plot( x=t, y=dexp(t), bty=&#39;n&#39;, type=&#39;l&#39;) And here is a histogram of 20 samples taken from that distribution: # sample 20 numbers from a histogram and plot the histogram ex = rexp( 20 ) round( sort(ex), 2) ## [1] 0.03 0.07 0.07 0.09 0.17 0.18 0.28 0.41 0.44 0.52 0.59 0.98 1.04 1.38 1.62 ## [16] 1.66 1.69 1.76 1.84 3.94 hist( ex ) The histograms are clearly different, but it would be difficult to definitively name the distribution of the data by looking at a sample. 17.3 Mathematical statistics The mean has some special properties: you’ve seen how we can calculate the frequency of samples being within an interval based on known distributions. But we need to know the distribution. It turns out that the distribution of the sample mean approaches the Normal distribution as the sample size increases, for almost any independent data. That allows us to create intervals and reason about the distribution of real data, even though the data’s distribution is unknown. 17.3.1 Law of large numbers The law of large numbers says that if the individual measurements are independent, then the mean of a sample tends toward the mean of the population as the sample size gets larger. This is what we’d expect, since we showed the rate at which the variance of the sample mean gets smaller is \\(1/n\\). nn = c(1, 2, 4, 8, 12, 20, 33, 45, 66, 100) means = sapply( nn, function(n) mean( rnorm(n) ) ) plot(nn, means, bty=&#39;n&#39;, ylab = &quot;sample mean&quot;) abline(h=0, lty=2) 17.3.2 Central limit theorem The most important mathematical result in statistics, the Central Limit Theorem says that if you take (almost) any sample of random numbers and calculate its mean, the distribution of the mean tends toward a normal distribution. We illustrate the “tending toward” with an arrow and it indicates that the distribution of a sample mean is only approximately Normal. But if the original samples were from a Normal distribution then the sample mean has an exactly Normal distribution. From here, I’ll start writing the mean of a random variable \\(X\\) as \\(\\bar{X}\\) and the mean of a sample \\(x\\) as \\(\\bar{x}\\). \\[ \\bar{X} \\rightarrow N(\\mu, \\frac{\\sigma^2}{n}) \\] And because of the identities we learned before, you can write this as \\[\\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\rightarrow N(0, 1) \\] This is significant because we can use the standard normal functions on the right, and the data on the left, to start answering questions like, “what is the 95% confidence interval for the population mean?” # generate 20 samples from a uniform distribution and plot their histogram N = 20 u = rexp( N ) hist( u ) # generate 100 repeated samples of the same size, calculate the mean of each one, and plot the histogram of the means. B = 100 uu = numeric( B ) for ( i in 1:B ) { uu[[i]] = mean( rexp(N) ) } hist(uu) # what happens as B and N get larger and smaller? Do they play different roles? 17.4 Statistical inference 17.4.1 Confidence intervals In the fosdata package there is a dataset called mice_pot, which contains data from an experiment where mice were dosed with THC and then measured for motor activity as a percentage of their baseline activity. We are going to look at the group that got a medium dose of THC. # extract just the mice that got the medium dose of THC mice_med = mice_pot[ mice_pot$group == 1, ] # assess normality with histogram and QQ plot hist( mice_med$percent_of_act ) qqnorm( mice_med$percent_of_act ) 17.4.1.1 Find the 80% confidence interval for the population mean Now we are using our sample to make some determination about the population, so this is statistical inference. Our best guess of the population mean is the sample mean, mean( mice_med$percent_of_act ), which is 99.1%. But to get a confidence interval, we need to use the formula \\[ \\bar{x} \\pm t_{n-1, 0.1} * S / \\sqrt{n} \\] Fortunately, R can do all the work for us: # 80% confidence interval for location of mice_med mean: t.test( mice_med$percent_of_act, conf.level=0.8 ) ## ## One Sample t-test ## ## data: mice_med$percent_of_act ## t = 13.068, df = 11, p-value = 4.822e-08 ## alternative hypothesis: true mean is not equal to 0 ## 80 percent confidence interval: ## 88.71757 109.38712 ## sample estimates: ## mean of x ## 99.05235 17.4.2 Two-population test The test of \\(\\mu_0 = 100\\) is a one-population test because it seeks to compare a single population against a specified standard. On the other hand, you may wish to assess the null hypothesis that the movement of mice in the high-THC group is equal to the movement of mice in the medium-THC group. This is called a two-population test, since there are two populations to compare against each other. The null hypothesis is \\(\\mu_{0, med} = \\mu_{0, high}\\). Testing a two-population hypothesis requires first assessing normality and also checking whether the variances are equal. There are separate procedures when the variances are equal vs. unequal. #extract the samples to be compared a = mice_pot$percent_of_act[ mice_pot$group == 1] b = mice_pot$percent_of_act[ mice_pot$group == 3] # check for equal variances - these are close enough var(a) ## [1] 689.4729 var(b) ## [1] 429.4551 # confirm equal variances with a boxplot boxplot(a, b) # check whether the high-THC mice movement is Normal # (we already checked for the medium-dose mice) qqnorm(b) # two pop test t.test(a, b, var.equal=TRUE) ## ## Two Sample t-test ## ## data: a and b ## t = 2.7707, df = 20, p-value = 0.0118 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 7.014608 49.754345 ## sample estimates: ## mean of x mean of y ## 99.05235 70.66787 17.4.3 Hypothesis tests for non-normal data Just as with the confidence intervals, there is a bootstrap hypothesis test that can be used where the data are not normal. There are other options, too, with clever derivations. The one I’ll show you is the Wilcoxon test, which is based on the ranks of the data. Since we’e already seen that the barnacles per square meter data are not normal, I will illustrate testing the null hypothesis that \\(\\mu_0\\) = 300 barnacles per square meter. This is a one-population test, and a two-sided alternative. # wilcoxon test for 300 barnacles per square meter wilcox.test( barnacles$per_m ) ## ## Wilcoxon signed rank test with continuity correction ## ## data: barnacles$per_m ## V = 3916, p-value = 3.797e-16 ## alternative hypothesis: true location is not equal to 0 17.5 Regression Regression is a mathematical tool that allows you to estimate how some response variable is related to some predictor variable(s). There are methods that handle continuous or discrete responses of many different distributions, but we are going to focus on linear regression here. Linear regression means that the relationship between the predictor variable(s) and the response is a linear one. To illustrate, we’ll create a plot of the relationship between the waist measurement and bmi of 81 adults: # plot the relationship between the waist_cm and bmi variables with(adipose, plot(waist_cm, bmi), bty=&#39;n&#39; ) The relationship between the two is apparently linear (you can imagine drawing a straight line through the data). The general mathematical form of a linear regression line is \\[ y = a + \\beta x + \\epsilon \\] Here, the response variable (e.g., BMI) is called \\(y\\) and the predictor (e.g. waist measurement) is \\(x\\). The coefficient \\(\\beta\\) indicates how much the response changes for a change in the predictors (e.g., the expected change in BMI with a 1cm change in waist measurement). Variable \\(a\\) denotes the intercept, which is a constant offset that aligns the mean of \\(y\\) with the mean of \\(x\\). Finally, \\(\\epsilon\\) is the so-called residual error in the relationship. It represents the variation in the response that is not due to the predictor(s). 17.5.1 Fitting a regression line The R function to fit the model is called lm(). Let’s take a look at an example: # fit the linear regression BMI vs waist_cm fit = lm( bmi ~ waist_cm, data=adipose ) # plot the fitted regression: begin with the raw data with( adipose, plot(waist_cm, bmi, bty=&#39;n&#39;) ) #now plot the fitted regression line (in red) abline( coef(fit)[[1]], coef(fit)[[2]], col=&#39;red&#39; ) 17.5.2 Assumptions and diagnostics “Fitting” a linear regression model involves estimating \\(a\\) and \\(\\beta\\) in the regression equation. You can can do this fitting procedure using any data, but the results won’t be reliable unless some conditions are met. The conditions are: Observations are independent. The linear model is correct. The residual error is Normally distributed. The variance of the residual error is constant for all observations. The first of these conditions can’t be checked - it has to do with the design of the experiment. The rest can be checked, though, and I’ll take them in order. 17.5.2.1 Checking that the linear model is correct In the cast of a simple linear regression model (one predictor variable), you can check this by plotting the predictor against the response and looking for a linear trend. If you have more than one predictor variable, then you need to plot the predictions against the response to look for a linear trend. We’ll see an example by adding height as a predictor for BMI (in addition to waist measurement). # linear model for BMI using waist size and height as predictors fit2 = lm( bmi ~ waist_cm + stature_cm, data=adipose ) # plot the fitted versus the predicted values plot( fit2$fitted.values, adipose$bmi, bty=&#39;n&#39; ) 17.5.2.2 Checking that the residuals are normally distributed We have already learned about the QQ plot, which shows visually whether some values are Normally distributed. In order to depend upon the fit from a linear regression model, we need to see that the residuals are Normally distributed, and we use the QQ plot to check. 17.5.2.3 Checking that the vaiance is constant In an earlier part, we saw that the variance is the average of the squared error. But that would just be a single number, when we want to see if there is a trend. So like the QQ plot, you’l plot the residuals and use your eyeball to discern whether there is a trend in the residuals or if they are approximately constant - this is called the scale-location plot. The QQ plot and scale-location plot are both created by plotting the fitted model object # set up the pattern of the panels layout( matrix(1:4, 2, 2) ) # make the diagnostic plots plot( fit ) The “Residuals vs. Fitted” plot is checking whether the linear model is correct. There should be no obvious pattern if the data are linear (as is the casre here). The Scale-Location plot will have no obvios pattern if the variance of the residuals is constant, as is the case here (you might see a slight pattern in the smoothed red line but it isn’t obvious). And the QQ plot will look like a straight line if the residuals are from a Normal distribution, as is the case here. So this model is good. The fourth diagnostic plot is the Residuals vs. Leverage plot, which is used to identify influential outliers. We won’t get into that here. 17.5.3 Functions for inspecting regression fits When you fit a linear regression model, you are estimating the parameters of the regression equation. In order to see those estimates, use the summary() function on the fitted model object. # get the model summary summary( fit2 ) ## ## Call: ## lm(formula = bmi ~ waist_cm + stature_cm, data = adipose) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.1290 -1.0484 -0.2603 1.2661 5.2572 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.38196 3.82700 3.758 0.000329 *** ## waist_cm 0.29928 0.01461 20.491 &lt; 2e-16 *** ## stature_cm -0.08140 0.02300 -3.539 0.000680 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.724 on 78 degrees of freedom ## Multiple R-squared: 0.844, Adjusted R-squared: 0.84 ## F-statistic: 211 on 2 and 78 DF, p-value: &lt; 2.2e-16 Here you can see that the average marginal effect of one additional centimeter of waist measurement is to increase BMI by 0.3 and an additional centimeter of height is associated with a change to BMI of -0.08. You can get the coefficients from the fitted model object using the coef() function, and there are some other functions that allow you to generate the values shown in the summary table. # get the coefficients of the fitted regression beta = coef( fit2 ) round( beta, 2 ) ## (Intercept) waist_cm stature_cm ## 14.38 0.30 -0.08 Get the variance-covariance matrix: round( vcov( fit2 ), 4) ## (Intercept) waist_cm stature_cm ## (Intercept) 14.6459 -0.0039 -0.0836 ## waist_cm -0.0039 0.0002 -0.0001 ## stature_cm -0.0836 -0.0001 0.0005 # compare the square root of the diagonals of the variance-covariance matrix # to the standard errors are reported in the summary table: se = sqrt( diag( vcov(fit2) )) # here are the standard errors: round( se, 3 ) ## (Intercept) waist_cm stature_cm ## 3.827 0.015 0.023 # calculate the t-statistics for the regression coefficients # (compare these to the t-statistics reorted in the summary table) t_stats = beta / se # show the t-statistics: round( t_stats, 2 ) ## (Intercept) waist_cm stature_cm ## 3.76 20.49 -3.54 # calculate the p-values: pval = 2 * pt( abs(t_stats), df=78, lower.tail=FALSE ) round(pval, 4) ## (Intercept) waist_cm stature_cm ## 3e-04 0e+00 7e-04 # this is the residual standard error: sd( fit2$residuals ) * sqrt(80 / 78) ## [1] 1.72357 # R-squared is the proportion of variance # explained by the regression model round( 1 - var(fit2$residuals) / var(adipose$bmi), 3 ) ## [1] 0.844 17.5.4 A model that fails diagnostics We’ve seen a model that has good diagnostics. Now let’s look at one that doesn’t. This time, we’ll use linear regression to make a model of the relationship between waist measurement and the visceral adipose tissue fat (measured in grams). The visceral adipose tissue fat is abbreviated vat in the data. First, since the model uses a single predictor variable, let’s look at the relationship with a pair plot. # plot the relationship between waist_cm and vat with( adipose, plot( waist_cm, vat, bty=&#39;n&#39; )) The plot is obviously not showing a linear relationship, which will violate one of the conditions for linear regression. Also, you can see that there is less variance of vat among the observations that have smaller waist measurements. So that will violate the assumption that the residual variance has no relationship to the fitted values. To see how these will show up in the diagnostic plots, we need to fit the linear regression model. # estimate the model for vat fit_vat = lm( vat ~ waist_cm, data = adipose ) # there is no problem creating the summary table: summary( fit_vat ) ## ## Call: ## lm(formula = vat ~ waist_cm, data = adipose) ## ## Residuals: ## Min 1Q Median 3Q Max ## -996.25 -265.96 -61.87 191.24 1903.46 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3604.196 334.241 -10.78 &lt;2e-16 *** ## waist_cm 51.353 3.937 13.04 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 479 on 79 degrees of freedom ## Multiple R-squared: 0.6829, Adjusted R-squared: 0.6789 ## F-statistic: 170.2 on 1 and 79 DF, p-value: &lt; 2.2e-16 # show the diagnostic plots layout( matrix(1:4, 2, 2) ) plot( fit_vat ) There is obviously a curved pattern in the Residuals vs. Fitted plot, and in the Scale vs. Location plot. Residuals vs. Fitted shows a fan-shaped pattern, too, which reflects the increasing variance among the greater fitted values. The QQ plot is not a straight line, although the difference is not as obvious. In particular, the upper tail of residuals is heavier than expected. Together, all of these are indications that we may need to do a log transformation of the response. A log transformation helps to exaggerate the differences between smaller numbers (make the lower tail heavier) and collapse some difference among larger numbers (make the upper tail less heavy). # fit a regression model where the response is log-transformed fit_log = lm( log(vat) ~ waist_cm, data=adipose ) # plot the diagnostics for the log-transformed model plot( fit_log ) The diagnostics do not look good after the log transformation, but now the problem is the opposite: a too-heavy lower tail and residual variance decreases as the fitted value increases. Perhaps a better transformation is something in between the raw data and the log transform. Try a square-root transformation. # fit a model where the vat is square root transformed fit_sqrt = lm( sqrt(vat) ~ waist_cm, data=adipose ) # plot the diagnostics for the log-transformed model plot( fit_sqrt ) These look acceptable for real-world data. 17.5.5 Predictions and variability There are two scales of uncertainty for a regression model: uncertainty in the fitted relationship, and the uncertainty of a predicted outcome. The uncertainty of a prediction is always greater because it is calculated by adding the uncertainty of the fitted line to the uncertainty of a single data point around that fitted line. We can illustrate using the example of the model we just created to relate the waist measurement to the square root of vat. For this block of code, we’ll need the mvtnorm library to be loaded. # import mvtnorm. install it if necessary. library( mvtnorm ) # draw the data on the transformed scale with( adipose, plot(waist_cm, sqrt(vat), bty=&#39;n&#39;) ) # plot the fitted regression line abline( coef(fit_sqrt)[[1]], coef(fit_sqrt)[[2]], col=&#39;red&#39; ) # plot 100 samples from the distributon of the regression line. for (i in 1:100) { cc = rmvnorm( n=1, mean=coef(fit_sqrt), sigma=vcov(fit_sqrt) ) abline( cc[[1]], cc[[2]], col=grey(0.8)) } Clearly, the variability of the data points is greater than the variability of the fitted line (that’s why they lie outside the envelope of the fitted lines). We can extract a confidence interval for fitted values or predictions with the predict() function. # draw the data on the transformed scale with( adipose, plot(waist_cm, sqrt(vat), bty=&#39;n&#39;) ) # plot the fitted regression line abline( coef(fit_sqrt)[[1]], coef(fit_sqrt)[[2]], col=&#39;red&#39; ) # define some waist measurements where we&#39;ll construct confidence intervals pred_pts = data.frame( waist_cm = c(70, 85, 110) ) # calculate the 90% CI at each of the pred_pts ff = predict(fit_sqrt, pred_pts, interval=&quot;confidence&quot;, level=0.9) pp = predict(fit_sqrt, pred_pts, interval=&quot;prediction&quot;, level=0.9) # convert the confidence intervals to data.frames ff = as.data.frame(ff) pp = as.data.frame(pp) # add the three confidence intervals to the plots # (offset them a bit for clarity in the plot) for (i in 1:3) { lines( x=rep( pred_pts$waist_cm[[i]] - 0.5, 2), y=c( ff$lwr[[i]], ff$upr[[i]] ), col=&#39;blue&#39;, lwd=2 ) lines( x=rep( pred_pts$waist_cm[[i]] + 0.5, 2), y=c( pp$lwr[[i]], pp$upr[[i]] ), col=&#39;orange&#39;, lwd=2 ) } # add a legend legend(c(&quot;90% CI (fitted values)&quot;, &quot;90% CI (predicted values)&quot;), x=&quot;topleft&quot;, lwd=2, col=c(&quot;blue&quot;, &quot;orange&quot;), bty=&#39;n&#39;) One thing to notice about the confidence intervals is that the interval is smallest (so the precision of the estimation is greatest) at the mean of the predictor variable. This is a general rule of fitting regression. 17.6 Model selection Choosing how to represent your data is a common task in statistics. The most common target is to choose the representation (or model) that does the best job of predicting new data. We set this target because if we have a representation that predicts the future, then we can say it must accurately represent the process that generates the data. 17.7 Cross-validation Unfortunately, we rarely have information about the future, so there isn’t new data to predict. One way to do prediction with the available data is to break it into a training part and a testing part. You make represent the training part with a model, and then use it to predict the left-out testing part. If you then swap the to parts and repeat the process, you’ll have a prediction for every data point. This would be called two-fold cross valdation because the data was broken into two parts. It’s more common to break the data into more than two parts - typically five or ten or one per data point. Then one part is taken as the testing part and all the others go into the training part. The result is five-fold or ten-fold, or leave-one-out cross validation. Let’s use cross-validation to do model selection. The model this time is a representation of the number of births per day in 1978 in the United States. # plot the data gf_point( births ~ day_of_year, color = ~wknd, data=Births78 ) # make models with two through ten knots in the spline for day_of_year bmod2 = lm( births ~ wknd + ns(day_of_year, 2), data=Births78 ) bmod4 = lm( births ~ wknd + ns(day_of_year, 4), data=Births78 ) bmod6 = lm( births ~ wknd + ns(day_of_year, 6), data=Births78 ) bmod8 = lm( births ~ wknd + ns(day_of_year, 8), data=Births78 ) bmod10 = lm( births ~ wknd + ns(day_of_year, 10), data=Births78 ) # plot the 2 and 10 knot models mod_plot(bmod2, births~day_of_year + wknd) + geom_point(mapping=aes(x=day_of_year, y=births, color=wknd), data=Births78) mod_plot(bmod10, births~day_of_year + wknd) + geom_point(mapping=aes(x=day_of_year, y=births, color=wknd), data=Births78) # cross-validate to choose the best model mod_cv( bmod2, bmod4, bmod6, bmod8, bmod10, k=nrow(Births78), ntrials=1 ) ## mse model ## 1 190815.9 bmod2 ## 2 143305.1 bmod4 ## 3 104875.7 bmod6 ## 4 106094.2 bmod8 ## 5 107130.5 bmod10 # plot the data mod_plot(bmod6, births~day_of_year + wknd) + geom_point(mapping=aes(x=day_of_year, y=births, color=wknd), data=Births78) Cross-validation suggests that six knots is the ideal number, because it has the smallest mean-squared error (mse). The resulting model looks good, too. 17.8 References "],["data-forensics.html", "18 Data Forensics 18.1 Introduction 18.2 Tidy Data 18.3 Data Types 18.4 Special Values 18.5 Outliers", " 18 Data Forensics After this lesson, you should be able to: Cleaning: Explain what it means for a data set to be “tidy” Pivot columns in a data set to make it tidy Separate values in a column that contains multiple values per cell Convert columns to appropriate data types Forensics: Locate and count missing values in a data set Explain what it means for a value to be an “outlier” Locate and count outliers in a data set 18.1 Introduction This lesson focuses on how to identify, diagnose, and fix potential problems in tabular data sets. There are several different kinds of problems that can arise: Structural (data are transposed or rotated) Data types (some columns have the wrong types) Missing values Outliers (extreme values) We’ll see examples of each of these. 18.2 Tidy Data The Tidyverse is so named because many functions in Tidyverse packages require data frames that are in tidy form. Before we see the requirements for a data set to be tidy, we need to define or review some terminology from statistics. A feature (also called a covariate or a variable) is measurement of something, usually across multiple subjects. For example, we might decide to measure the heights of everyone in the class. Each person in the class is a subject, and the height measurement is a feature. Features don’t have to be quantitative. If we also asked each person their favorite color, then favorite color would be another feature in our data set. Features are usually, but not always, the columns in a tabular data set. An observation is a set of features measured for a single subject or at a single time. So in the preceding example, the combined height and favorite color measurement for one student is one observation. Observations are usually, but not always, the rows in a tabular data set. Now we can define what it means to be tidy. A tabular data set is tidy if and only if: Each observation has its own row. Each feature has its own column. Each value has its own cell. These rules ensure that all of the values are visually organized and are easy to access with R’s built-in indexing operations. For instance, the $ operator gets a column, and in a tidy data set, columns are features. The rules also reflect the way statisticians traditionally arrange tabular data sets. Let’s look at some examples of tidy and untidy data sets. The tidyr package provides examples, and as we’ll see later, it also provides functions to make untidy data sets tidy. As usual, we first need to load the package: # install.packages(&quot;tidyr&quot;) library(tidyr) Let’s start with an example of tidy data. This data set is included in the tidyr package and records the number of tuberculosis cases across several different countries and years: table1 ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 When you first look at a data set, think about what the observations are and what the features are. If the data set comes with documentation, it may help you figure this out. Since this data set is a tidy data set, we already know each row is an observation and each column is a feature. Features in a data set tend to take one of two roles. Some features are identifiers that describe the observed subject. These are usually not what the researcher collecting the data is trying to find out. For example, in the tuberculosis data set, the country and year columns are identifiers. Other features are measurements. These are usually the reason the researcher collected the data. For the tuberculosis data set, the cases and population columns are measurements. Thinking about whether features are identifiers or measurements can be helpful when you need to use tidyr to rearrange a data set. 18.2.1 Columns into Rows Tidy data rule 1 says each observation must have its own row. Here’s a table that breaks rule 1: table4a ## # A tibble: 3 x 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 All of the numbers measure the same thing: cases. To make the data tidy, we must rotate the 1999 and 2000 column names into rows, one for each value in the columns. The new columns are year and cases. This process means less columns (generally) and more rows, so the data set becomes longer. We can use the pivot_longer function to rotate columns into rows. We need to specify: Columns to rotate as cols. Name(s) of new identifier column(s) as names_to. Name(s) of new measuerment column(s) as values_to. Here’s the code: pivot_longer(table4a, -country, names_to = &quot;year&quot;, values_to = &quot;cases&quot;) ## # A tibble: 6 x 3 ## country year cases ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 ## 2 Afghanistan 2000 2666 ## 3 Brazil 1999 37737 ## 4 Brazil 2000 80488 ## 5 China 1999 212258 ## 6 China 2000 213766 18.2.1.1 How to Pivot Longer without tidyr You also can do this without tidyr: Subset columns to separate 1999 and 2000 into two data frames. Add a year column to each. Rename the 1999 and 2000 columns to cases. Stack the two data frames with rbind. # Step 1 df99 = table4a[-3] df00 = table4a[-2] # Step 2 df99$year = &quot;1999&quot; df00$year = &quot;2000&quot; # Step 3 names(df99)[2] = &quot;cases&quot; names(df00)[2] = &quot;cases&quot; # Step 4 rbind(df99, df00) ## # A tibble: 6 x 3 ## country cases year ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Afghanistan 745 1999 ## 2 Brazil 37737 1999 ## 3 China 212258 1999 ## 4 Afghanistan 2666 2000 ## 5 Brazil 80488 2000 ## 6 China 213766 2000 18.2.2 Rows into Columns Tidy data rule 2 says each feature must have its own column. Let’s look at a table that breaks rule 2: table2 ## # A tibble: 12 x 4 ## country year type count ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 cases 745 ## 2 Afghanistan 1999 population 19987071 ## 3 Afghanistan 2000 cases 2666 ## 4 Afghanistan 2000 population 20595360 ## 5 Brazil 1999 cases 37737 ## 6 Brazil 1999 population 172006362 ## 7 Brazil 2000 cases 80488 ## 8 Brazil 2000 population 174504898 ## 9 China 1999 cases 212258 ## 10 China 1999 population 1272915272 ## 11 China 2000 cases 213766 ## 12 China 2000 population 1280428583 Here the count column contains two different features: cases and population. To make the data tidy, we must rotate the count values into columns, one for each type value. New columns are cases and population. This process means less rows and more columns, so the data set becomes wider. We can use pivot_wider to rotate rows into columns. We need to specify: Column names to rotate as names_from. Measurements to rotate as values_from. Here’s the code: pivot_wider(table2, names_from = type, values_from = count) ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 18.2.2.1 How to Pivot Wider without tidyr You can also do this without tidyr: Subset rows to separate cases and population values. Remove the type column from each. Rename the count column to cases and population. Merge the two subsets by matching country and year. # Step 1 cases = table2[table2$type == &quot;cases&quot;, ] pop = table2[table2$type == &quot;population&quot;, ] # Step 2 cases = cases[-3] pop = pop[-3] # Step 3 names(cases)[3] = &quot;cases&quot; names(pop)[3] = &quot;population&quot; # Step 4 tidy = cbind(cases, pop[3]) This code uses the cbind function to merge the two subsets, but it would be better to use the merge function. The cbind function does not use identifier columns to check that the rows in each subset are from the same observations. Run vignette(\"pivot\") for more examples of how to use tidyr. 18.2.3 Separating Values Tidy data rule 3 says each value must have its own cell. Here’s a table that breaks rule 3: table3 ## # A tibble: 6 x 3 ## country year rate ## * &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745/19987071 ## 2 Afghanistan 2000 2666/20595360 ## 3 Brazil 1999 37737/172006362 ## 4 Brazil 2000 80488/174504898 ## 5 China 1999 212258/1272915272 ## 6 China 2000 213766/1280428583 Cells in the rate column contain two values: cases and population. These are two different features, so to make the data set tidy, we need to separate them into two different columns. So how can we separate the rate column? The rate column is a character vector (you can check this with str(table3)), so we can use the string processing functions in the stringr package. In particular, we can use the str_split_fixed function: library(stringr) columns = str_split_fixed(table3$rate, fixed(&quot;/&quot;), 2) Now we have a character matrix where the values are in separate columns. Now we need to combine these with the original data frame. There are several ways to approach this, but to be safe, let’s make a new data frame rather than overwrite the original. First we make a copy of the original: tidy_tb = table3 Next, we need to assign each column in the character matrix to a column in the tidy_tb data frame. Since the columns contain numbers, we can also use the as.numeric function to convert them to the correct data type: tidy_tb$cases = as.numeric(columns[, 1]) tidy_tb$population = as.numeric(columns[, 2]) Extracting values, converting to appropriate data types, and then combining everything into a single data frame is an extremely common pattern in data science. Using stringr functions is the most general way to separate out values in a column, but the tidyr package also provides a function separate specifically for the case we just worked through. Either package is appropriate for solving this problem. 18.3 Data Types Another problem that can arise with a data set is the data types of the columns. Recall that R’s most common data types are: character complex numeric integer logical For each of these data types, there’s a corresponding as. function to convert to that data type. For instance, as.character converts an object to a string: x = 3.1 class(x) ## [1] &quot;numeric&quot; y = as.character(x) y ## [1] &quot;3.1&quot; class(y) ## [1] &quot;character&quot; It’s also a good idea to convert categorical columns into factors with the factor function, and to convert columns of dates into dates (more about this in the next section). Let’s look at some examples using a data set collected from the classified advertisements website Craigslist. The data set contains information from ads for rentals in the Sacramento area. First we need to load the data set: cl = read.csv(&quot;data/cl_rentals.csv&quot;) Now we can use the str function to check the classes of the columns: str(cl) ## &#39;data.frame&#39;: 2987 obs. of 20 variables: ## $ title : chr &quot;$1,125 / 1br - 550ft2 - 1Bedroom Prime Location -2520 S Limited Access/Gated $1125 Avail Now (2520 S St)&quot; &quot;$1,449 / 1br - 680ft2 - 1x1 with washer &amp; dryer in unit! Move in ready! (The Phoenix/Sacramento/Folsom/SF)&quot; &quot;$1,449 / 1br - 680ft2 - 1x1 with washer &amp; dryer in unit! Move in ready! (The Phoenix/Sacramento/Folsom/SF)&quot; &quot;$1,479 / 1br - 680ft2 - 1x1 with washer &amp; dryer in unit! Move in ready! (The Phoenix/Sacramento/Folsom/SF)&quot; ... ## $ text : chr &quot;QR Code Link to This Post\\n \\n \\n* SEE MY OTHER MIDTOWN 1 bedroom apts-text for web site\\n*An&quot;| __truncated__ &quot;QR Code Link to This Post\\n \\n \\n Lease our 1x1 Apartment with Che starting at $1449+ Pric&quot;| __truncated__ &quot;QR Code Link to This Post\\n \\n \\n Lease our 1x1 Apartment with Che starting at $1449+ Pric&quot;| __truncated__ &quot;QR Code Link to This Post\\n \\n \\n Lease our 1x1 Apartment with Che starting at $1479+ Pric&quot;| __truncated__ ... ## $ latitude : num 38.6 38.6 38.6 38.6 38.6 ... ## $ longitude : num -121 -121 -121 -121 -121 ... ## $ city : chr &quot;2520 S St&quot; &quot;The Phoenix/Sacramento/Folsom/SF&quot; &quot;The Phoenix/Sacramento/Folsom/SF&quot; &quot;The Phoenix/Sacramento/Folsom/SF&quot; ... ## $ date_posted : chr &quot;2021-02-04 15:03:12&quot; &quot;2021-03-02 12:41:17&quot; &quot;2021-03-02 13:26:17&quot; &quot;2021-03-03 10:02:05&quot; ... ## $ date_updated: chr &quot;2021-03-03 08:41:39&quot; NA NA NA ... ## $ price : int 1125 1449 1449 1479 1414 1441 1615 1660 1877 1611 ... ## $ deleted : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ sqft : int 550 680 680 680 680 680 816 816 916 916 ... ## $ bedrooms : int 1 1 1 1 1 1 2 2 2 2 ... ## $ bathrooms : num 1 1 1 1 1 1 1 1 2 2 ... ## $ pets : chr NA &quot;both&quot; &quot;both&quot; &quot;both&quot; ... ## $ laundry : chr &quot;shared&quot; &quot;in-unit&quot; &quot;in-unit&quot; &quot;in-unit&quot; ... ## $ parking : chr &quot;off-street&quot; &quot;covered&quot; &quot;covered&quot; &quot;covered&quot; ... ## $ craigslist : chr &quot;sacramento&quot; &quot;sacramento&quot; &quot;sacramento&quot; &quot;sacramento&quot; ... ## $ shp_place : chr &quot;Sacramento&quot; &quot;Sacramento&quot; &quot;Sacramento&quot; &quot;Sacramento&quot; ... ## $ shp_city : chr &quot;Sacramento&quot; &quot;Sacramento&quot; &quot;Sacramento&quot; &quot;Sacramento&quot; ... ## $ shp_state : chr &quot;CA&quot; &quot;CA&quot; &quot;CA&quot; &quot;CA&quot; ... ## $ shp_county : chr &quot;Sacramento&quot; &quot;Sacramento&quot; &quot;Sacramento&quot; &quot;Sacramento&quot; ... Some of the columns have the wrong types. For instancce, the pets, laundry, and parking columns all contain categorical data, so they should be factors. Let’s convert these: cl$pets = factor(cl$pets) cl$laundry = factor(cl$laundry) cl$parking = factor(cl$parking) There’s another way we could’ve done this that uses only two lines of code, no matter how many columns there are: cols = c(&quot;pets&quot;, &quot;laundry&quot;, &quot;parking&quot;) cl[cols] = lapply(cl[cols], factor) You can use whichever approach is more convenient and makes more sense to you. If there were other columns to convert, we’d go through the same steps with the appropriate conversion function. The read.csv function does a good job at identifying columns of numbers, so it’s rarely necessary to convert columns of numbers manually. However, you may have to do this for data you got some other way (rather than loading a file). For instance, it’s common to make these conversions when scraping data from the web. 18.3.1 Dates The as.Date function converts times and dates to R’s Date class. This is data type allows us to do computations on dates, such as sorting by date or finding the number of days between two dates. How does as.Date work? We can use it to convert just about any date. The syntax is: as.Date(x, format) The parameter x is a string to convert to a date. The parameter format is a string that explains how the date is formatted. In the format string, a percent sign % followed by a character is called a specification and has special meaning. The most useful are: Specification Description January 29, 2015 %Y 4-digit year 2015 %y 2-digit year 15 %m 2-digit month 01 %B full month name January %b short month name Jan %d day of month 29 %% literal % % You can find a complete list in ?strptime. Let’s look at some examples: as.Date(&quot;January 29, 2015&quot;, &quot;%B %d, %Y&quot;) as.Date(&quot;01292015&quot;, &quot;%m%d%Y&quot;) x = c(&quot;Dec 13, 98&quot;, &quot;Dec 12, 99&quot;, &quot;Jan 1, 16&quot;) class(x) y = as.Date(x, &quot;%b %d, %y&quot;) class(y) y # You can do arithmetic on dates. y[2] - y[1] Now let’s convert the date_posted column in the Craigslist data. It’s always a good idea to test your format string before saving the results back into the data frame: dates = as.Date(cl$date_posted, &quot;%Y-%m-%d %H:%M:%S&quot;) head(dates) ## [1] &quot;2021-02-04&quot; &quot;2021-03-02&quot; &quot;2021-03-02&quot; &quot;2021-03-03&quot; &quot;2021-03-04&quot; ## [6] &quot;2021-03-04&quot; The as.Date function returns NA if conversion failed, so in this case it looks like the dates were converted correctly. Now we can save the dates back into the data frame. We can also do the same thing for the other column of dates, date_updated: cl$date_posted = dates dates = as.Date(cl$date_updated, &quot;%Y-%m-%d %H:%M:%S&quot;) head(dates) # some NAs here because the column already contained NAs ## [1] &quot;2021-03-03&quot; NA NA NA NA ## [6] NA cl$date_updated = dates 18.4 Special Values R has four special values: NA, which represents a missing value. Inf, which represents an infinite value. NaN, read as “not a number,” which represents a value that’s not defined mathematically. For example: 0 / 0 or sqrt(-1). NULL, which represents a value that’s not defined in R. Any R vector can contain a missing value NA. Only numeric and complex vectors can contain Inf and NaN values. Vectors can’t contain NULL values, but lists can. In a data frame, each column is a vector, so you’ll generally only have to deal with NA, Inf, and NaN. Missing values are what you’re most likely to encounter. Missing values represent unknown information. Using an unknown value in a computation produces an unknown result, so we say that missing values are contagious. Here’s an example: NA + 3 ## [1] NA Because of this property, testing equality on missing values with == returns a missing value! So if we want to check whether an object is the missing value, we have to use the is.na function instead: is.na(3) ## [1] FALSE is.na(NA) ## [1] TRUE There are analogous functions is.infinite, is.nan, and is.null for checking whether an object is one of the other special values. The first time you work with a data set, it’s a good idea to check for special values. If too much data is missing, it might not be possible to produce useful visualizations and statistics. We can use is.na together with the table function to check how many values are missing in a column. Let’s try it with some of the columns in the Craigslist data: table(is.na(cl$parking)) ## ## FALSE ## 2987 table(is.na(cl$sqft)) ## ## FALSE TRUE ## 2640 347 Some people prefer to use is.na with the sum function to count missing values, so you may see that as well. The summary function is another way to count missing values, but keep in mind that it only shows the missing values for some data types: summary(cl) ## title text latitude longitude ## Length:2987 Length:2987 Min. :33.99 Min. :-123.2 ## Class :character Class :character 1st Qu.:38.55 1st Qu.:-121.5 ## Mode :character Mode :character Median :38.59 Median :-121.4 ## Mean :38.59 Mean :-121.5 ## 3rd Qu.:38.67 3rd Qu.:-121.3 ## Max. :40.19 Max. : -76.5 ## NA&#39;s :3 NA&#39;s :3 ## city date_posted date_updated price ## Length:2987 Min. :2021-01-30 Min. :2021-02-27 Min. : 1 ## Class :character 1st Qu.:2021-02-24 1st Qu.:2021-03-02 1st Qu.: 1471 ## Mode :character Median :2021-03-01 Median :2021-03-03 Median : 1730 ## Mean :2021-02-26 Mean :2021-03-02 Mean : 1764 ## 3rd Qu.:2021-03-03 3rd Qu.:2021-03-03 3rd Qu.: 1975 ## Max. :2021-03-04 Max. :2021-03-04 Max. :15630 ## NA&#39;s :1801 NA&#39;s :35 ## deleted sqft bedrooms bathrooms pets ## Mode :logical Min. : 200.0 Min. :0.000 Min. :1.00 both:2511 ## FALSE:2987 1st Qu.: 681.0 1st Qu.:1.000 1st Qu.:1.00 cats: 46 ## Median : 801.0 Median :2.000 Median :1.00 dogs: 31 ## Mean : 881.5 Mean :1.529 Mean :1.36 none: 385 ## 3rd Qu.: 1000.0 3rd Qu.:2.000 3rd Qu.:2.00 NA&#39;s: 14 ## Max. :88900.0 Max. :7.000 Max. :4.00 ## NA&#39;s :347 NA&#39;s :10 NA&#39;s :10 ## laundry parking craigslist shp_place ## hookup : 18 covered :1872 Length:2987 Length:2987 ## in-unit:2030 garage : 430 Class :character Class :character ## none : 21 none : 30 Mode :character Mode :character ## shared : 918 off-street: 482 ## street : 169 ## valet : 4 ## ## shp_city shp_state shp_county ## Length:2987 Length:2987 Length:2987 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## Finally, you can use is.na with the which function to get the specific positions of elements that are missing in a vector or column: which(is.na(cl$sqft)) ## [1] 14 17 18 19 29 36 37 46 47 48 69 70 73 79 83 ## [16] 96 97 166 178 189 227 228 229 252 286 293 294 298 303 376 ## [31] 378 410 418 487 490 493 498 499 500 515 600 614 626 653 664 ## [46] 680 681 685 686 687 705 706 708 709 713 720 721 724 733 736 ## [61] 737 740 741 742 744 745 746 750 848 849 852 865 866 878 879 ## [76] 880 881 882 883 884 885 886 887 888 889 890 934 1060 1061 1062 ## [91] 1090 1092 1095 1102 1103 1104 1106 1107 1108 1109 1114 1115 1116 1117 1118 ## [106] 1121 1129 1130 1133 1134 1152 1155 1157 1158 1159 1166 1172 1175 1182 1184 ## [121] 1186 1208 1236 1237 1259 1270 1275 1276 1283 1302 1315 1320 1321 1322 1323 ## [136] 1328 1336 1359 1360 1365 1366 1377 1380 1383 1403 1404 1417 1426 1427 1430 ## [151] 1433 1434 1475 1476 1491 1492 1493 1494 1496 1497 1508 1546 1547 1548 1550 ## [166] 1551 1552 1561 1565 1589 1590 1592 1593 1600 1636 1637 1638 1657 1658 1659 ## [181] 1663 1666 1667 1684 1691 1727 1731 1738 1750 1781 1790 1800 1832 1833 1835 ## [196] 1836 1845 1846 1871 1897 1902 1913 1917 1959 1960 2014 2017 2019 2031 2033 ## [211] 2036 2038 2040 2114 2115 2118 2119 2120 2124 2147 2148 2152 2163 2164 2165 ## [226] 2184 2189 2207 2212 2214 2238 2239 2249 2258 2259 2302 2318 2329 2330 2349 ## [241] 2350 2351 2352 2353 2357 2360 2361 2378 2398 2399 2412 2418 2423 2424 2441 ## [256] 2444 2445 2448 2460 2481 2484 2485 2490 2491 2497 2508 2515 2516 2517 2527 ## [271] 2528 2534 2539 2554 2558 2571 2572 2574 2575 2581 2584 2587 2611 2648 2649 ## [286] 2650 2651 2661 2662 2856 2857 2858 2859 2860 2864 2865 2866 2870 2871 2872 ## [301] 2878 2879 2882 2883 2886 2887 2888 2889 2890 2895 2896 2898 2901 2905 2906 ## [316] 2907 2909 2910 2911 2912 2915 2919 2920 2941 2944 2945 2951 2952 2953 2954 ## [331] 2959 2960 2961 2962 2963 2964 2965 2966 2967 2968 2972 2975 2976 2977 2978 ## [346] 2979 2980 cl[14, ] ## title ## 14 $1,250 / 2br - Amazing location: Midtown 1 Bedroom Apt. ## text ## 14 QR Code Link to This Post\\n \\n \\nEntrances at front and rear gates are locked for added privacy. This property also offers an enclosed garage with 5 rental parking spaces for residents (all are rented at this moment, but you will be offered the space when it becomes available). Street parking is easy to find, and permits through the city are free and easy to obtain with proof of residence. ## latitude longitude city date_posted date_updated price deleted sqft bedrooms ## 14 NA NA &lt;NA&gt; 2021-03-03 &lt;NA&gt; 1250 FALSE NA 2 ## bathrooms pets laundry parking craigslist shp_place shp_city shp_state ## 14 1 none hookup garage sacramento &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## shp_county ## 14 &lt;NA&gt; 18.4.1 Reasoning about Missing Values If your data contains missing values, it’s important to think about why the values are missing. Statisticians use two different terms to describe why data is missing: missing at random (MAR) missing not at random (MNAR) - causes bias! When values are missing at random, the cause for missingness is not related to any of the other features. This is rare in practice. For example, if people in a food survey accidentally overlook some questions. When values are missing not at random, the cause for missingness depends on other features. These features may or may not be in the data set. Think of this as a form of censorship. For example, if people in a food survey refuse to report how much sugar they ate on days where they ate junk food, data is missing not at random. Values MNAR can bias an analysis. The default way to handle missing values in R is to ignore them. This is just a default, not necessarily the best or even an appropriate way to deal with them. You can remove missing values from a data set by indexing: cl_no_sqft_na = cl[!is.na(cl$sqft), ] head(cl_no_sqft_na) ## title ## 1 $1,125 / 1br - 550ft2 - 1Bedroom Prime Location -2520 S Limited Access/Gated $1125 Avail Now (2520 S St) ## 2 $1,449 / 1br - 680ft2 - 1x1 with washer &amp; dryer in unit! Move in ready! (The Phoenix/Sacramento/Folsom/SF) ## 3 $1,449 / 1br - 680ft2 - 1x1 with washer &amp; dryer in unit! Move in ready! (The Phoenix/Sacramento/Folsom/SF) ## 4 $1,479 / 1br - 680ft2 - 1x1 with washer &amp; dryer in unit! Move in ready! (The Phoenix/Sacramento/Folsom/SF) ## 5 $1,414 / 1br - 680ft2 - 1x1 with washer &amp; dryer in unit! Move in ready! (The Phoenix/Sacramento/Folsom/SF) ## 6 $1,441 / 1br - 680ft2 - 1x1 with washer &amp; dryer in unit! Move in ready! (The Phoenix/Sacramento/Folsom/SF) ## text ## 1 QR Code Link to This Post\\n \\n \\n* SEE MY OTHER MIDTOWN 1 bedroom apts-text for web site\\n*An upstairs apt @ 2520 S is coming available 3/18/21\\n*I have 4 apts coming avail in midtown\\n*New flooring in lower apt and redone hardwood flooring in upper unit\\n*1 Bedroom lower unit in 20 unit complex (2-10 unit buildings-courtyard in middle) with manager on site\\n*Gated front and back\\n*9 parking spots in back\\n*Laundry on site with new washers and dryers (coin op)\\n*Owner pays water/sewer/garbage\\n*Wall heat and window air\\n*New paint and new Pergo-type wood flooring \\n*Updated lighting\\n*Nicely maintained building and grounds\\n*$500 deposit\\n*Non-Smoking/vaping Complex\\n*Long time Mgr on Site\\n*No dogs\\n*Pictures of a like unit\\n*Text/call showing Wes show contact info\\n to get copy of video\\n*You need to make 3X rent, have good rental history and credit score of 600 or greater to qualify-no dogs. ## 2 QR Code Link to This Post\\n \\n \\n Lease our 1x1 Apartment with Che starting at $1449+ Prices Subject to change daily! \\n Apartment Features: \\nPrivate Balconies &amp; Patios \\nLuxury Vinyl Wood Flooring \\nDesigner Interior Paint \\nUpgraded cabinets and countertops \\nModern white doors and trim \\nModern white base boards \\nEnergy efficient appliances \\nSpacious bathroom \\nTONS of closet and storage space \\nSac State Shuttle-including Sac State Shuttle App! PET FRIENDLY - MEOW! WOOF! \\n\\nUpgraded Fitness Centers \\nTanning Booth \\nHydro Massage Table \\nResident Business Center \\nResort-Style Pool Furniture \\nSparkling Waterfall Wall \\nFire pit locations \\nBeach Area for lounging \\nOutdoor Recreation Area \\nSand Volleyball Court \\nGrilling Locations Call today to schedule your tour ... show contact info\\n -or- \\nStop in to visit Mon - Sat 10:00a - 5:00p for more info! :D \\n\\n\\nCommunity Amenities: \\nComplete fitness center \\nFour sparkling pools \\nLighted tennis courts \\nFull basketball court \\nWhirlpool Jacuzzi \\n24 hour Emergency Maintenance Hornet Shuttle Access to Sacramento State University \\n Easy Access to Public Transportation (lightrail and bus) \\n Easy Access to the American River Trail And so much more! \\n\\n Check us out online @ www.thephoenixsacramento.com TODAY! \\n Connect with us: \\nFacebook = The Phoenix Apartment Living \\nTwitter = @thephoenixlife Prices and availability are subject to change. Offered prices are starting prices for base rent only. Other charges, conditions, fees, and terms may apply. Equal housing opportunity provider ## 3 QR Code Link to This Post\\n \\n \\n Lease our 1x1 Apartment with Che starting at $1449+ Prices Subject to change daily! \\n Apartment Features: \\nPrivate Balconies &amp; Patios \\nLuxury Vinyl Wood Flooring \\nDesigner Interior Paint \\nUpgraded cabinets and countertops \\nModern white doors and trim \\nModern white base boards \\nEnergy efficient appliances \\nSpacious bathroom \\nTONS of closet and storage space \\nSac State Shuttle-including Sac State Shuttle App! PET FRIENDLY - MEOW! WOOF! \\n\\nUpgraded Fitness Centers \\nTanning Booth \\nHydro Massage Table \\nResident Business Center \\nResort-Style Pool Furniture \\nSparkling Waterfall Wall \\nFire pit locations \\nBeach Area for lounging \\nOutdoor Recreation Area \\nSand Volleyball Court \\nGrilling Locations Call today to schedule your tour ... show contact info\\n -or- \\nStop in to visit Mon - Sat 10:00a - 5:00p for more info! :D \\n\\n\\nCommunity Amenities: \\nComplete fitness center \\nFour sparkling pools \\nLighted tennis courts \\nFull basketball court \\nWhirlpool Jacuzzi \\n24 hour Emergency Maintenance Hornet Shuttle Access to Sacramento State University \\n Easy Access to Public Transportation (lightrail and bus) \\n Easy Access to the American River Trail And so much more! \\n\\n Check us out online @ www.thephoenixsacramento.com TODAY! \\n Connect with us: \\nFacebook = The Phoenix Apartment Living \\nTwitter = @thephoenixlife Prices and availability are subject to change. Offered prices are starting prices for base rent only. Other charges, conditions, fees, and terms may apply. Equal housing opportunity provider ## 4 QR Code Link to This Post\\n \\n \\n Lease our 1x1 Apartment with Che starting at $1479+ Prices Subject to change daily! \\n Apartment Features: \\nPrivate Balconies &amp; Patios \\nLuxury Vinyl Wood Flooring \\nDesigner Interior Paint \\nUpgraded cabinets and countertops \\nModern white doors and trim \\nModern white base boards \\nEnergy efficient appliances \\nSpacious bathroom \\nTONS of closet and storage space \\nSac State Shuttle-including Sac State Shuttle App! PET FRIENDLY - MEOW! WOOF! \\n\\nUpgraded Fitness Centers \\nTanning Booth \\nHydro Massage Table \\nResident Business Center \\nResort-Style Pool Furniture \\nSparkling Waterfall Wall \\nFire pit locations \\nBeach Area for lounging \\nOutdoor Recreation Area \\nSand Volleyball Court \\nGrilling Locations Call today to schedule your tour ... show contact info\\n -or- \\nStop in to visit Mon - Sat 10:00a - 5:00p for more info! :D \\n\\n\\nCommunity Amenities: \\nComplete fitness center \\nFour sparkling pools \\nLighted tennis courts \\nFull basketball court \\nWhirlpool Jacuzzi \\n24 hour Emergency Maintenance Hornet Shuttle Access to Sacramento State University \\n Easy Access to Public Transportation (lightrail and bus) \\n Easy Access to the American River Trail And so much more! \\n\\n Check us out online @ www.thephoenixsacramento.com TODAY! \\n Connect with us: \\nFacebook = The Phoenix Apartment Living \\nTwitter = @thephoenixlife Prices and availability are subject to change. Offered prices are starting prices for base rent only. Other charges, conditions, fees, and terms may apply. Equal housing opportunity provider ## 5 QR Code Link to This Post\\n \\n \\n Lease our 1x1 Apartment with Juliet starting at $1414+ Prices Subject to change daily! \\n Apartment Features: \\nPrivate Balconies &amp; Patios \\nLuxury Vinyl Wood Flooring \\nDesigner Interior Paint \\nUpgraded cabinets and countertops \\nModern white doors and trim \\nModern white base boards \\nEnergy efficient appliances \\nSpacious bathroom \\nTONS of closet and storage space \\nSac State Shuttle-including Sac State Shuttle App! PET FRIENDLY - MEOW! WOOF! \\n\\nUpgraded Fitness Centers \\nTanning Booth \\nHydro Massage Table \\nResident Business Center \\nResort-Style Pool Furniture \\nSparkling Waterfall Wall \\nFire pit locations \\nBeach Area for lounging \\nOutdoor Recreation Area \\nSand Volleyball Court \\nGrilling Locations Call today to schedule your tour ... show contact info\\n -or- \\nStop in to visit Mon - Sat 10:00a - 5:00p for more info! :D \\n\\n\\nCommunity Amenities: \\nComplete fitness center \\nFour sparkling pools \\nLighted tennis courts \\nFull basketball court \\nWhirlpool Jacuzzi \\n24 hour Emergency Maintenance Hornet Shuttle Access to Sacramento State University \\n Easy Access to Public Transportation (lightrail and bus) \\n Easy Access to the American River Trail And so much more! \\n\\n Check us out online @ www.thephoenixsacramento.com TODAY! \\n Connect with us: \\nFacebook = The Phoenix Apartment Living \\nTwitter = @thephoenixlife Prices and availability are subject to change. Offered prices are starting prices for base rent only. Other charges, conditions, fees, and terms may apply. Equal housing opportunity provider ## 6 QR Code Link to This Post\\n \\n \\n Lease our 1x1 Apartment with Che starting at $1441+ Prices Subject to change daily! \\n Apartment Features: \\nPrivate Balconies &amp; Patios \\nLuxury Vinyl Wood Flooring \\nDesigner Interior Paint \\nUpgraded cabinets and countertops \\nModern white doors and trim \\nModern white base boards \\nEnergy efficient appliances \\nSpacious bathroom \\nTONS of closet and storage space \\nSac State Shuttle-including Sac State Shuttle App! PET FRIENDLY - MEOW! WOOF! \\n\\nUpgraded Fitness Centers \\nTanning Booth \\nHydro Massage Table \\nResident Business Center \\nResort-Style Pool Furniture \\nSparkling Waterfall Wall \\nFire pit locations \\nBeach Area for lounging \\nOutdoor Recreation Area \\nSand Volleyball Court \\nGrilling Locations Call today to schedule your tour ... show contact info\\n -or- \\nStop in to visit Mon - Sat 10:00a - 5:00p for more info! :D \\n\\n\\nCommunity Amenities: \\nComplete fitness center \\nFour sparkling pools \\nLighted tennis courts \\nFull basketball court \\nWhirlpool Jacuzzi \\n24 hour Emergency Maintenance Hornet Shuttle Access to Sacramento State University \\n Easy Access to Public Transportation (lightrail and bus) \\n Easy Access to the American River Trail And so much more! \\n\\n Check us out online @ www.thephoenixsacramento.com TODAY! \\n Connect with us: \\nFacebook = The Phoenix Apartment Living \\nTwitter = @thephoenixlife Prices and availability are subject to change. Offered prices are starting prices for base rent only. Other charges, conditions, fees, and terms may apply. Equal housing opportunity provider ## latitude longitude city date_posted date_updated ## 1 38.5728 -121.4675 2520 S St 2021-02-04 2021-03-03 ## 2 38.5511 -121.4068 The Phoenix/Sacramento/Folsom/SF 2021-03-02 &lt;NA&gt; ## 3 38.5511 -121.4068 The Phoenix/Sacramento/Folsom/SF 2021-03-02 &lt;NA&gt; ## 4 38.5511 -121.4068 The Phoenix/Sacramento/Folsom/SF 2021-03-03 &lt;NA&gt; ## 5 38.5511 -121.4068 The Phoenix/Sacramento/Folsom/SF 2021-03-04 &lt;NA&gt; ## 6 38.5511 -121.4068 The Phoenix/Sacramento/Folsom/SF 2021-03-04 &lt;NA&gt; ## price deleted sqft bedrooms bathrooms pets laundry parking craigslist ## 1 1125 FALSE 550 1 1 &lt;NA&gt; shared off-street sacramento ## 2 1449 FALSE 680 1 1 both in-unit covered sacramento ## 3 1449 FALSE 680 1 1 both in-unit covered sacramento ## 4 1479 FALSE 680 1 1 both in-unit covered sacramento ## 5 1414 FALSE 680 1 1 both in-unit covered sacramento ## 6 1441 FALSE 680 1 1 both in-unit covered sacramento ## shp_place shp_city shp_state shp_county ## 1 Sacramento Sacramento CA Sacramento ## 2 Sacramento Sacramento CA Sacramento ## 3 Sacramento Sacramento CA Sacramento ## 4 Sacramento Sacramento CA Sacramento ## 5 Sacramento Sacramento CA Sacramento ## 6 Sacramento Sacramento CA Sacramento The na.omit function is less precise than indexing, because it removes rows that have a missing value in any column. This means lots of information gets lost. Another way to handle missing values is to impute, or fill in, the values with estimates based on other data in the data set. We won’t get into the details of how to impute missing values here, since it is a fairly deep subject. Generally it is safe to impute MAR values, but not MNAR values. 18.5 Outliers An outlier is an anomalous or extreme value in a data set. We can picture this as a value that’s far away from most of the other values. Sometimes outliers are a natural part of the data set. In other situations, outliers can indicate errors in how the data were measured, recorded, or cleaned. There’s no specific definition for “extreme” or “far away.” A good starting point for detecting outliers is to make a plot that shows how the values are distributed. Box plots and density plots work especially well for this: library(ggplot2) ggplot(cl, aes(x = sqft)) + geom_boxplot() ## Warning: Removed 347 rows containing non-finite values (stat_boxplot). Statisticians tend to use the rule of thumb that any value more than 3 standard deviations away from the mean is an outlier. You can use the scale function to compute how many standard deviations the elements in a column are from their mean: z = scale(cl$sqft) head(z) ## [,1] ## [1,] -0.1910838 ## [2,] -0.1161393 ## [3,] -0.1161393 ## [4,] -0.1161393 ## [5,] -0.1161393 ## [6,] -0.1161393 which(z &lt;= -3 | 3 &lt;= z) ## [1] 1261 2461 Be careful to think about what your specific data set measures, as this definition isn’t appropriate in every situation. How can you handle outliers? First, try inspecting other features from the row to determine whether the outlier is a valid measurement or an error. When an outlier is valid, keep it. If the outlier interferes with a plot you want to make, you can adjust the x and y limits on plots as needed to “ignore” the outlier. Make sure to mention this in the plot’s title or caption. When an outlier is not valid, first try to correct it. For example: Correct with a different covariate from the same observation. Estimate with a mean or median of similar observations. This is another example of imputing values. For example, in the Craigslist data, we can use the text column to try to correct outliers: cat(cl$text[1261]) ## QR Code Link to This Post ## ## ## Villages of the Galleria ## 701 Gibson Drive, Roseville, CA, 95678 ## Want more information? Follow this link: ## http://rcmi.aptdetails.com/49u13n ## Call Now: show contact info ## ## Roseville&#39;s Premier Luxury Condominium Rentals ## This is a 1 Bedroom, 1 Bath, approximately 819 Sq. Ft. ## Signature Collection ## This collection of fully renovated homes is limited to a select few. These unique homes are renting quickly. ## The beautifully remodeled floor plan offers an entertainment style kitchen, gracious living area, formal dining room with access to your outdoor balcony, designer two tone paint with crown molding, spacious bathroom with relaxing oval bath tub, linen closet and large vanity. The bedroom offers a sliding glass door giving you additional access to the private balcony over looking the picturesque courtyard. ## Brand New Featured Interiors: ## Entertainment Style Kitchen ## • Beautiful warm espresso custom built cabinets with brush nickel hardware ## • Opulent Granite Countertops with backsplash ## • Satin finish under mount sink with disposal and upgraded Moen faucet and fixtures ## • Stainless steel appliances, spacious built in microwave, multi-cycle dishwasher and self-cleaning oven ## • Plant/décor cabinet ledge ## • Spacious pantry and personalized custom shelving in all cabinets ## • Attractive bright recessed lighting ## • Private in home personal laundry room with full size washer &amp; dryer ## Living and Dining ## • Hand laid tile resembling hard wood flooring ## • Designer two-tone paint with white accent crown molding and baseboards ## • Upgraded wooden style blinds ## • Dual pane windows featuring custom framed molding ## • Spacious coat closet ## Bath ## • Oval Roman soaking tub with surround Opulent granite walls ## • Warm espresso custom built cabinets with brush nickel hardware ## • Spacious linen closet and personal cabinet storage ## • Hand selected Opulent Granite countertops ## • Unique hand crafted above counter sink ## • Contemporary waterfall faucet ## • Custom wood-look framed mirror ## • White glass contemporary light fixture with brush nickel base ## • Upgraded brush nickel accents (towel bars and holders) ## • Hand laid tile resembling hard wood flooring ## Bedroom and Closet ## • Rich plush carpeting ## • Spacious walk in closets with personalized built in custom organizers and compartments ## Other Features and Amenities ## • Brilliant bright recessed lighting ## • Central heat and air ## • Private balcony or terrace ## • Pre-wired for high-speed internet, multi-line phone and cable ## • Brushed nickel hardware accents (door knobs, latches, deadbolts, locks, door knocker and light fixtures) ## • Covered parking ## • Additional patio storage ## Select Homes Offer ## • Private detached garage ## • Additional linen or storage space ## • Cozy Gas Fireplace with carved stone-look mantel and molding ## Style, sophistication, beautiful landscaping and stunning architecture accent the Villages of the ## Galleria apartment homes, located in dynamic Roseville, California. Villages of the Galleria is just minutes from the Galleria Mall and Fountains at Roseville and offers easy freeway access to downtown, Sacramento International Airport, Arco Arena and major employers such as NEC, Oracle and HP. Select from a variety of one, two or three bedroom floor plans. All apartment homes offer gracious living areas with designer two-tone paint, crown molding, large walk-in closets and in home full size washer and dryer. Enjoy the many fine conveniences offered, such as an expansive fitness center, executive business center and refreshing pool. Villages of the Galleria is the perfect place to call home. ## Features ## - Contemporary Recessed Lighting ## - Built-In Linen Closet in Bathroom * ## - Entertainment Style Kitchens ## - Private Balconies and Patios ## - Crown Molding Accents ## - Six-Panel Interior Doors ## - Custom Maple-Front Cabinetry ## - Nine-Foot Ceilings ## - Microwave ## - Pre-Wired for High Speed Internet ## - Private Garages * ## - Full Size Washer/Dryer ## - Pantry * ## - Oval Roman Soaking Tub * ## - Cozy Gas Fireplace with Mantel * ## - Covered Parking * ## - Spacious Walk-In Closet(s) * ## - Energy-Saving Multi-Cycle Dishwasher ## Community Amenities ## - Community Garden ## - Executive Business Center ## - Sand Volleyball ## - Close to Shopping ## - Beautiful Landscaped Court Yards ## - Playground ## - Fitness Center ## - Clubhouse ## - Open Air Cabanas ## - Easy Access to Freeways ## - Pool and Spa ## - Gated Community ## - Professional Onsite Management w/ 24-Hour Emergency Maintenance ## - Picnic Area with Barbecue ## Office Hours ## Monday - Friday 9:00 AM - 6:00 PM ## Saturday 10:00 AM - 5:00 PM ## Sunday 12:00 PM - 5:00 PM ## Pet Policy ## Maximum of 2 pets cats or dogs. No weight limit. Additional $25 rent per month and additional $500 deposit per pet. Inquire about our breed restrictions. ## Equal Housing Opportunity ## VJWLzl1wXG Based on the text, this apartment is 819 square feet, not 8190 square feet. So we can reassign the value: cl$sqft[1261] = 819 If other features don’t help with correction, try getting information from external sources. If you can’t correct the outlier but know it’s invalid, replace it with a missing value NA. "],["working-with-unstructured-data.html", "19 Working with Unstructured Data 19.1 Preliminaries 19.2 From File Names to Metadata 19.3 Loading a Corpus 19.4 Preprocessing 19.5 Counting Terms 19.6 Text Mining Pipepline 19.7 Document Term Matrix 19.8 Corpus Analytics", " 19 Working with Unstructured Data 19.1 Preliminaries Lesson Objectives By the end of this lesson, you should: Be able to identify patterns in unstructured data Create metadata about a collection of documents Load and clean a collection of text files into R, which entails: Determining and applying stop words Normalizing, lemmatizing, and stemming texts Creating a document-term matrix Getting high-level data about text documents (term frequencies, tf–idf scores) Understand how preprocessing steps impact analysis Packages install.packages(c(&quot;tidyverse&quot;, &quot;tokenizers&quot;, &quot;tm&quot;, &quot;cluster&quot;)) 19.2 From File Names to Metadata First, let’s get some information about a collection of files. input_dir &lt;- &quot;./IST8_text_corpus/&quot; fnames &lt;- list.files(input_dir) While we could start analyzing these files immediately, their names contain a lot of metadata, which could be helpful. We’ll need to structure this info first (yes, we’re structuring unstructured data so we can structure more unstructured data—welcome to data forensics!). Mercifully, whoever created these files had a convention in mind for giving them names. We can latch onto the patterns within this convention to make our own representation of the files’ metadata. Here’s the pattern: [LANGUAGE]_[YEAR]_[LASTNAME,FIRSTNAME]_[N OR G].txt Let’s use it to make a data frame. Using stringr in combination with regex patterns will be essential to do so. First, let’s break apart the strings on their underscores and transform that output into a data frame. library(stringr) C19_novels &lt;- str_split_fixed(fnames, &quot;_&quot;, 5) C19_novels &lt;- as.data.frame(C19_novels) This is already pretty close to a good data sheet for us, but we’ll want to refine it a little further. First, let’s name our columns. The letters in the file names are genre tags, which stand for either “gothic” or “not gothic,” so we’ll be sure to record them. colnames(C19_novels) &lt;- c(&quot;lang&quot;, &quot;year&quot;, &quot;author_name&quot;, &quot;title&quot;, &quot;genre&quot;) Now, let’s split author names into “first” and “last” and add them back to the data frame. author_names &lt;- str_split_fixed(C19_novels$author_name, &quot;,&quot;, 2) C19_novels$last_name &lt;- author_names[, 1] C19_novels$first_name &lt;- author_names[, 2] And for good measure, let’s remove the .txt extension in the genre tags and convert those tags to factors. C19_novels$genre &lt;- sapply(C19_novels$genre, function(x) str_remove_all(x, &quot;.txt&quot;)) C19_novels$genre &lt;- as.factor(C19_novels$genre) Finally, we’ll clean up, removing the author_names and lang columns and doing a bit of reordering. (Language could be useful in some instances, but we don’t need it for now, especially because these novels are all in English.) C19_novels &lt;- subset(C19_novels, select= -c(lang, author_name)) C19_novels &lt;- C19_novels[, c(4,5,2,1,3)] C19_novels ## last_name first_name title year genre ## 1 Beckford William Vathek 1786 G ## 2 Radcliffe Ann ASicilianRomance 1790 G ## 3 Radcliffe Ann TheMysteriesofUdolpho 1794 G ## 4 Lewis Matthew TheMonk 1795 G ## 5 Austen Jane SenseandSensibility 1811 N ## 6 Shelley Mary Frankenstein 1818 G ## 7 Scott Walter Ivanhoe 1820 N ## 8 Poe EdgarAllen TheNarrativeofArthurGordonPym 1838 N ## 9 Bronte Emily WutheringHeights 1847 G ## 10 Hawthorne Nathaniel TheHouseoftheSevenGables 1851 N ## 11 Gaskell Elizabeth NorthandSouth 1854 N ## 12 Collins Wilkie TheWomaninWhite 1860 N ## 13 Dickens Charles GreatExpectations 1861 N ## 14 James Henry PortraitofaLady 1881 N ## 15 Stevenson RobertLouis TreasureIsland 1882 N ## 16 Stevenson RobertLouis JekyllandHyde 1886 G ## 17 Wilde Oscar ThePictureofDorianGray 1890 G ## 18 Stoker Bram Dracula 1897 G Nice and tidy! 19.3 Loading a Corpus With our metadata structured, it’s time to load our files. files &lt;- lapply(paste0(input_dir, fnames), readLines) Loading our files like this will create a giant list of vectors, where each vector is a full text file. Those vectors are chunked by paragraph right now, but for our purposes it would be easier if each vector was a single stream of text (like the output of ocr(), if you’ll remember). We can collapse them together with paste(). files &lt;- lapply(files, function(x) paste(x, collapse=&quot; &quot;)) From here, we can wrap these files in a special “corpus” object, which the tm package enables (a corpus is a large collection of texts). A tm corpus works somewhat like a database. It has a section for “content,” which contains text data, as well as various metadata sections, which we can populate with additional information about our texts, if we wished. Taken together, these features make it easy to streamline workflows with text data. To make a corpus with tm, we call the Corpus() function, specifying with VectorSource() (because our texts are vectors): library(tm) corpus &lt;- Corpus(VectorSource(files)) Here’s a high-level glimpse at what’s in this object: corpus ## &lt;&lt;SimpleCorpus&gt;&gt; ## Metadata: corpus specific: 1, document level (indexed): 0 ## Content: documents: 18 Zooming in to metadata about a text in the corpus: corpus[[6]] ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 424017 Not much here so far, but we’ll add more later. Finally, we can get content from a text: str_sub(corpus[[6]]$content, 1, 500) ## [1] &quot;FRANKENSTEIN: OR, THE MODERN PROMETHEUS. BY MARY W. SHELLEY. PREFACE. The event on which this fiction is founded, has been supposed, by Dr. Darwin, and some of the physiological writers of Germany, as not of impossible occurrence. I shall not be supposed as according the remotest degree of serious faith to such an imagination; yet, in assuming it as the basis of a work of fancy, I have not considered myself as merely weaving a series of supernatural terrors. The event on which the interest &quot; In this last view, you can see that the text file is still formatted (at least we didn’t have to OCR it!). This formatting is unwieldy and worse, it makes it so we can’t really access the elements that comprise each novel. We’ll need to do more work to preprocess our texts before we can analyze them. 19.4 Preprocessing Part of preprocessing entails making decisions about the kinds of information we want to know about our data. Knowing what information we want often guides the way we structure data. Put another way: research questions drive preprocessing. 19.4.1 Tokenizing and Bags of Words For example, it’d be helpful to know how many words are in each novel, which might enable us to study patterns and differences between authors’ styles. To get word counts, we need to split the text vectors into individual words. One way to do this would be to first strip out everything in each novel that isn’t an alphabetic character or a space. Let’s grab one text to experiment with. frankenstein &lt;- corpus[[6]]$content frankenstein &lt;- str_replace_all(frankenstein, &quot;[^A-Za-z]&quot;, &quot; &quot;) From here, it would be easy enough to count the words in a novel by splitting its vector on spaces, removing empty elements in the vector, and calling length() on the vector. The end result is what we call a bag of words. frankenstein &lt;- str_split(frankenstein, &quot; &quot;) frankenstein &lt;- lapply(frankenstein, function(x) x[x != &quot;&quot;]) length(frankenstein[[1]]) ## [1] 76015 And here are the first nine words (or “tokens”): frankenstein[[1]][1:9] ## [1] &quot;FRANKENSTEIN&quot; &quot;OR&quot; &quot;THE&quot; &quot;MODERN&quot; &quot;PROMETHEUS&quot; ## [6] &quot;BY&quot; &quot;MARY&quot; &quot;W&quot; &quot;SHELLEY&quot; 19.4.2 Text Normalization While easy, producing our bag of words this way is a bit clunky. And further, this process can’t handle contractions (“I’m,” “don’t,” “that’s”) or differences in capitalization. frankenstein[[1]][188:191] ## [1] &quot;Midsummer&quot; &quot;Night&quot; &quot;s&quot; &quot;Dream&quot; Should be: Midsummer Night&#39;s Dream And &quot;FRANKENSTEIN&quot;, &quot;Frankenstein&quot; Should be: &quot;Frankenstein&quot; Or, even better: frankenstein Typically, when we work with text data we want all of our words to be in the same case because this makes it easier to do things like counting operations. Remember that, to a computer, “Word” and “word” are two separate words, and if we want to count them together, we need to pick one version or the other. Making all words lowercase (even proper nouns) is the standard. Doing this is part of what’s called text normalization. (Other forms of normalization might entail handling orthographic differences between British and American English, like “color” and “colour.”) As for contractions, we have some decisions to make. On the one hand, it’s important to retain as much information as we can about the original text, so keeping “don’t” or “what’s” (which would be “don t” and “what s” in our current method) is important. One way corpus linguists handle these words is to lemmatize them. Lemmatizing involves removing inflectional endings to return words to their base form: car, cars, car’s, cars’ =&gt; car don’t =&gt; do This is a helpful step if what we’re primarily interested in is doing a high- level analysis of semantics. On the other hand, though, many words that feature contractions are high-frequency function words, which don’t have much meaning beyond the immediate context of a sentence or two. Words like “that’s” or “won’t” appear in huge numbers in text data, but they don’t carry much information in and of themselves—it may in fact be the case that we could get rid of them entirely… 19.4.3 Stop Words …and indeed this is the case! When structuring text data to study it at scale, it’s common to remove, or stop out, words that don’t have much meaning. This makes it much easier to identify significant (i.e. unique) features in a text, without having to swim through all the noise of “the” or “that,” which would almost always show up as the highest-occurring words in an analysis. But what words should we remove? Ultimately, this depends on your text data. We can usually assume that function words will be on our list of stop words, but it may be that you’ll have to add or subtract others depending on your data and, of course, your research question. The tm package has a good starting list. Let’s look at the first 100 words. head(stopwords(&quot;SMART&quot;), 100) ## [1] &quot;a&quot; &quot;a&#39;s&quot; &quot;able&quot; &quot;about&quot; &quot;above&quot; ## [6] &quot;according&quot; &quot;accordingly&quot; &quot;across&quot; &quot;actually&quot; &quot;after&quot; ## [11] &quot;afterwards&quot; &quot;again&quot; &quot;against&quot; &quot;ain&#39;t&quot; &quot;all&quot; ## [16] &quot;allow&quot; &quot;allows&quot; &quot;almost&quot; &quot;alone&quot; &quot;along&quot; ## [21] &quot;already&quot; &quot;also&quot; &quot;although&quot; &quot;always&quot; &quot;am&quot; ## [26] &quot;among&quot; &quot;amongst&quot; &quot;an&quot; &quot;and&quot; &quot;another&quot; ## [31] &quot;any&quot; &quot;anybody&quot; &quot;anyhow&quot; &quot;anyone&quot; &quot;anything&quot; ## [36] &quot;anyway&quot; &quot;anyways&quot; &quot;anywhere&quot; &quot;apart&quot; &quot;appear&quot; ## [41] &quot;appreciate&quot; &quot;appropriate&quot; &quot;are&quot; &quot;aren&#39;t&quot; &quot;around&quot; ## [46] &quot;as&quot; &quot;aside&quot; &quot;ask&quot; &quot;asking&quot; &quot;associated&quot; ## [51] &quot;at&quot; &quot;available&quot; &quot;away&quot; &quot;awfully&quot; &quot;b&quot; ## [56] &quot;be&quot; &quot;became&quot; &quot;because&quot; &quot;become&quot; &quot;becomes&quot; ## [61] &quot;becoming&quot; &quot;been&quot; &quot;before&quot; &quot;beforehand&quot; &quot;behind&quot; ## [66] &quot;being&quot; &quot;believe&quot; &quot;below&quot; &quot;beside&quot; &quot;besides&quot; ## [71] &quot;best&quot; &quot;better&quot; &quot;between&quot; &quot;beyond&quot; &quot;both&quot; ## [76] &quot;brief&quot; &quot;but&quot; &quot;by&quot; &quot;c&quot; &quot;c&#39;mon&quot; ## [81] &quot;c&#39;s&quot; &quot;came&quot; &quot;can&quot; &quot;can&#39;t&quot; &quot;cannot&quot; ## [86] &quot;cant&quot; &quot;cause&quot; &quot;causes&quot; &quot;certain&quot; &quot;certainly&quot; ## [91] &quot;changes&quot; &quot;clearly&quot; &quot;co&quot; &quot;com&quot; &quot;come&quot; ## [96] &quot;comes&quot; &quot;concerning&quot; &quot;consequently&quot; &quot;consider&quot; &quot;considering&quot; That looks pretty comprehensive so far, though the only way we’ll know whether it’s a good match for our corpus is to process our corpus with it. At first glance, the extra random letters in this list seem like they could be a big help, on the off chance there’s some noise from OCR. If you look at the first novel in the corpus, for example, there are a bunch of stray p’s, which is likely from a pattern for marking pages (“p. 7”): cat(str_sub(corpus[[1]]$content, 1, 1000)) ## VATHEK; AN ARABIAN TALE, BY WILLIAM BECKFORD, ESQ. p. 7VATHEK. Vathek, ninth Caliph [7a] of the race of the Abassides, was the son of Motassem, and the grandson of Haroun Al Raschid. From an early accession to the throne, and the talents he possessed to adorn it, his subjects were induced to expect that his reign would be long and happy. His figure was pleasing and majestic; but when he was angry, one of his eyes became so terrible [7b] that no person could bear to behold it; and the wretch upon whom it was fixed instantly fell backward, and sometimes expired. For fear, however, of depopulating his dominions, and making his palace desolate, he but rarely gave way to his anger. Being much addicted to women, and the pleasures of the table, he sought by his affability to procure agreeable companions; and he succeeded the better, p. 8as his generosity was unbounded and his indulgences unrestrained; for he was by no means scrupulous: nor did he think, with the Caliph Omar Ben A Our stop word list would take care of this. With it, we could return to our original collection of novels, split them on spaces as before, and filter out everything that’s stored in our stop_list variable. Before we did the filtering, though, we’d need to transform the novels into lowercase (which can be done with R’s tolower() function). 19.4.4 Tokenizers This whole process is ultimately straightforward so far, but it would be nice to collapse all its steps. Luckily, there are packages we can use to streamline our process. tokenizers has functions that split a text vector, turn words into lowercase forms, and remove stop words, all in a few lines of code. Further, we can combine these functions with a special tm_map() function in the tm package, which will globally apply our changes. library(tokenizers) cleaned_corpus &lt;- tm_map(corpus, function(x) tokenize_words(x, stopwords=stopwords(&quot;SMART&quot;), lowercase=TRUE, strip_punct=TRUE, strip_numeric=TRUE)) You may see a “transformation drops documents” warning after this. You can disregard it. It has to do with the way tm references text changes against a corpus’s metadata, which we’ve left blank. We can compare our tokenized output with the text data we had been working with earlier: list(untokenized=frankenstein[[1]][1:9], tokenized=cleaned_corpus[[6]]$content[1:5]) ## $untokenized ## [1] &quot;FRANKENSTEIN&quot; &quot;OR&quot; &quot;THE&quot; &quot;MODERN&quot; &quot;PROMETHEUS&quot; ## [6] &quot;BY&quot; &quot;MARY&quot; &quot;W&quot; &quot;SHELLEY&quot; ## ## $tokenized ## [1] &quot;frankenstein&quot; &quot;modern&quot; &quot;prometheus&quot; &quot;mary&quot; &quot;shelley&quot; From the title alone we can see how much of a difference tokenizing with stop words makes. And while we lose a bit of information by doing this, what we can is a much clearer picture of key words we’d want to further analyze. 19.4.5 Document Chunking and N-grams Finally, it’s possible to change the way we separate out our text data. Instead of tokenizing on words, we could use tokenizers to break apart our texts on paragraphs (tokenize_paragraphs()), sentences (tokenize_sentences), and more. There might be valuable information to be learned about the average sentence length of a novel, for example, so we might chunk it accordingly. We might also want to see whether a text contains repeated phrases, or if two or three words often occur in the same sequence. We could investigate this by adjusting the window around which we tokenize individual words. So far we’ve used the “unigram,” or a single word, as our basic unit of counting, but we could break our texts into “bigrams” (two word phrases), “trigrams” (three word phrases), or, well any sequence of n units. Generally, you’ll see these sequences referred to as n-grams: frankenstein_bigrams &lt;- tokenize_ngrams(corpus[[6]]$content, n=2, stopwords=stopwords(&quot;SMART&quot;)) Here, n=2 sets the n-gram window at two: frankenstein_bigrams[[1]][1:20] ## [1] &quot;frankenstein modern&quot; &quot;modern prometheus&quot; &quot;prometheus mary&quot; ## [4] &quot;mary shelley&quot; &quot;shelley preface&quot; &quot;preface event&quot; ## [7] &quot;event fiction&quot; &quot;fiction founded&quot; &quot;founded supposed&quot; ## [10] &quot;supposed dr&quot; &quot;dr darwin&quot; &quot;darwin physiological&quot; ## [13] &quot;physiological writers&quot; &quot;writers germany&quot; &quot;germany impossible&quot; ## [16] &quot;impossible occurrence&quot; &quot;occurrence supposed&quot; &quot;supposed remotest&quot; ## [19] &quot;remotest degree&quot; &quot;degree faith&quot; Note though that, for this function, we’d need to do some preprocessing on our own to remove numeric characters and punctuation; tokenize_ngrams() won’t do it for us. 19.5 Counting Terms Let’s return to our single word counts. Now that we’ve transformed our novels into bags of single words, we can start with some analysis. Simply counting the number of times a word appears in some data can tell us a lot about a text. The following steps should feel familiar: we did them with OCR. Let’s look at Wuthering Heights, which is our ninth text: library(tidyverse) wuthering_heights &lt;- table(cleaned_corpus[[9]]$content) wuthering_heights &lt;- data.frame(word=names(wuthering_heights), count=as.numeric(wuthering_heights)) wuthering_heights &lt;- arrange(wuthering_heights, desc(count)) head(wuthering_heights, 30) ## word count ## 1 heathcliff 422 ## 2 linton 348 ## 3 catherine 339 ## 4 mr 312 ## 5 master 185 ## 6 hareton 169 ## 7 answered 156 ## 8 till 151 ## 9 house 144 ## 10 door 133 ## 11 mrs 133 ## 12 joseph 130 ## 13 miss 129 ## 14 time 127 ## 15 back 121 ## 16 thought 118 ## 17 cathy 117 ## 18 good 117 ## 19 replied 117 ## 20 earnshaw 116 ## 21 eyes 116 ## 22 cried 114 ## 23 young 107 ## 24 day 106 ## 25 father 106 ## 26 asked 105 ## 27 make 105 ## 28 edgar 104 ## 29 night 104 ## 30 made 102 Looks good! The two main characters in this novel are named Heathcliff and Catherine, so it makes sense that these words would appear a lot. You can see, however, that we might want to fine tune our stop word list so that it removes “mr” and “mrs” from the text. Though again, it depends on our research question. If we’re exploring gender roles in nineteenth-century literature, we’d probably keep those words in. In addition to fine tuning stop words, pausing here at these counts would be a good way to check whether some other form of textual noise is present in your data, which you haven’t yet caught. There’s nothing like that here, but you might imagine how consistent OCR noise could make itself known in this view. 19.5.1 Term Frequency After you’ve done your fine tuning, it would be good to get a term frequency number for each word in this data frame. Raw counts are nice, but expressing those counts in proportion to the total words in a document will tell us more information about a word’s contribution to the document as a whole. We can get term frequencies for our words by dividing a word’s count by document length (which is the sum of all words in the document). wuthering_heights$term_frequency &lt;- sapply(wuthering_heights$count, function(x) (x/sum(wuthering_heights$count))) head(wuthering_heights, 30) ## word count term_frequency ## 1 heathcliff 422 0.009619549 ## 2 linton 348 0.007932709 ## 3 catherine 339 0.007727552 ## 4 mr 312 0.007112084 ## 5 master 185 0.004217101 ## 6 hareton 169 0.003852379 ## 7 answered 156 0.003556042 ## 8 till 151 0.003442066 ## 9 house 144 0.003282500 ## 10 door 133 0.003031754 ## 11 mrs 133 0.003031754 ## 12 joseph 130 0.002963368 ## 13 miss 129 0.002940573 ## 14 time 127 0.002894983 ## 15 back 121 0.002758212 ## 16 thought 118 0.002689827 ## 17 cathy 117 0.002667031 ## 18 good 117 0.002667031 ## 19 replied 117 0.002667031 ## 20 earnshaw 116 0.002644236 ## 21 eyes 116 0.002644236 ## 22 cried 114 0.002598646 ## 23 young 107 0.002439080 ## 24 day 106 0.002416285 ## 25 father 106 0.002416285 ## 26 asked 105 0.002393490 ## 27 make 105 0.002393490 ## 28 edgar 104 0.002370695 ## 29 night 104 0.002370695 ## 30 made 102 0.002325104 19.5.2 Plotting Term Frequency Let’s plot the top 50 words in Wuthering Heights. We’ll call fct_reorder() in the aes() field of ggplot to sort words in the descending order of their term frequency. library(ggplot2) ggplot(data=wuthering_heights[1:50, ], aes(x=fct_reorder(word, -term_frequency), y=term_frequency)) + geom_bar(stat=&quot;identity&quot;) + theme(axis.text.x=element_text(angle=90, vjust=0.5, hjust=1)) + labs(title=&quot;Top 50 words in Wuthering Heights&quot;, x=&quot;Word&quot;, y=&quot;Term Frequency&quot;) This is a good start for creating a high-level view of the novel, but further tuning might be in order. We’ve already mentioned “mrs” and “mr” as two words that we could cut out of the text. Another option would be to collapse these two words together into a base form by stemming them. Though this would overweight their base form (which in this case is “mr”) in terms of term frequency, it would also free up space to see other terms in the document. Other examples of stemming words would be transforming “fishing,” “fished,” and “fisher” all into “fish.” That said, like all preprocessing, lemmatizing words is an interpretive decision, which comes with its own consequences. Maybe it’s okay to transform “mr” and “mrs” into “mr” for some analyses, but it’s also the case that we’d be erasing potentially important gender differences in the text—and would do so by overweighting the masculine form of the word. Regardless of what you decide, it’s important to keep track of these decisions as you make them because they will impact the kinds of claims you make about your data later on. 19.5.3 Comparing Term Frequencies Across Documents Term frequency is helpful if we want to start comparing words across two texts. We can make some comparisons by transforming the above code into a function: term_table &lt;- function(text) { term_tab &lt;- table(text) term_tab &lt;- data.frame(word=names(term_tab), count=as.numeric(term_tab)) term_tab$term_frequency &lt;- sapply(term_tab$count, function(x) (x/sum(term_tab$count))) term_tab &lt;- arrange(term_tab, desc(count)) return(term_tab) } We already have a term table for Wuthering Heights. Let’s make one for Dracula. dracula &lt;- term_table(cleaned_corpus[[18]]$content) head(dracula, 30) ## word count term_frequency ## 1 time 387 0.007280458 ## 2 van 321 0.006038829 ## 3 helsing 299 0.005624953 ## 4 back 261 0.004910076 ## 5 room 231 0.004345699 ## 6 good 225 0.004232824 ## 7 lucy 225 0.004232824 ## 8 man 224 0.004214012 ## 9 dear 219 0.004119949 ## 10 mina 217 0.004082324 ## 11 night 217 0.004082324 ## 12 hand 209 0.003931823 ## 13 face 205 0.003856573 ## 14 door 201 0.003781323 ## 15 made 193 0.003630822 ## 16 poor 192 0.003612010 ## 17 sleep 190 0.003574385 ## 18 eyes 186 0.003499135 ## 19 looked 185 0.003480322 ## 20 friend 183 0.003442697 ## 21 great 182 0.003423884 ## 22 jonathan 182 0.003423884 ## 23 dr 178 0.003348634 ## 24 things 174 0.003273384 ## 25 make 163 0.003066446 ## 26 day 160 0.003010008 ## 27 professor 155 0.002915946 ## 28 count 153 0.002878320 ## 29 found 153 0.002878320 ## 30 thought 153 0.002878320 Now we can compare the relative frequency of a word across two novels: comparison_words &lt;- c(&quot;dark&quot;, &quot;night&quot;, &quot;ominous&quot;) for (i in comparison_words) { wh &lt;- list(wh=subset(wuthering_heights, word==i)) drac &lt;- list(drac=subset(dracula, word==i)) print(wh) print(drac) } ## $wh ## word count term_frequency ## 183 dark 32 0.0007294445 ## ## $drac ## word count term_frequency ## 90 dark 77 0.001448566 ## ## $wh ## word count term_frequency ## 29 night 104 0.002370695 ## ## $drac ## word count term_frequency ## 11 night 217 0.004082324 ## ## $wh ## word count term_frequency ## 7283 ominous 1 2.279514e-05 ## ## $drac ## word count term_frequency ## 7217 ominous 1 1.881255e-05 Not bad! We might be able to make a few generalizations from this, but to say anything definitively, we’ll need to scale our method. Doing so wouldn’t be easy with this setup as it stands now. While it’s true that we could write some functions to roll through these two data frames and systematically compare the words in each, it would take a lot of work to do so. Luckily, the tm package (which we’ve used to make our stop word list) features generalized functions for just this kind of thing. 19.6 Text Mining Pipepline Before going further, we should note that tm has its own functions for preprocessing texts. To send raw files directly through those functions, you’d call tm_map() in conjunction with these functions. You can think of tm_map() as a cognate to the apply() family. corpus_2 &lt;- Corpus(VectorSource(files)) corpus_2 &lt;- tm_map(corpus_2, removeNumbers) corpus_2 &lt;- tm_map(corpus_2, removeWords, stopwords(&quot;SMART&quot;)) corpus_2 &lt;- tm_map(corpus_2, removePunctuation) corpus_2 &lt;- tm_map(corpus_2, stripWhitespace) Note the order of operations here: because our stop words list takes into account punctuated words, like “don’t” or “i’m,” we want to remove stop words before removing punctuation. If we didn’t do this, removeWords() wouldn’t catch the un-punctuated “dont” or “im.” This won’t always be the case, since we can use different stop word lists, which may have a different set of terms, but in this instance, the order in which we preprocess matters. Preparing your text files like this would be fine, and indeed sometimes it’s preferable to sequentially step through each part of the preprocessing workflow. That said, tokenizers manages the order of operations above on its own and its preprocessing functions are generally a bit faster to run (in particular, removeWords() is quite slow in comparison to tokenize_words()). There is, however, one caveat to using tokenizers. It splits documents up to do text cleaning, but other functions in tm require non-split documents. If we use tokenizers, then, we need to do a quick workaround with paste(). cleaned_corpus &lt;- lapply(cleaned_corpus, function(x) paste(x, collapse=&quot; &quot;)) And then reformat that output as a corpus object: cleaned_corpus &lt;- Corpus(VectorSource(cleaned_corpus)) Ultimately, it’s up to you to decide what workflow makes sense. Personally, I (Tyler) like to do exploratory preprocessing steps with tokenizers, often with a sample set of all the documents. Then, once I’ve settled on my stop word list and so forth, I reprocess all my files with the tm-specific functions above. Regardless of what workflow you choose, preprocessing can take a while, so now would be a good place to save your data. That way, you can retrieve your corpus later on. saveRDS(cleaned_corpus, &quot;./data/C19_novels_cleaned.rds&quot;) Loading it back in is straightforward: cleaned_corpus &lt;- readRDS(&quot;./data/C19_novels_cleaned.rds&quot;) 19.7 Document Term Matrix The advantage of using a tm corpus is that it makes comparing data easier. Remember that, in our old workflow, looking at the respective term frequencies in two documents entailed a fair bit of code. And further, we left off before generalizing that code to the corpus as a whole. But what if we wanted to look at a term across multiple documents? To do so, we need to create what’s called a document-term matrix, or DTM. A DTM describes the frequency of terms across an entire corpus (rather than just one document). Rows of the matrix correspond to documents, while columns correspond to the terms. For a given document, we count the number of times that term appears and enter that number in the column in question. We do this even if the count is 0; key to the way a DTM works is that it’s a corpus-wide representation of text data, so it matters if a text does or doesn’t contain a term. Here’s a simple example with three documents: Document 1: “I like cats” Document 2: “I like dogs” Document 3: “I like both cats and dogs” Transforming these into a document-term matrix would yield: n_doc I like both cats and dogs 1 1 1 0 1 0 0 2 1 1 0 0 0 1 3 1 1 1 1 1 1 Representing texts in this way is incredibly useful because it enables us to easily discern similarities and differences in our corpus. For example, we can see that each of the above documents contain the words “I” and “like.” Given that, if we wanted to know what makes documents unique, we can ignore those two words and focus on the rest of the values. Now, imagine doing this for thousands of words! What patterns might emerge? Let’s try it on our corpus. We can transform a tm corpus object into a DTM by calling DocumentTermMatrix(). (Note: this is one of the functions in tm that requires non-split documents, so before you call it make sure you know how you’ve preprocessed your texts!) dtm &lt;- DocumentTermMatrix(cleaned_corpus) This object is quite similar to the one that results from Corpus(): it contains a fair bit of metadata, as well as an all-important “dimnames” field, which records the documents in the matrix and the entire term vocabulary. We access all of this information with the same syntax we use for data frames. Let’s look around a bit and get some high-level info. 19.8 Corpus Analytics Number of columns in the DTM (i.e. the vocabulary size): dtm$ncol ## [1] 34925 Number of rows in the DTM (i.e. the number of documents this matrix represents): dtm$nrow ## [1] 18 Right now, the document names are just a numbers in a vector: dtm$dimnames$Docs ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; &quot;14&quot; &quot;15&quot; ## [16] &quot;16&quot; &quot;17&quot; &quot;18&quot; But they’re ordered according to the sequence in which the corpus was originally created. This means we can use our metadata from way back when to associate a document with its title: dtm$dimnames$Docs &lt;- C19_novels$title dtm$dimnames$Docs ## [1] &quot;Vathek&quot; &quot;ASicilianRomance&quot; ## [3] &quot;TheMysteriesofUdolpho&quot; &quot;TheMonk&quot; ## [5] &quot;SenseandSensibility&quot; &quot;Frankenstein&quot; ## [7] &quot;Ivanhoe&quot; &quot;TheNarrativeofArthurGordonPym&quot; ## [9] &quot;WutheringHeights&quot; &quot;TheHouseoftheSevenGables&quot; ## [11] &quot;NorthandSouth&quot; &quot;TheWomaninWhite&quot; ## [13] &quot;GreatExpectations&quot; &quot;PortraitofaLady&quot; ## [15] &quot;TreasureIsland&quot; &quot;JekyllandHyde&quot; ## [17] &quot;ThePictureofDorianGray&quot; &quot;Dracula&quot; With this information associated, we can use inspect() to get a high-level view of the corpus. inspect(dtm) ## &lt;&lt;DocumentTermMatrix (documents: 18, terms: 34925)&gt;&gt; ## Non-/sparse entries: 145233/483417 ## Sparsity : 77% ## Maximal term length: 19 ## Weighting : term frequency (tf) ## Sample : ## Terms ## Docs back day eyes good great long made man thought time ## Dracula 261 160 186 225 182 147 193 224 153 387 ## GreatExpectations 244 216 180 256 198 173 300 307 238 373 ## Ivanhoe 77 138 100 298 111 154 151 235 46 182 ## NorthandSouth 184 257 197 316 179 211 234 270 332 423 ## PortraitofaLady 210 241 226 520 421 187 381 317 302 339 ## TheHouseoftheSevenGables 79 113 72 100 144 153 144 211 60 113 ## TheMonk 81 106 184 80 66 108 167 95 72 162 ## TheMysteriesofUdolpho 117 167 225 186 164 359 316 213 341 367 ## TheWomaninWhite 417 351 233 235 112 188 244 443 183 706 ## WutheringHeights 121 106 116 117 63 97 102 88 118 127 Of special note here is sparsity. Sparsity measures the amount of 0s in the data. This happens when a document does not contain a term that appears elsewhere in the corpus. In our case, of the 628,650 entries in this matrix, 80% of them are 0. Such is the way of working with DTMs: they’re big, expansive data structures that have a lot of empty space. We can zoom in and filter on term counts with findFreqTerms(). Here are terms that appear more than 1,000 times in the corpus: findFreqTerms(dtm, 1000) ## [1] &quot;answered&quot; &quot;appeared&quot; &quot;asked&quot; &quot;back&quot; &quot;day&quot; &quot;dear&quot; ## [7] &quot;death&quot; &quot;door&quot; &quot;eyes&quot; &quot;face&quot; &quot;father&quot; &quot;felt&quot; ## [13] &quot;found&quot; &quot;friend&quot; &quot;gave&quot; &quot;give&quot; &quot;good&quot; &quot;great&quot; ## [19] &quot;half&quot; &quot;hand&quot; &quot;hands&quot; &quot;head&quot; &quot;hear&quot; &quot;heard&quot; ## [25] &quot;heart&quot; &quot;hope&quot; &quot;kind&quot; &quot;knew&quot; &quot;lady&quot; &quot;leave&quot; ## [31] &quot;left&quot; &quot;life&quot; &quot;light&quot; &quot;long&quot; &quot;looked&quot; &quot;love&quot; ## [37] &quot;made&quot; &quot;make&quot; &quot;man&quot; &quot;men&quot; &quot;mind&quot; &quot;moment&quot; ## [43] &quot;morning&quot; &quot;mother&quot; &quot;night&quot; &quot;part&quot; &quot;passed&quot; &quot;people&quot; ## [49] &quot;person&quot; &quot;place&quot; &quot;poor&quot; &quot;present&quot; &quot;put&quot; &quot;replied&quot; ## [55] &quot;returned&quot; &quot;round&quot; &quot;side&quot; &quot;speak&quot; &quot;stood&quot; &quot;thing&quot; ## [61] &quot;thou&quot; &quot;thought&quot; &quot;till&quot; &quot;time&quot; &quot;told&quot; &quot;turned&quot; ## [67] &quot;voice&quot; &quot;woman&quot; &quot;words&quot; &quot;world&quot; &quot;young&quot; &quot;count&quot; ## [73] &quot;house&quot; &quot;madame&quot; &quot;room&quot; &quot;sir&quot; &quot;emily&quot; &quot;margaret&quot; ## [79] &quot;miss&quot; &quot;mrs&quot; &quot;isabel&quot; Using findAssocs(), we can also track which words rise and fall in usage alongside a given word. (The number in the third argument position of this function is a cutoff for the strength of a correlation.) Here’s “boat”: findAssocs(dtm, &quot;boat&quot;, .85) ## $boat ## thumping scoundrels midday direction ## 0.94 0.88 0.87 0.85 Here’s “writing” (there are a lot of terms, so we’ll limit to 15): writing &lt;- findAssocs(dtm, &quot;writing&quot;, .85) writing[[1]][1:15] ## letter copy disposal inquiries bedrooms hindrance ## 0.99 0.97 0.97 0.97 0.97 0.97 ## messages certificate distrust plainly drawings anonymous ## 0.97 0.97 0.96 0.96 0.96 0.96 ## ladyship plantation lodgings ## 0.96 0.96 0.96 19.8.1 Corpus Term Counts From here, it would be useful to get a full count of all the terms in the corpus. We can transform the DTM into a matrix and then a data frame. term_counts &lt;- as.matrix(dtm) term_counts &lt;- data.frame(sort(colSums(term_counts), decreasing=TRUE)) term_counts &lt;- cbind(newColName=rownames(term_counts), term_counts) colnames(term_counts) &lt;- c(&quot;term&quot;, &quot;count&quot;) As before, let’s plot the top 50 terms in these counts, but this time, they will cover the entire corpus: ggplot(data=term_counts[1:50, ], aes(x=fct_reorder(term, -count), y=count)) + geom_bar(stat=&quot;identity&quot;) + theme(axis.text.x=element_text(angle=90, vjust=0.5, hjust=1)) + labs(title=&quot;Top 50 words in 18 Nineteenth-Century Novels&quot;, x=&quot;Word&quot;, y=&quot;Count&quot;) This looks good, though the words here are all pretty common. In fact, many of them are simply the most common words in the English language. “Time” is the 64th-most frequent word in English; “make” is the 50th. As it stands, then, this graph doesn’t tell us very much about the specificity of our particular collection of texts; if we ran the same process on English novels from the twentieth century, we’d probably produce very similar output. 19.8.2 tf–idf Scores Given this, if we want to know what makes our corpus special, we need a measure of uniqueness for the terms it contains. One of the most common ways to do this is to get what’s called a tf–idf score (short for “term frequency—inverse document frequency”) for each term in our corpus. tf–idf is a weighting method. It increases proportionally to the number of times a word appears in a document but is importantly offset by the number of documents in the corpus that contain this term. This offset adjusts for common words across a corpus, pushing their scores down while boosting the scores of rarer terms in the corpus. Inverse document frequency can be expressed as: \\[\\begin{align*} idf_i = log(\\frac{n}{df_i}) \\end{align*}\\] Where \\(idf_i\\) is the idf score for term \\(i\\), \\(df_i\\) is the number of documents that contain \\(i\\), and \\(n\\) is the total number of documents. A tf-idf score can be calculated by the following: \\[\\begin{align*} w_i,_j = tf_i,_j \\times idf_i \\end{align*}\\] Where \\(w_i,_j\\) is the tf–idf score of term \\(i\\) in document \\(j\\), \\(tf_i,_j\\) is the term frequency for \\(i\\) in \\(j\\), and \\(idf_i\\) is the inverse document score. While it’s good to know the underlying equations here, you won’t be tested on the math specifically. And as it happens, tm has a way to perform the above math for each term in a corpus. We can implement tf–idf scores when making a document-term matrix: dtm_tfidf &lt;- DocumentTermMatrix(cleaned_corpus, control=list(weighting=weightTfIdf)) dtm_tfidf$dimnames$Docs &lt;- C19_novels$title To see what difference it makes, let’s plot the top terms in our corpus using their tf–idf scores. tfidf_counts &lt;- as.matrix(dtm_tfidf) tfidf_counts &lt;- data.frame(sort(colSums(tfidf_counts), decreasing=TRUE)) tfidf_counts &lt;- cbind(newColName=rownames(tfidf_counts), tfidf_counts) colnames(tfidf_counts) &lt;- c(&quot;term&quot;, &quot;tfidf&quot;) ggplot(data=tfidf_counts[1:50, ], aes(x=fct_reorder(term, -tfidf), y=tfidf)) + geom_bar(stat=&quot;identity&quot;) + theme(axis.text.x=element_text(angle=90, vjust=0.5, hjust=1)) + labs(title=&quot;Words with the 50-highest tf--idf scores in 18 Nineteenth-Century Novels&quot;, x=&quot;Word&quot;, y=&quot;TF-IDF&quot;) Lots of names! That makes sense: heavily weighted terms in these novels are going to be terms that are unique to each text. Main characters’ names are used a lot in novels, and the main character names in these novels are all unique. To see in more concrete way how tf–idf scores might make a difference in the way we analyze our corpus, we’ll do two last things. First, we’ll look again at term correlations, using the same words from above with findAssoscs(), but this time we’ll use tf–idf scores. Here’s “boat”: findAssocs(dtm_tfidf, &quot;boat&quot;, .85) ## $boat ## thumping shore bucket cables doo geese ## 0.95 0.93 0.92 0.92 0.92 0.92 ## pickled sea rudder gunwale scoundrels boats ## 0.92 0.91 0.91 0.91 0.91 0.90 ## keel sailed crew baffling biscuit bowsprit ## 0.90 0.89 0.89 0.89 0.89 0.89 ## hauling muskets ripped splash anchor oar ## 0.89 0.89 0.89 0.89 0.88 0.88 ## rattling sandy cook patted shipped beach ## 0.88 0.88 0.88 0.88 0.88 0.87 ## pistols seamen tobacco lee bulwarks hauled ## 0.87 0.87 0.87 0.87 0.87 0.87 ## inkling musket navigation rags steering island ## 0.87 0.87 0.87 0.87 0.87 0.86 ## bottle tumbled avast belay bilge broadside ## 0.86 0.86 0.86 0.86 0.86 0.86 ## cruising cutlasses diagonal furtively headway jupiter ## 0.86 0.86 0.86 0.86 0.86 0.86 ## mainland marlin midday monthly mutineers outnumbered ## 0.86 0.86 0.86 0.86 0.86 0.86 ## plumped riggers schooner schooners seaworthy swamping ## 0.86 0.86 0.86 0.86 0.86 0.86 ## tide&#39;s tiller tonnage towed yawed sail ## 0.86 0.86 0.86 0.86 0.86 0.85 ## ship tap loading sails aft berths ## 0.85 0.85 0.85 0.85 0.85 0.85 ## pinned ## 0.85 Here’s “writing”: findAssocs(dtm_tfidf, &quot;writing&quot;, .85) ## $writing ## hindrance messages disposal inquiries bedrooms ## 0.92 0.91 0.90 0.90 0.89 ## ladyship copy lodgings london unforeseen ## 0.88 0.87 0.87 0.87 0.87 ## drawings plantation explanations certificate dears ## 0.86 0.86 0.86 0.86 0.86 ## neighbourhood allowances ## 0.85 0.85 The semantics of these results have changed. For “boats,” we get much more terms related to sefaring. Most probably this is because only a few novels talk about boats so these terms correlate highly with one another. For “writing,” we’ve interestingly lost a lot of the words associated with writing in a strict sense (“copy,” “message”) but we’ve gained instead a list of terms that seem to situate us in where writing takes place in these novels, or what characters write about. So far though this is speculation; we’d have to look into this further to see whether the hypothesis holds. Finally, we can disaggregate our giant term count graph from above to focus more closely on the uniqueness of individual novels in our corpus. First, we’ll make a data frame from our tf–idf DTM. We’ll transpose the DTM so the documents are our variables (columns) and the corpus vocabulary terms are our observations (or rows). Don’t forget the t! tfidf_df &lt;- as.matrix(dtm_tfidf) tfidf_df &lt;- as.data.frame(t(tfidf_df)) colnames(tfidf_df) &lt;- C19_novels$title 19.8.3 Unique Terms in a Document With this data frame made, we can order our rows by the highest value for a given column. In other words, we can find out not only the top terms for a novel, but the top most unique terms in that novel. Here’s Dracula: rownames(tfidf_df[order(tfidf_df$Dracula, decreasing=TRUE)[1:50],]) ## [1] &quot;helsing&quot; &quot;mina&quot; &quot;lucy&quot; &quot;jonathan&quot; &quot;van&quot; ## [6] &quot;harker&quot; &quot;godalming&quot; &quot;quincey&quot; &quot;seward&quot; &quot;professor&quot; ## [11] &quot;morris&quot; &quot;lucy&#39;s&quot; &quot;harker&#39;s&quot; &quot;diary&quot; &quot;seward&#39;s&quot; ## [16] &quot;arthur&quot; &quot;renfield&quot; &quot;westenra&quot; &quot;whilst&quot; &quot;undead&quot; ## [21] &quot;tonight&quot; &quot;whitby&quot; &quot;dracula&quot; &quot;varna&quot; &quot;carfax&quot; ## [26] &quot;journal&quot; &quot;helsing&#39;s&quot; &quot;count&quot; &quot;count&#39;s&quot; &quot;hawkins&quot; ## [31] &quot;madam&quot; &quot;galatz&quot; &quot;jonathan&#39;s&quot; &quot;mina&#39;s&quot; &quot;pier&quot; ## [36] &quot;wolves&quot; &quot;tomorrow&quot; &quot;czarina&quot; &quot;telegram&quot; &quot;boxes&quot; ## [41] &quot;today&quot; &quot;holmwood&quot; &quot;hypnotic&quot; &quot;garlic&quot; &quot;vampire&quot; ## [46] &quot;phonograph&quot; &quot;transylvania&quot; &quot;cliff&quot; &quot;piccadilly&quot; &quot;slovaks&quot; Here’s Frankenstein: rownames(tfidf_df[order(tfidf_df$Frankenstein, decreasing=TRUE)[1:50],]) ## [1] &quot;clerval&quot; &quot;justine&quot; &quot;elizabeth&quot; &quot;felix&quot; &quot;geneva&quot; ## [6] &quot;frankenstein&quot; &quot;safie&quot; &quot;cottagers&quot; &quot;dæmon&quot; &quot;ingolstadt&quot; ## [11] &quot;kirwin&quot; &quot;agatha&quot; &quot;victor&quot; &quot;ernest&quot; &quot;mont&quot; ## [16] &quot;krempe&quot; &quot;lacey&quot; &quot;waldman&quot; &quot;agrippa&quot; &quot;walton&quot; ## [21] &quot;mountains&quot; &quot;creator&quot; &quot;cottage&quot; &quot;sledge&quot; &quot;hovel&quot; ## [26] &quot;switzerland&quot; &quot;ice&quot; &quot;beaufort&quot; &quot;cornelius&quot; &quot;william&quot; ## [31] &quot;protectors&quot; &quot;moritz&quot; &quot;henry&quot; &quot;labours&quot; &quot;chamounix&quot; ## [36] &quot;glacier&quot; &quot;jura&quot; &quot;blanc&quot; &quot;endeavoured&quot; &quot;lake&quot; ## [41] &quot;leghorn&quot; &quot;monster&quot; &quot;rhine&quot; &quot;magistrate&quot; &quot;belrive&quot; ## [46] &quot;lavenza&quot; &quot;salêve&quot; &quot;saville&quot; &quot;strasburgh&quot; &quot;werter&quot; And here’s Sense and Sensibility: rownames(tfidf_df[order(tfidf_df$SenseandSensibility, decreasing=TRUE)[1:50],]) ## [1] &quot;elinor&quot; &quot;marianne&quot; &quot;dashwood&quot; &quot;jennings&quot; &quot;willoughby&quot; ## [6] &quot;lucy&quot; &quot;brandon&quot; &quot;barton&quot; &quot;ferrars&quot; &quot;colonel&quot; ## [11] &quot;mrs&quot; &quot;marianne&#39;s&quot; &quot;edward&quot; &quot;middleton&quot; &quot;elinor&#39;s&quot; ## [16] &quot;norland&quot; &quot;palmer&quot; &quot;steele&quot; &quot;dashwoods&quot; &quot;jennings&#39;s&quot; ## [21] &quot;willoughby&#39;s&quot; &quot;edward&#39;s&quot; &quot;delaford&quot; &quot;steeles&quot; &quot;cleveland&quot; ## [26] &quot;mama&quot; &quot;dashwood&#39;s&quot; &quot;lucy&#39;s&quot; &quot;brandon&#39;s&quot; &quot;fanny&quot; ## [31] &quot;allenham&quot; &quot;middletons&quot; &quot;devonshire&quot; &quot;combe&quot; &quot;ferrars&#39;s&quot; ## [36] &quot;sister&quot; &quot;morton&quot; &quot;miss&quot; &quot;margaret&quot; &quot;park&quot; ## [41] &quot;charlotte&quot; &quot;exeter&quot; &quot;magna&quot; &quot;berkeley&quot; &quot;harley&quot; ## [46] &quot;john&quot; &quot;middleton&#39;s&quot; &quot;parsonage&quot; &quot;beaux&quot; &quot;behaviour&quot; Names still rank high, but we can see in these results other words that indeed seem to be particular to each novel. With this data, we now have a sense of what makes each document unique in its relationship with all other documents in a corpus An, Weihua, and Yu-Hsin Liu. 2016. “Keyplayer: An R Package for Locating Key Players in Social Networks.” The R Journal 8 (1): 257. https://doi.org/10.32614/RJ-2016-018. Balaban, Alexandru T. 1985. “Applications of Graph Theory in Chemistry.” Journal of Chemical Information and Modeling 25 (3): 334–43. https://doi.org/10.1021/ci00047a033. Bassett, Danielle S., and Olaf Sporns. 2017. “Network Neuroscience.” Nature Neuroscience 20 (3): 353–64. https://doi.org/10.1038/nn.4502. Fan, Chao, and Ali Mostafavi. 2019. “A Graph-Based Method for Social Sensing of Infrastructure Disruptions in Disasters.” Computer-Aided Civil and Infrastructure Engineering 34 (12): 1055–70. https://doi.org/10.1111/mice.12457. Kadushin, Charles. 2012. Understanding Social Networks: Theories, Concepts, and Findings. New York, NY: Oxford University Press. Krebs, Valdis E. 2002. “Mapping Networks of Terrorist Cells.” Connections 24 (3): 43–52. Moreno, Jacob L. 1953. “Who Shall Survive?: Foundations of Sociometry, Group Psychotherapy and Sociodrama.” In. Beacon, N.Y.: Beacon House Inc. Page, Lawrence. 2001. Method for node ranking in a linked database. US6285999B1, issued September 2001. https://patents.google.com/patent/US6285999/en. Robins, Garry, Pip Pattison, Yuval Kalish, and Dean Lusher. 2007. “An Introduction to Exponential Random Graph (p*) Models for Social Networks.” Social Networks, Special section: Advances in Exponential Random Graph (p*) Models, 29 (2): 173–91. https://doi.org/10.1016/j.socnet.2006.08.002. Robins, Garry, Tom Snijders, Peng Wang, Mark Handcock, and Philippa Pattison. 2007. “Recent Developments in Exponential Random Graph (p*) Models for Social Networks.” Social Networks, Special section: Advances in Exponential Random Graph (p*) Models, 29 (2): 192–215. https://doi.org/10.1016/j.socnet.2006.08.003. Wasserman, Stanley, and Katherine Faust. 1994. Social Network Analysis: Methods and Applications. Cambridge, UK: Cambridge University Press. "]]
