[["index.html", "Adventures in Data Science Overview", " Adventures in Data Science Dr. Carl Stahmer Dr. Pamela L. Reynolds Dr. Nick Ulle Dr. Tyler Shoemaker Dr. Michele Tobias Dr. Wesley Brooks Arthur Koehl Carrie Alexander Jared Joseph 2022-02-01 Overview This is the course reader for IST008, Adventures in Data Science: Social Science Edition. The course is designed to provide students with a basic understanding of computing and network architecture, basic programming skills, and an introduction to common methods in Data Science and Digital Humanities. This course reader provides background information that will help you to better understand the concepts that we will discuss in class and to better participate in the hands-on portion of the course. "],["working-with-the-command-line.html", "1 Working with the Command Line 1.1 Interacting with the Command Line 1.2 Common Command Line Commands 1.3 Command Line Text Editors 1.4 Basic Vim Commands", " 1 Working with the Command Line Most users interact with their computer through a Graphical User Interface (GUI) that allows them to use a mouse, keyboard, and graphical elements on screen (such as file menus, pictures of folders and files, etc.) to perform their work. Users tend to conflate their Operating System and their GUI because computer hardware and software manufacturers tightly pack these two concerns as a convenience to users. But the Windows 10 or Mac Big Sur operating system that makes your computer work and the Windows 10 or Mac Big Sur GUI that you interact with are, in fact completely different and separable software packages and it is possible to use different methods/software to interact with your computer than the stock, tightly coupled GUI that launches automatically when you turn on your computer. Because computer manufacturers like Windows and Mac devote so many resources to the development of their system GUIs, there are few viable (at present, none, commercially available) competing GUIs for these platforms. This is not the case in the Linux world, however, where users have several system GUI packages from which to choose and can seamlessly switch between them as desired. Despite the lack of competition/choice on the GUI front when it comes to interacting with your computer, there are other, non-graphical ways of communicating directly with your operating system that exist for all operating systems. We call these “Command Line” interfaces. The Command Line offers a text-only, non graphical means of interacting with your computer. In the early days of computing, all user interaction with the computer happened at the command line. In the current days of graphical user interfaces, using the Command Line requires you to launch a special program that provides Command Line access. Mac users will use an application called “Terminal” which ships by default with the Mac operating system. To launch the Terminal application, go to: Applications -&gt; Utilities -&gt; Terminal When you launch the application, you will see something like this: Windows users will use an application called Git Bash, which was installed on your system when you installed Git. To launch Git Bash, go to: Click on the Windows Start Menu and search for “Git Bash” Alternatively, Click on the Windows Start Menu, select Programs, and browse to Git Bash When you launch the application, you will see something like this: 1.1 Interacting with the Command Line While it can look intimidating to those raised on the GUI, working with the Command Line is actually quite simple. Instead of pointing and clicking on things to make them happen, you type written commands. The figure below shows a new, empty Command Line Interface in the Mac Terminal application The Command Line prompt contains a lot of valuable information. The beginning of the line, “(base) MacPro-F5KWP01GF694” tells us exactly which computer we are communication with. This may seem redundant, but it is actually possible to interact with computers other than the one you are typing on by connecting to them via the Command Line over the network. The bit of information after the colon, in this example the “~” character tells us where in the computer’s filesystem we are. We’ll learn more about this later, for now you need to undersant that the “~” character means that you are in your home directory. The next piece of information we are given is the username under which we are logged into the computer, in this case, my local username, “cstahmer”. After the username, we see the “$” character. This is known as the Command Prompt. It is an indicator that the Command Line application is waiting for you to enter something. The Command Prompt character is used througout these materials when giving command examples. When working through materials, DO NOT ENTER the Command Prompt. It will already be there telling you that the computer is ready to receive your command. Depending on your system and/or Command Line interface, you may or may not also see a solid or flashing box that appears after the Command Prompt. This is a Cursor Position Indicator, which tells you where the current cursor is in the terminal. This is useful if you need to go gack and correct an error. Generally speaking, you can’t click a mouse in a terminal app to edit text. You need to use your computer’s right and left arrows to move the cursor to the correct location and then make your edit. As noted earlier, we interact with the Command Line by typing commands. The figure below shows an example of a simple command, “echo” being entered into the Command Line. The “echo” command prints back to screen any text that you supply to the command It literally echoes your text. To execute, this or any command, you simply hit the “return” or “enter” key on your keyboard. You’ll see that when you execute a Command Line command the sytem performs the indicated operation, prints any output from the operation to screen and then delivers a new Command Line prompt. Note that depending on your particular system and/or Command Line interface, things might look slightly different on your computer. However, the basic presentation and function as described above will be the same. 1.2 Common Command Line Commands During our hands-on, in-class session we will practice using the following Command Line commands. Be prepared to have this page ready as a reference during class to make things easier. Table 1.1: Command Name Function ls List Lists all files in the current directory. ls -l List with Long flag Lists additional information about each file. ls -a List with All flag Lists all files, including hidden files. pwd Print Working Directory Prints the current working directory. mkdir Make Directory Creates a new file directory. cd Change Directory Navigates to another directory on the file system. mv Move Moves files. cp Copy Copies files. rm Remove/delete Deletes files. For a more complete list of Unix Commands, see the Unix Cheat Sheet. 1.3 Command Line Text Editors The Command Line also features a variety of different text editors, similar in nature to Microsoft Word or Mac Pages but much more stripped down. These editors are only accessible from the Command Line; we won’t spend very much time with them, but it is important to know how to use them so that you can open, read, and write directly in the Command Line window. Macs and Git Bash both ship with a text editor called Vim (other common editors include Emacs and Nano). To open a file with vim, type vi in a Command Line window, followed by the filename. If you want to create a new file, simply type the filename you’d like to use for that file after vi. Vim works a bit differently than other text editors and word processors. It has a number of ‘modes,’ which provide different forms of interaction with a file’s data. We will focus on two modes, Normal mode and Insert. When you open a file with Vim, the program starts in Normal mode. This mode is command-based and, somewhat strangely, it doesn’t let you insert text directly in the document (the reasons for this have to do with Vim’s underlying design philosophy: we edit text more than we write it on the Command Line). To insert text in your document, switch to Insert mode by pressing i. You can check whether you’re in Insert mode by looking at the bottom left hand portion of the window, which should read -- INSERT --. Once you are done inserting text, pressing ESC (the Escape key) will bring you back to Normal mode. From here, you can save and quit your file, though these actions differ from other text editors and word processors: saving and quitting with Vim works through a sequence of key commands (or chords), which you enter from Normal mode. To save a file in Vim, make sure you are in Normal mode and then enter :w. Note the colon, which must be included. After you’ve entered this key sequence, in the bottom left hand corner of your window you should see “[filename] XL, XC written” (L stands for “lines” and C stands for “characters”). To quit Vim, enter :q. This should take you back to your Command Line and, if you have created a new file, you will now see that file in your window. If you don’t want to save the changes you’ve made in a file, you can toss them out by typing :q! in place of :w and then :q. Also, in Vim key sequences for save, quit, and hundreds of other commands can be chained together. For example, instead of separately inputting :w and :q to save and quite a file, you can use :wq, which will produce the same effect. There are dozens of base commands like this in Vim, and the program can be customized far beyond what we need for our class. More information about this text editor can be found here. 1.4 Basic Vim Commands Table 1.2: Command Function esc Enter Normal mode. i Enter Insert mdoe. :w Save. :q Quit. :q! Quit without saving. For a more complete list of Vim commands, see this Cheat Sheet. "],["introduction-to-version-control.html", "2 Introduction to Version Control 2.1 What is Version Control? 2.2 Software Assisted Version Control 2.3 Local vs Server Based Version Control 2.4 Central Version Control Systems 2.5 Distributed Version Control Systems 2.6 The Best of Both Worlds 2.7 VCS and the Computer File System 2.8 How Computers Store and Access Information 2.9 How VCS Manage Your Files 2.10 Graph-Based Data Management 2.11 Additional Resources", " 2 Introduction to Version Control This section covers the basics of using Version Control Software (VCS) to track and record changes to files on your local computer. It provides background information that will help you to better understand what VCS is, why we use it, and how it does its work. 2.1 What is Version Control? Version control describes a process of storing and organizing multiple versions (or copies) of documents that you create. Approaches to version control range from simple to complex and can involve the use of various human workflows and/or software applications to accomplish the overall goal of storing and managing multiple versions of the same document(s). Most people have a folder/directory somewhere on their computer that looks something like this: Or perhaps, this: This is a rudimentary form of version control that relies completely on the human workflow of saving multiple versions of a file. This system works minimally well, in that it does provide you with a history of file versions theoretically organized by their time sequence. But this filesystem method provides no information about how the file has changed from version to version, why you might have saved a particular version, or specifically how the various versions are related. This human-managed filesystem approach is more subject to error than software-assisted version control systems. It is not uncommon for users to make mistakes when naming file versions, or to go back and eit files out of sequence. Software-assisted version control systems (VCS) such as Git were designed to solve this problem. 2.2 Software Assisted Version Control Version control software has its roots in the software development community, where it is common for many coders to work on the same file, sometimes synchronously, amplifying the need to track and understand revisions. But nearly all types of computer files, not just code, can be tracked using modern version control systems. IBM’s OS/360 IEBUPDTE software update tool is widely regarded as the earliest and most widely adopted precursor to modern, version control systems. Its release in 1972 of the Source Code Control System (SCCS) package marked the first, fully fledged system designed specifically for software version control. Today’s marketplace offers many options when it comes to choosing a version control software system. They include systems such as Git, Visual Source Safe, Subversion, Mercurial, CVS, and Plastic SCM, to name a few. Each of these systems offers its twist on version control, differing sometimes in the area of user functionality, sometimes in how it handles things on the back-end, and sometimes both. This tutorial focuses on the Git VCS, but in the sections that follow we offer some general information about classes of version control systems to help you better understand how Git does what it does and help you make more informed decisions about how to deploy it for you own work. 2.3 Local vs Server Based Version Control There are two general types of version control systems: Local and Server (sometimes called Cloud) based systems. When working with a Local version control system, all files, metadata, and everything associated with the version control system live on your local drive in a universe unto itself. Working locally is a perfectly reasonable option for those who work independently (not as part of a team), have no need to regularly share their files or file versions, and who have robust back-up practices for their local storage drive(s). Working locally is also sometimes the only option for projects involving protected data and/or proprietary code that cannot be shared. Server based VCS utilize software running on your local computer that communicates with a remote server (or servers) that store your files and data. Depending on the system being deployed, files and data may reside exclusively on the server and are downloaded to temporary local storage only when a file is being actively edited. Or, the system may maintain continuous local and remote versions of your files. Server based systems facilitate team science because they allow multiple users to have access to the same files, and all their respective versions, via the server. They can also provide an important, non-local back-up of your files, protecting you from loss of data should your local storage fail. Git is a free Server based version control system that can store files both locally and on a remote server. While the sections that follow offer a broader description of Server based version control, in this workshop we will focus only on using Git locally and will not configure the software to communicate with, store files on, or otherwise interact with a remote server. DataLab’s companion “Git for Teams” workshop focuses on using Git with the GitHub cloud service to capitalize on Git’s distributed version control capabilities. Server based version control systems can generally be segmented into two distinct categories: 1) Centralized Version Control Systems (Centralized VCS) and 2) Distributed Version Control Systems (Distributed VCS). 2.4 Central Version Control Systems Centralized VCS is the oldest and, surprisingly to many, still the dominant form of version control architecture worldwide. Centralized VCS implement a “spoke and wheel” architecture to provided server based version control. With the spoke and wheel architecture, the server maintains a centralized collection of file versions. Users utilize version control clients to “check-out” a file of interest to their local file storage, where they are free to make changes to the file. Centralized VCS typically restrict other users from checking out editable versions of a file if another user currently has the file checked out. Once the user who has checked out the file has finished making changes, they “check-in” their new version, which is then stored on the server from where it can be retrieved and “checked-out” by another user. As can be seen, Centralized VCS provide a very controlled and ordered universe that ensures file integrity and tracking of changes. However, this regulation comes at a cost. Namely, it reduces the ease with which multiple users can work simultaneously on the same file. 2.5 Distributed Version Control Systems Distributed VCS are not dependent on a central repository as a means of sharing files or tracking versions. Distributed VCS implement a network architecture (as opposed to the spoke and wheel of the Centralized VCS as pictured above) to allow each user to communicate directly with every other user. In Distributed VCS, each user maintains their own version history of the files being tracked, and the VCS software communicates between users to keep the various local file systems in sync with each other. With this type of system, the local versions of two different users will diverge from each other if both users make changes to the file. This divergence will remain in place until the local repositories are synced, at which time the VCS stitches (or merges) the two different versions of the file into a single version that reflects the changes made by each individual, and then saves the stitched version of the file onto both systems as the current version. Various mechanisms can then be used to resolve the conflicts that may arise during this merge process. Distributed VCS offer greater flexibility and facilitate collaborative work, but a lack of understanding of the sync/merge workflow can cause problems. It is not uncommon for a user to forget to synch their local repository with the repositories of other team members and, as a result, work for extended periods of time on outdated files that don’t reflect their teammates and result in work inefficiencies and merge challenges. 2.6 The Best of Both Worlds An important feature of Distributed VCS is that many users and organizations choose to include a central server as a node in the distributed network. This creates an hybrid universe in which some users will sync directly to each other while other users will sync through a central server. Syncing with a cloud-based server provides an extra level of backup for your files and also facilitates communication between users. But treating the server as just another node on the network (as opposed to a centralized point of control) puts the control and flexibility back in the hands of the individual developer. For example, in a true Centralized CVS, if the server goes down then nobody can check files in and out of the server, which means that nobody can work. But in a Distributed CVS this is not an issue. Users can continue to work on local versions and the system will sync any changes when the server becomes available. Git, which is the focus of this tutorial, is a Distributed VCS. You can use Git to share and sync repositories directly with other users or through a central Git server such as, for example, GitHub or GitLab. 2.7 VCS and the Computer File System When we think about Version Control, we typically think about managing changes to individual files. From the user perspective, the File is typically the minimum accessible unit of information. Whether working with images, tabular data, or written text, we typically use software to open a File that contains the information we want to view or edit. As such, it comes as a surprise to most users that the concept of Files, and their organizing containers (Folders or Directories), are not intrinsic to how computers themselves store and interact with data. In this section of the tutorial we will learn about how computers store and access information and how VCS interact with this process to track and manage files. 2.8 How Computers Store and Access Information For all of their computing power and seeming intelligence, computers still only know two things: 0 and 1. In computer speak, we call this a binary system, and the unit of memory on a hard-disk, flash drive, or computer chip that stores each 1 or 0 is called a bit. You can think of your computer’s storage device (regardless of what kind it is) as a presenting a large grid, where each box is a bit: In the above example, as with most computer storage, the bits in our storage grid are addressable, meaning that we can designate a particular bit using a row and column number such as, for example, A7, or E12. Also, remember, that each bit can only contain one of two values: 0 or 1. So, in practice, our storage grid would actually look something like this: All of the complex information that we store in the computer is translated to this binary language prior to storage using a system called Unicode. You can think of Unicode as a codebook that assigns a unique combination of 8, 16, 32, 64, etc. (depending on how old your computer is) ones and zeros to each letter, numeral, or symbol. For example, the 8-bit Unicode for the upper case letter “A” is “01000001”, and the 8-bit Unicode character for the digit “3” is “00110011”. The above grid actually spells out the phrase, “Call me Ishmael”, the opening line of Herman Melville’s novel Moby Dick. An important aspect of how computers story information in binary form is that, unlike most human readable forms of data storage, there is no right to left, up or down, or any other regularized organization of bits on a storage medium. When you save a file on your computer, the computer simply looks for any open bits and starts recording information. The net result is that the contents of single file are frequently randomly interleaved with data from other files. This mode of storage is used because it maximizes the use of open bits on the storage device. But it presents the singular problem of not making data readable in a regularized, linear fashion. To solve this problem, all computers reserve a particular part of their internal memory for a “Directory” which stores a sector map of all chunks of data. For example, if you create a file called README.txt with the word “hello” in it, the computer would randomly store the Unicode for the five characters in the word “hello” on the storage device and make a directory entry something like the following: Understanding the Directory concept and how computers store information is crucial to understanding how VCS mange your Files. 2.9 How VCS Manage Your Files Most users think about version control as a process of managing files. For example, if I might have a directory called “My Project” that holds several files related to this project as follows: One approach to managing changes to the above project files would be to store multiple versions of each file as in the figure below for the file analysis.r: In fact, many VCS do exactly this. They treat each file as the minimum unit of data and simply save various versions of each file along with some additional information about the version. This approach can work reasonably well. However, it has limitations. First, this approach can unnecessarily consume space on the local storage device, especially if you are saving many versions of a very large file. It also has difficulty dealing with changes in filenames, typically treating the same file with a new name as a completely new file, thereby breaking the chain of version history. To combat these issues, good VCS don’t actually manage files at all. They manage Directories. Distributed VCS like Git take this alternate approach to data storage that is Directory, rather than file, based. 2.10 Graph-Based Data Management Git (and many other Distributed VCS) manage your files as collections of data rather than collections of files. Git’s primary unit of management is the “Repository,” or “Repo” for short, which is aligned with your computer’s Directory/Folder structure. Consider, for example, the following file structure: Here we see a user, Tom’s, home directory, which contains three sub directories (Data, Thesis, and Tools) and one file (Notes.txt). Both the Data and Tools directories contain sub files and/or directories. If Tom wanted to track changes to the two files in the Data directory, he would first create a Git repository by placing the Data directory “under version control.” When a repository is created, the Git system writes a collection of hidden files into the Data Directory that it uses to store information about all of the data that lives under that directory. This includes information about the addition, renaming, and deletion of both files and folders as well as information about changes to the data contained in the files themselves. Additions, deletions and versions of files are tracked and stored not as copies of files, but rather as a set of instructions that describes changes made to the underling data and the directory structure that describes them. 2.11 Additional Resources The Git Book is the defintive Git resource and provides an excellent reference for everythign that we will cover in the Interactive session. There is no need to read the book prior to the session, but it’s a good reference resource to have avaialable as you begin to work with Git after the workshop. "],["git-version-control-basics.html", "3 Git Version Control Basics 3.1 Save, Stage, Commit 3.2 Creating Your First Repo 3.3 Checking the Status of a Repo 3.4 Version of a File 3.5 View a History of Your Commits 3.6 Comparing Commits 3.7 Comparing Files 3.8 To View an Earlier Commit 3.9 Undoing Things 3.10 When Things go Wrong! 3.11 Git Branching", " 3 Git Version Control Basics 3.1 Save, Stage, Commit Git does not automatically preserve versions of every “saved” file. When working with Git, you save files as you always do, but this has no impact on the versions that are preserved in the repository. To create a “versions”, you must first add saved files to a Staging area and then “Commit” your staged files to the repository. The Commits that you make constituted the versions of files that are preserved in the repository. 3.2 Creating Your First Repo Move to your Home directory $ cd ~ note: The $ character represents your command prompt. DO NOT type it into your terminal Create a new directory for this course module $ cd ~ $ mkdir dsadventures Change to the new directory $ cd dsadventures Put the new directory under version control $ git init 3.3 Checking the Status of a Repo To check the status of a repository use the followign command $ git status 3.4 Version of a File In Gitspeak, we ‘commit’ if version of a file to the repository to save a copy of the current working version of a file as a version. This is a multi-step process in which we first ‘stage’ the file to be committed and then ‘commit’ the file. STEP 1: Place the file you want to version into the Staging Area $ git add &lt;filename&gt; Replace in the command above with the actual name of the file you want to version. STEP 2: Commit Staged Files $ git commit -m &#39;A detailed comment explaining the nature of the versio being committed. Do not include any apostrophe&#39;s in your comment.&#39; 3.5 View a History of Your Commits To get a history of commits $ git log To see commit history with patch data (insertions and deletions) for a specified number of commits $ git log -p -2 To see abbreviated stats for the commit history $ git log --stat You can save a copy of your Git log to a text file with the following command: $ git --no-pager log &gt; log.txt 3.6 Comparing Commits $ git diff &lt;commit&gt; &lt;commit&gt; 3.7 Comparing Files $ git diff &lt;commit&gt; &lt;file&gt; or $ git diff &lt;commit&gt;:&lt;file&gt; &lt;commit&gt;:&lt;file&gt; 3.8 To View an Earlier Commit $ git checkout &lt;commit&gt; To solve Detached Head problem either RESET HEAD as described below or just chekout another branch git checkout &lt;branch&gt; To save this older version as a parallel branch execute $ git checkout -b &lt;new_branch_name This will save the older commit as a new branch running parallel to master. 3.9 Undoing Things One of the common undos takes place when you commit too early and possibly forget to add some files, or you mess up your commit message. If you want to redo that commit, make the additional changes you forgot, stage them, and commit again using the –amend option $ git commit --amend To unstage a file for commit use $ git reset HEAD &lt;file&gt; Throwing away changes you’ve made to a file $ git checkout -- &lt;file&gt; Rolling everything back to the last commit $ git reset --hard HEAD Rolling everything back to the next to last commit (The commit before the HEAD commit) $ git reset --hard HEAD^ Rolling everything back tp two commits before the head $ git reset --hard HEAD^2 Rolling everything back to an identified commit using HASH/ID from log $ git reset --hard &lt;commit&gt; 3.10 When Things go Wrong! To reset everything back to an earlier commit and make sure that the HEAD pointer is pointing to the newly reset HEAD, do the following $ git reset --hard &lt;commit&gt; $ git reset --soft HEAD@{1} 3.11 Git Branching Branching provides a simple way to maintain multiple, side-by-side versions of the files in a repository. Conceptually, branching a repository creates a copy of the codebase in its current state that you can work on without affecting the primary version from which it was copied. This alows you to work down multiple paths without affecting the main (or other) codebase. To see a list of branches in your repository $ git branch To create a new branch $ git checkout -b hotfix New branches are created of the current working branch. To change branches use $ git checkout &lt;branch name&gt; 3.11.1 Merging Branches When you merge a branch, git folds any changes that you made to files in an identified branch into the current working branch. It also adds any new files. When you perform a merge, a new commit will be automatically created to track the merge. To merge branches, commit any changes to the branch you want to merge (in this example, the ‘hotfix’ branch) then checkout the branch into which you want to merge (for example, master), and then execute a merge command. $ git commit -m &#39;commiting staged files in hotfix branch&#39; $ git checkout master $ git merge hotfix 3.11.2 Branching Workflows There are as many different branching workflows as there are development teams and projects. However, over the years something approximating an “industry standard” has evolved as follows: The “master” or “primary” branch is typically reserved for the current, live and in production version of the codebase. The “development” or “dev” branch holds the current, combined, working version of the code. “topic” branches are created on-the-fly by individuals and are focused on particular coding efforts, one each for each development task. For example, let’s consider a case where there is a team maintaining and developing a company website. In this case, the “master” branch would contain the version of the code that is currently deployed on the live webserver. The “dev” branch would contain a testable version of the code that reflects completed changes to the site made by all team members that have yet to be deployed. Finally, the repository would also contain many topic branches, each of which holds code related to a particular change that was or is being worked on. For exmaple, a team developing a new widget for visualizing data some area of the site would create a suitably named topic branch (somehting like “viz_widget”) for this topic and do all their initial coding in this branch. Once they have completed and tested their code in this branch, they would merge it into the “dev” branch. The new code can then be vidwed and tested by others as part of the “dev” branch. Once all topics branches for planned features for the next release of the website have been merged to “dev” and “dev” has been thoroughly testes (and fixed as necessary), “dev” is then merged into “master” and the “master” branch is then deployed to the live webserver. "],["working-with-remote-repositories.html", "4 Working with Remote Repositories 4.1 GitHub Basics 4.2 Basic GitHub Account Setup 4.3 GitHub Desktop, or the Command Line? 4.4 Sync with GitHub 4.5 Cloning a Repository", " 4 Working with Remote Repositories One of the advantages of working with a version control system like Git is the ability to maintain and sync repositories across multiple computers and users. While there a variety of available, internet accesible remote repository hosting options, in this course, we will work with the Github platform. 4.1 GitHub Basics At its simplest, GitHub is a hosting service for Git repositories. Much like Dropbox or Google Drive, it gives you a space to remotely store your code and related files. This can be useful when working on projects that require, for example, some kind of server, whether for the purposes of running large, potentially time-consuming data analyses or for serving up public-facing content (like a website). For such projects, GitHub acts as a reference point with which you can add, or push, changes on one computer and bring them down, or pull them, onto another. The process would look something like the following, where pushing and pulling from a remote branch entails keeping a reference point for a project that you’re developing locally: Image source. With this diagram in mind, it’s not much of a conceptual leap to imagine how two or more people could work from the same remote repository. Each would pull that repository onto their respective local computers, make a branch, implement their changes, and push those changes back to the remote source. That way, multiple parts of a project could be under development simultaneously, and any such changes made to that project would be trackable according to the logic of version control. Simultaneously pushing and pulling on multiple computers would look something like the following: Image source. 4.1.1 Communicating Through GitHub What makes GitHub special is the fact that, more than being simply a place to store files, the service is above all a communication channel. Where GitHub extends the functionality of version control is not just where it offers various forms of cloud hosting; it is also where GitHub provides tools that let people talk about the code they’re working on. It’s a place where team members can propose and explain the changes they make, look at changes others have made, track and discuss any bugs that might come up, get feedback from others, and plan for any future changes the team intends to make. Learning how to use GitHub, then, is as much about learning how to communicate effectively through the different facets of the service as it is about acquainting yourself with new technical skills (i.e., using your computer to track code remotely). A short summary of the different facets of communication GitHub provides includes: Documentation, often through README files Issue tracking for bug reporting and assigning tasks Pull requests for proposing and discussing changes Wikis, which may feature additional documentation, tutorials, etc. Project boards for long-term planning Various graph visualizations for project overview Additionally, GitHub users can monitor and modify other projects’ code using “Watch”, “Star”, and “Fork” functionalities. The service also provides teams with the ability to specify licensing information for their projects. 4.1.2 What Should I Push to GitHub? A quick word about what should and shouldn’t be pushed to a remote repository, especially with an eye toward what we’ve said about communication. You can, of course, host large data files on GitHub, but there are a few caveats. For one, the site does have a storage limit, and it can also become quite inefficient to have team members constantly push/pull large files to/from GitHub. Further, hosting data files might not be particularly relevant to what a team might need to discuss. Data may change often over the course of a project, but tracking individual observations might not be necessary—more meaningful would be a conversation about how code has made, or might make, such changes. The latter is likely to be something that GitHub is better suited to facilitate. It’s best, then, to host your data files separately from GitHub, either by way of a remote database or some kind of cloud service like Google Drive. Exceptions may come up, however, so the decision about what to track should ultimately be one made by the team. Examples of what should be tracked with GitHub: Code Documentation Make files Some supporting media (small images, for example) Finally, note that even though you can set a repository to either “Public” or “Private” (which controls who can see your project), it’s recommended that you refrain from uploading various access credentials (API keys, database passwords, etc.) to GitHub. 4.2 Basic GitHub Account Setup To use GitHub, you need to make a (free) account. You can do so by going to github.com. Once you’re there, click “Sign Up” in the top-right corner of the page. This should take you to a form, which asks you to enter a username, email address, and password. After you’ve entered in this information (and completed a quick CAPTCHA), GitHub will make you an account. Then, the site will prompt you to complete an optional survey. Fill it out, or scroll to the bottom to skip it. Either way, you’ll need to then verify your email address. Go to your inbox and look for an email from GitHub. Click the “Verify email address” button. Doing so will take you to your homepage, where, if you’d like, you can add a few details about yourself. You now have a GitHub account! 4.2.1 Locally Setting Up Your Git Credentials Regardless of how you make your commits, you will need to use the command line to provide Git with some information about who will be making commits. You may have already done this, however (and sometimes your computer does it automatically). To check, enter the following two commands in either Terminal (Mac) or Git Bash (Windows): git config --global user.name git config --global user.email If you see your name (or some kind of username) and your email after entering the above commands, you’re set. If nothing happens when you type them, you’ll need to provide this information with the following: git config --global user.name &quot;&lt;your name&gt;&quot; git config --global user.email &quot;&lt;your email&gt;&quot; You can check whether this was successful by simply calling either, or both, of the first two commands. They should echo back the information you’ve just entered. 4.2.2 SSH Keys and GitHub When you work with remote repositories on GitHub, you’ll often need to enter your username/password to identify yourself. This is for two reasons: 1) it allows GitHub to track who has made changes to what files; 2) it adds a layer of security to projects, letting teams control who can make changes to their files. Repositories can be either public or private, and this layer of security helps teams control who has access to files in the first place. It can be a pain, though, to have to enter and re-enter your credentials when making changes. More, passwords can be lost or worse, stolen. To avoid these problems, we can set up an SSH key. SSH keys (short for “Secure Shell”) are special, machine-readable credentials that allow users to safely connect and authenticate with remote servers over unsecure networks. An SSH key has two parts: A public key, which encrypts messages intended for a particular recipient. This can be stored on remote servers, or even shared with others, to facilitate secure data transfers A private key, which deciphers messages encrypted by the public key. Your private key is the only thing capable of unlocking what is sent with your public key. It stays on your computer and should never be shared with anyone Beyond what security measures an SSH key brings, it also acts as your digital signature. GitHub uses this internally to verify that you are, in fact, who you say you are when you commit code to a repository. 4.2.3 Connecting to GitHub with SSH GitHub offers thorough, straightforward documentation for setting up an SSH key with its services, which we won’t repeat here. Instead, please visit the link below and follow the step-by-step instructions there to get yourself set up with a key. Connecting to GitHub with SSH The following steps at the link above are required: Checking for existing SSH keys Generating a new SSH key and adding it to the ssh-agent Adding a new SSH key to your GitHub account Testing your SSH connection Once you have completed these steps, be sure you can successfully run the following command: ssh -T git@github.com If your connection is successful, you will see this message (a warning may first appear—see the documentation on GitHub for more information): Hi &lt;your username&gt;! You&#39;ve successfully authenticated, but GitHub does not provide shell access. 4.3 GitHub Desktop, or the Command Line? Remember that Git is separate from GitHub. The latter is a service that’s been built around the former. One part of the services that GitHub offers is an application called GitHub Desktop, which allows users to manage their local repositories with a point-and-click graphical user interface (or GUI). Ultimately, it’s a matter of preference whether you use the GUI or stick with the command line for your own projects, but it is generally a good idea to become proficent at interacting with GitHub via the command line. One of the primary reasons for this has to do with the fact that not every computer you use will have GitHub’s GUI installed—or even have a screen! Many remote servers offer command line-only access, and if you ever want to sync your files with these machines, you’ll need to do so without GitHub Desktop. Luckily, GitHub seamlessly extends Git commands, so using the service without the GUI is, as we’ll see, quite straightforward. 4.4 Sync with GitHub Now that you’re all set up with GitHub, it’s time to sync the website with a local repository on your computer. We’ll start by creating a test repository on your local Git intance. First, use the command line to make a new directory in your Home folder: mkdir ~/my_first_remote_directory Put this directory under version control with Git: cd ~/my_first_remote_directory git init With Vim, make a README markdown file: vim README.md Write and save “Hello world!” in the file. You should see something like the following: Exit Vim. Then, add README.md to Git and commit your changes. Don’t forget to write a short note in the commit message. git add README.md git commit -m &#39;Add a README file&#39; You should see the following: 4.4.1 Preparing to Sync Your Repository So far so good! All we’ve done is repeat the normal workflow for putting files under version control. But now we need to step away from the command line for a moment and prepare a space for receiving this repository on GitHub. To do so, go to github.com and, on your homepage, click the “Create repository” button. You’ll be taken to this page: There are a few things of note here: Repository name: your repository’s name, which should be the same as what’s on your computer Description: a short (1-2 sentence) explanation of what’s in this repository Public/private setting: repositories may be either “public” (viewable by anyone) or “private” (only viewable by you and those to whom you grant access) Initialize with details, including: A README file: a form of documentation; provides information about the files in the repository A .gitignore file: instructs Git to ignore specific files or filetypes A license: governs the use or redistribution of your files Because we’re initializing this repository from an existing directory, we won’t bother with most of the extra details. But we do need a title, which should be the same as what’s on your local computer (“my_first_remote_directory”). A description is helpful but not necessary for our purposes; the same goes for a license. Finally, we will choose to make this a public repository (meaning anyone can see it). 4.4.2 Pushing a Local Repository Once you’ve entered the above information, click “Create repository.” GitHub will take you to a new screen, which gives you a number of options for making or uploading new files to the repository. Since we already have a repository made, we need to use the “Push an existing repository from the command line.” Pushing our repository is as easy as sequentially entering into the command line the three commands GitHub provides. git remote add origin git@github.com:&lt;your user account&gt;/my_first_remote_directory.git git branch -M main git push -u origin On the command line, that looks like this: To summarize the above, we’ve done the following: Associated GitHub’s remote repository with our local repository (git remote etc.) Made a new branch in our local repository called “main” (git branch -M main) Pushed the contents of main (from origin) to a new, corresponding remote branch on GitHub From here on out, when you want to update the remote repository with further changes, you can simply use the shorthand git push after the usual save, add, commit steps. Importantly, Git will only update the branch you’re on when you enter git push, so before making any pushes, it’s a good idea to run a quick git status command to make sure you’re on the branch you want to be on. When you make your changes, the GitHub site won’t immediately refresh itself, but if you click on the “&lt; &gt; Code” tab or on the name of the directory, you’ll see that the repository has been synced and your README.md file is now online. Note that GitHub automatically looks for a README file in your repository. If it finds one that contains renderable markdown code, it will render the file on your repository’s main page. (More information about writing effective README files is available through the DataLab’s data documentation workshop.) 4.4.3 Tracking Files Remotely With this repository made, GitHub can start tracking changes you make to your files, much as Git does locally. The process works exactly like the one you do for Git, though it requires one more step. First, we’ll alter our README.md. Reopen the file with vim, skip a line down from the line you’ve already written, and add “My name is .” Save and quit. Then, add the file and commit your changes. If you want to push these changes to your remote repository, simply enter git push. You’ll see a similar message appear about enumerating, counting, and writing objects to GitHub. Afterwards, if you refresh your file on GitHub, you should see your changes: Note that your commit message appears here as well: If you click the commit tag: You’ll be taken to another page, which shows you the differences between your old version and the new one: 4.4.4 Pulling Changes from a Remote Repository Before moving on, it’s also worth noting that we can pull changes directly from GitHub. If a file has been altered on the remote version of a project, GitHub offers functionality for syncing that file with your local copy (or creating a new file altogether, if need be). For example, if you return to the main page of “my_first_remote_directory”, you can alter the README directly on GitHub. Click the pencil in the right-hand corner of the rendered file. This will open up a text editor interface. Using it, add “What’s yours?” on the fifth line of the document. The complete document should look like this: Hello world! My name is &lt;your name&gt; What&#39;s yours? Scroll to the bottom and click the green “Commit changes” button. This is the equivalent of doing git add &lt;file&gt; and git commit -m &lt;message&gt; on the command line. You’ll see something like the following: Back on the command line, if you type git status, you’ll see that your local repository is now out of sync. If you haven’t made any changes to your directory, syncing it with the remote version can be achieved with a straighforward pull command: git pull Once you enter this command, your command line should look something like this: Your files are now synced. A later portion of this reader will discuss how to handle this process when you have made changes to your directory between the time the remote was altered and the time you go to make a pull. 4.5 Cloning a Repository While tracking your own files remotely with GitHub is great for managing and storing your files, this doesn’t quite tap into the full use of the service. Remember, GitHub is above all a communication channel, in which people can share and discuss the code/files they’re working on. We haven’t yet taken advantage of much of what makes GitHub useful: getting files for a project, modifying them, discussing the changes with team members, and implementing those changes. 4.5.1 How to Clone a Repository To start using GitHub collaboratively, we need to retrieve, or clone, a repository. This will create a local copy of project files. First, go back to your Home directory. You’ll be putting a repository here (in command line speak, the repository will be a “child” of Home). cd ~ Then, go to the following link: https://github.com/ucdavis-datalab-training/workshop_git_for_teams_sandbox Once there, click on the green “Code” button, which should show the following: Since you have SSH keys, select the “SSH” option. Copy the text GitHub provides to your clipboard. Then, in the command line, type git clone, add a space, and paste in the line of text GitHub generated for you. The full command should look like this: git clone git@github.com:ucdavis-datalab-training/workshop_git_for_teams_sandbox.git Hit “Enter”. If you’d like, you can use ls to see the newly made directory. You should see something like the following: If you cd into the directory and then type ls -a, you’ll see a README.md file and a .git file, which contains all the logging info for the repository. "],["introduction-to-r.html", "5 Introduction to R 5.1 Learning objectives 5.2 Before We Start 5.3 Mathematical Operations 5.4 Variables 5.5 Calling Functions 5.6 HELP! 5.7 Vectors 5.8 Data Frames 5.9 Data Types &amp; Classes", " 5 Introduction to R 5.1 Learning objectives After this lecture, you should be able to: define reproducible research and the role of programming languages explain what R and RStudio are, how they relate to eachother, and identify the purpose of the different RStudio panes create and save a script file for later use; use comments to annotate solve simple mathematical operations in R create variables and dataframes inspect the contents of vectors in R and manipulate their content identify the data type and class of an object and vector subset and extract values from vectors use the help function 5.2 Before We Start What is R and RStudio? “R” is both a free and open source programming language designed for statistical computing and graphics, and the software for interpreting the code written in the R language. RStudio is an integrative development environment (IDE) within which you can write and execute code, and interact with the R software. It’s an interface for working with the R software that allows you to see your code, plots, variables, etc. all on one screen. This functionality can help you work with R, connect it with other tools, and manage your workspace and projects. You don’t need RStudio to use R, but many people find that using RStudio makes writing, editing, searching and running their code easier. You cannot run RStudio without having R installed. While RStudio is a commercial product, the free version is sufficient for most researchers. You can download R for free here. You can download RStudio Desktop Open-Source Edition for free here. Why learn R? There are many advantages to working with R. Scientific integrity. Working with a scripting language like R facilitates reproducible research. Having the commands for an analysis captured in code promotes transparency and reproducibility. Someone using your code and data should be able to exactly reproduce your analyses. An increasing number of research journals not only encourage, but are beginning to require, submission of code along with a manuscript. Many data types and sizes. R was designed for statistical computing and thus incorporates many data structures and types to facilitate analyses. It can also connect to local and cloud databases. Graphics. R has buit-in plotting functionalities that allow you to adjust any aspect of your graph to effectively tell the story of your data. Open and cross-platform. Because R is free, open-source software that works across many different operating systems, anyone can inspect the source code, and report and fix bugs. It is supported by a large community of users and developers. Interdisciplinary and extensible. Because anyone can write and share R packages, it provides a framework for integrating approaches across domains, encouraging innovation. Navigating the interface The first time you open RStudio, you’ll see a window divided into several panes, like this: The exact presentation of the panes might be slightly different depending on your operating system, versions of R and RStudio, and any set preferences. Generally, the panes include: Source is your script. You can write your code here and save this as a .R file and re-run to reproduce your results. Console is where you run the code. You can type directly here, but anything entered here won’t be saved when you exit RStudio. Environment/history lists all the objects you have created (including your data) and the commands you have run. Files/plots/packages/help/viewer pane is useful for locating files on your machine to read into R, inspecting any graphics you create, seeing a list of available packages, and getting help. To interact with R, compose your code in the source pane and use the execute (or run) command to send them to the console. (Shortcuts: You can use the shortcut Ctrl + Enter, or Cmd + Return, to run a line of code.) Create a script file for today’s lecture and save it to your lecture_3 folder under ist008_2022 in your home directory. (It’s good practice to keep your projects organized., Some suggested sub-folders for a research project might be: data, documents, scripts, and, depending on your needs, other relevant outputs or products such as figures. 5.3 Mathematical Operations R works by the process of “REPL”: Read-Evaluate-Print-Loop: R waits for you to type an expression (a single piece of code) and press Enter. R then reads in your expressions and parses them. It reads whether the expression is syntactically correct. If so, it will then evaluate the code to compute a result. R then prints the result in the console and loops back around to wait for your next expression. You can use R like a calculator to see how it processes expressions. 7 + 2 R always puts the result on a separate line (or lines) from your code. In this case, the result begins with the tag [1], which is a hint from R that the result is a vector and that this line starts with the element at position 1. We’ll learn more about vectors later in this lesson, and eventually learn about other data types that are displayed differently. If you enter an incomplete expression, R will get stuck at the evaluate step and change the prompt to +, then wait for you to type the rest of the expression and press the Enter key. If this happens, you can finish entering the expression on the new line, or you can cancel it by pressing the Esc key (or Ctrl-c if you’re using R without RStudio). R can only tell an expression is incomplete if it’s missing something that it is expecting, like the second operand here: 7 - ## Error: &lt;text&gt;:2:0: unexpected end of input ## 1: 7 - ## ^ Let’s do more math! Other arithmetic operators are: - for subtraction * for multiplication / for division %% for remainder division (modulo) ^ or ** for exponentiation 7 - 2 244/12 2 * 12 Arithmetic in R follows an order of operations (aka PEMDAS): parenthesis, exponents, multiplication and division, addition and subtraction. You can combine these and use parentheses to make more complicated expressions, just as you would when writing a mathematical expression. For example, to estimate the area of a circle with radius 3, you can write: 3.14 * 3^2 ## [1] 28.26 To see the complete order of operations, use the help command: ?Syntax You can also perform other operations in R: 3 &gt; 1 3 &lt; 1 Tip: You can write R expressions with any number of spaces (including none) around the operators and R will still compute the result. Nevertheless, putting spaces in your code makes it easier for you and others to read, so it’s good to make it a habit. Put spaces around most operators, after commas, and after keywords. 5.4 Variables Since R is designed for mathematics and statistics, you might expect that it provides a better appoximation for \\(\\pi\\) than 3.14. R and most other programming languages allow you to create and store values called variables. Variables allow you to reuse the result of a computation, write general expressions (such as a*x + b), and break up your code into smaller steps so it’s easier to test and understand. R has a built in variable called ‘pi’ for the value of \\(\\pi\\). You can display a variable’s value by entering its name in the console: pi You can also use variables in mathematical expressions. Here’s a more precise calculation of the area of a circle with radius 3: pi *3^2 You can define your own variables with the assignment operator ‘=’ or ‘&lt;-’. Variable names can contain letters, numbers, dots ., and underscores _, but they cannot begin with a number. Spaces and other symbols are not allowed in variable names. In general, variable names should be descriptive but concise, and should not use the same name as common (base R) functions like mean, T, median, sum, etc.. Let’s make some more variables: x &lt;- 10 y &lt;- 24 fantastic.variable2 = x x &lt;- y / 2 In R, variables are copy-on-write. When we change a variable (a “write”), R automatically copies the original value so dependent variables are unchanged until they are re-run. x &lt;- 13 y &lt;- x x &lt;- 16 y Why do you think copy-on-write is helpful? Where do you think it could trip you up? 5.5 Calling Functions R has many functions (reusable commands) built-in that allow you to compute mathematical operations, statistics, and perform other computing tasks on your variables. You can think of a function as a machine that takes some inputs and uses them to produce some output. Code that uses a function is said to call that function. When you call a function, the values that you assign as input are called arguments. The output is called the return value. We call a function by writing its name followed by a parentheses containing the arguments. log(10) sqrt(9) Many functions have multiple parameters and can accept multiple arguments. For example, the sum function accepts any number of arguments and adds them all together. When you call a function with multiple arguments, separate the arguments with commas. sum(5, 4, 1) When you call a function, R assigns each argument to a parameter. Parameters are special variables that represent the inputs to a function and only exist while that function runs. For example, the log function, which computes a logarithm, has parameters x and base for the operand and base of the logaritm, respectively. By default, R assigns arguments to parameters based on their order. The first argument is assigned to the function’s first parameter, the second to the second, and so on. If you know the order that a function expects to receive the parameters then you can list them separated by commas. Here the argument 64 is assigned to the parameter x, and the argument 2 is assigned to the parameter base. log(64, 2) You can also assign arguments to parameters by name with = (but not with &lt;-), overriding their positions. log(64, base = 2) log(base = 2, x= 64) Tip: Both of these expressions are equivalent, so which one should you use? When you write code, choose whatever seems the clearest to you. Leaving parameter names out of calls saves typing, but including some or all of them can make the code easier to understand. Not sure what parameters a specific function needs? Read on for how to get… 5.6 HELP! This is just the beginning, and there are lots of resources to help you learn more. R has built-in help files that can be accessed with the ? and help commands. You can also search within the help documentation using the ?? commands. There’s also a vibrant online help community. Here are some examples of how you can use all this help to help yourself: ? The help pages for all of R’s built-in functions usually have the same name as the function itself. Function help pages usually include a brief description, a list of parameters, a description of the return value, and some examples. To open the help page for the log function: ?log There are also help pages for other topics, such as built-in mathematical constants (such as ?pi), data sets (such as ?cars), and operators. To look up the help page for an operator, put the operator’s name in single or double quotes. For example, this code opens the help page for the arithmetic operators: ?&quot;+&quot; Tip: It’s always okay to put single or double quotes around the name of the page when you use ?, but they’re only required if it contains arithmetic commands or non-alphabetic characters. So ?sqrt, ?'sqrt', and ?\"sqrt\" all open the documentation for sqrt, the square root function. Why does this work? R treats anything inside single or double quotes as literal text rather than as an expression to evaluate. In programming jargon, a piece of literal text is called a string. You can use whichever kind of quotes you prefer, but the quote at the beginning of the string must match the quote at the end. We’ll learn more about strings in later lessons when we cover working with unstructured data. ?? Sometimes you might not know the name of the help page you want to look up. You can do a general search of R’s help pages with ?? followed by a string of search terms. For example, to get a list of all help pages related to linear models: ??&quot;linear model&quot; This search function doesn’t always work well, and it’s often more efficient to use an online search engine. When you search for help with R online, include “R” as a search term. Alternatively, you can use RSeek, which restricts the search to a selection of R-related websites. In later lessons we’ll learn about packages, which are sharable bundles of code. You’ll often need to look up the documentation to get help figuring out how to work with a new package. You can view a package’s help documentation using packageDescription(\"Name\"). 5.6.1 When Something Goes Wrong (and it will) Sooner or later you’ll run some code and get an error message or result you didn’t expect. Don’t panic! Even experienced programmers make mistakes regularly, so learning how to diagnose and fix problems is vital. We call this troubleshooting or debugging. Stay calm and try going through these steps: If R returned a warning or error message, read it! If you’re not sure what the message means, try searching for it online. Check your code for typ0s. Did you capitalize something that should be lower case? Are you missing or have an extra comma, quote, parenthesis? Test your code one line at a time, starting from the beginning. After each line that assigns a variable, check that the value of the variable is what you expect. Try to determine the exact line where the problem originates (which may differ from the line that emits an error!). Sometimes the “shut it down and restart” trick really works - you might have created a variable and forgot about it, and you need a fresh start for the code to work as intended. If all else fails, just Google it. Stack Overflow is a popular question and answer website and you can often find solutions to your problems there, or pick up tips to help you tackle your problem in a new way. On CRAN, check out the Intro to R Manual and R FAQ. Many regions also have grassroots R-Users Groups that you can join and ask for help. Just remember to pay it forward and use your newfound R prowess to help others in the community on their learning journies! Tip: When asking for help, clearly state the problem and provide a reproducible example. R also has a posting guide to help you write questions that are more likely to get a helpful reply. It’s also a good idea to save your sessionInfo() so you can show others how your machine and session was configured. Doing this before coming to office hours for a programming class is also highly recommended! 5.7 Vectors A vector is an ordered collection of values. The values in a vector are called elements. Vectors can have any number of elements, including 0 or 1 element. For example, a single value, like 3, is a vector with 1 element. So every value that you’ve worked with in R so far was a vector. The elements of a vector must all be the same type of data (we say the elements are homogeneous). A vector can contain integers, decimal numbers, strings (text), or several other types of data, but not a mix these all at once. You can combine or concatenate vectors to create a longer vector with the c function: # numbers time.min &lt;- c(5, 4, 4, 12, 10, 2, 3, 4, 4, 5, 19) # strings pets &lt;- c(&quot;woof&quot;, &quot;woof&quot;, &quot;cat&quot;, &quot;woof&quot;, &quot;woof&quot;, &quot;cat&quot;, &quot;woof&quot;, &quot;woof&quot;, &quot;woof&quot;, &quot;woof&quot;, &quot;woof&quot;) place &lt;- c(&quot;Temple&quot;, &quot;Yakitori&quot;, &quot;Panera&quot;, &quot;Yakitori&quot;, &quot;Guads&quot;, &quot;Home&quot;, &quot;Tea List&quot;, &quot;Raising Canes&quot;, &quot;Pachamama&quot;, &quot;Lazi Cow&quot;, &quot;Wok of Flame&quot;) You can check the length of a vector (and other objects) with the length function: length(3) ## [1] 1 length(&quot;hello&quot;) ## [1] 1 length(time.min) ## [1] 11 length(pets) ## [1] 11 5.7.1 Indexing Vectors Vectors are ordered, which just means the elements have specific positions. For example, in the place vector, the value of the 1st element is \"Temple\", the 2nd is \"Yakitori\", the 5th is \"Guads\", and so on. You can access individual elements of a vector with the indexing operator [ (also called the square bracket operator). The way to write it, or syntax is: VECTOR[INDEXES] Here INDEXES is a vector of positions of elements you want to get or set. For example, let’s get the 2nd element of the place vector: place[2] ## [1] &quot;Yakitori&quot; Now let’s get the 3rd and 1st element: place[c(3, 1)] ## [1] &quot;Panera&quot; &quot;Temple&quot; Indexing is among the most frequently used operations in R, so take some time to try it out with few different vectors and indexes. We’ll revisit indexing in a later lesson to learn a lot more about it. 5.7.2 Vectorization Let’s look at what happens if we call a mathematical function, like sqrt, on a vector: x &lt;- c(2, 16, 4, 7) sqrt(x) ## [1] 1.414214 4.000000 2.000000 2.645751 This gives us the same result as if we had called the function separately on each element. That is, the result is the same as: c(sqrt(2), sqrt(16), sqrt(4), sqrt(7)) ## [1] 1.414214 4.000000 2.000000 2.645751 Of course, the first version is much easier to type. Functions that take a vector argument and get applied element-by-element are called vectorized functions. Most functions in R are vectorized, especially math functions. Some examples include sin, cos, tan, log, exp, and sqrt. A function can be vectorized across multiple arguments. This is easiest to understand in terms of the arithmetic operators. Let’s see what happens if we add two vectors together: x = c(1, 2, 3, 4) y = c(-1, 7, 10, -10) x + y ## [1] 0 9 13 -6 The elements are paired up and added according to their positions. The other arithmetic operators are also vectorized: x - y ## [1] 2 -5 -7 14 x * y ## [1] -1 14 30 -40 x / y ## [1] -1.0000000 0.2857143 0.3000000 -0.4000000 Functions that are not vectorized tend to be ones that combine or aggregate values in some way. For instance, the sum, length, and class functions are not vectorized. 5.8 Data Frames We frequently work with 2-dimensional tables of data (also called tabular data). Typically each row corresponds to a single subject and is called an observation. Each column corresponds to a measurement of the subject. In data science, the columns of a table are called features or covariates. Sometimes people also refer to them as “variables”, but that can be confusing as “variable” means something else in R, so here we’ll try to avoid that term. R’s structure for tabular data is the data frame. The columns are vectors, so the elements within a column must all be the same type of data. In a data frame, every column must have the same number of elements (so the table is rectangular). There are no restrictions on the data types in each row. You can use the data.frame function to create a data frame from column vectors: # current data (vectors) time.min place pets # create new data (vectors) distance.mi &lt;- c(0.9, 0.6, 0.8, 0.6, 2, 100, 0.6, 0.7, 0.8, 1, 3.7) major &lt;- c(&quot;chicanix studies&quot;, &quot;human development&quot;, &quot;economics&quot;, &quot;undeclared&quot;, &quot;psychology&quot;, &quot;MMM&quot;, &quot;psychology&quot;, &quot;undeclared&quot;, &quot;human development&quot;, &quot;undeclared&quot;, &quot;GG&quot;) # combine vectors into dataframe my.data &lt;- data.frame(place, distance.mi, time.min, major, pets) 5.8.1 Selecting Columns You can select an individual column from a data frame by name with $, the dollar sign operator. The syntax is: VARIABLE$COLUMN_NAME For instance, to select the place column: my.data$place ## [1] &quot;Temple&quot; &quot;Yakitori&quot; &quot;Panera&quot; &quot;Yakitori&quot; ## [5] &quot;Guads&quot; &quot;Home&quot; &quot;Tea List&quot; &quot;Raising Canes&quot; ## [9] &quot;Pachamama&quot; &quot;Lazi Cow&quot; &quot;Wok of Flame&quot; The selected column is just a vector, so you can assign it to a variable and use it in functions. For example, to compute the sum of the distance.mi column: sum(my.data$distance.mi) ## [1] 111.7 Preview of future lesson content: What if you want to extract a row from your data frame? For example, to pull out all responses from only the 11th row, you would subset it: my.data[11, ] 5.8.2 Inspecting Data You can print a small dataset, but it can be slow and hard to read especially if there are a lot of coumns. R has many built in functions to inspect objects: head(my.data) # this prints only the beginning of the dataset tail(my.data) # this prints only the end of the dataset nrow(my.data) # number of rows ncol(my.data) # number of columns ls(my.data) # lists the names of the columns alphabetically names(my.data) # lists the names of the columns as they appear in the data frame rownames(my.data) # names of the rows A highly informative function for inspecting the structure of a data frame or other object is str: str(my.data) The table function is another useful function for inspecting data. The table function computes the frequency of each unique value in a vector. For instance, you can use table to compute how many entries in the pets column are woof: table(my.data$pets) ## ## cat woof ## 2 9 Additional reading: Check out this DataLab workshop on “Keeping Your Data Tidy” that covers best practices for structuring, naming, and organizing your data frames (and spreadsheets). 5.9 Data Types &amp; Classes Data can be categorized into different types based on sets of shared characteristics. For instance, statisticians tend to think about whether data are numeric or categorical: numeric continuous (real or complex numbers) discrete (integers) categorical nominal (categories with no ordering) ordinal (categories with some ordering) Of course, other types of data, like graphs (networks) and natural language (books, speech, and so on), are also possible. Categorizing data this way is useful for reasoning about which methods to apply to which data. In R, data objects are categorized in two different ways: The class of an R object describes what the object does, or the role that it plays. Sometimes objects can do more than one thing, so objects can have more than one class. The class function returns the classes of its argument. The type of an R object describes what the object is. Technically, the type corresponds to how the object is stored in your computer’s memory. Each object has exactly one type. The typeof function returns the type of its argument. Of the two, classes tend to be more important than types. If you aren’t sure what an object is, checking its classes should be the first thing you do. The built-in classes you’ll use all the time correspond to vectors and lists (which we’ll learn more about in Section 5.9.1): Class Example Description logical TRUE, FALSE Logical (or Boolean) values integer -1L, 1L, 2L Integer numbers numeric -2.1, 7, 34.2 Real numbers complex 3-2i, -8+0i Complex numbers character \"hi\", \"YAY\" Text strings list list(TRUE, 1, \"hi\") Ordered collection of heterogeneous elements The class of a vector is the same as the class of its elements: class(&quot;hi&quot;) ## [1] &quot;character&quot; class(c(&quot;hello&quot;, &quot;hi&quot;)) ## [1] &quot;character&quot; In addition, for ordinary vectors, the class and the type are the same: x &lt;- c(TRUE, FALSE) class(x) ## [1] &quot;logical&quot; typeof(x) ## [1] &quot;logical&quot; The exception to this rule is numeric vectors, which have type double for historical reasons: class(pi) ## [1] &quot;numeric&quot; typeof(pi) ## [1] &quot;double&quot; typeof(3) ## [1] &quot;double&quot; The word “double” here stands for double-precision floating point number, a standard way to represent real numbers on computers. By default, R assumes any numbers you enter in code are numeric, even if they’re integer-valued. The class integer also represents integer numbers, but it’s not used as often as numeric. A few functions, such as the sequence operator : and the length function, return integers. The difference between numeric and integer is generally not important. class(3) ## [1] &quot;numeric&quot; class(length(pets)) ## [1] &quot;integer&quot; class(1:3) ## [1] &quot;integer&quot; Besides the classes for vectors and lists, there are several built-in classes that represent more sophisticated data structures: Class Description function Functions factor Categorical values matrix Two-dimensional ordered collection of homogeneous elements array Multi-dimensional ordered collection of homogeneous elements data.frame Data frames For these, the class is usually different from the type. We’ll learn more about most of these later on. 5.9.1 Lists A list is an ordered data structure where the elements can have different types (they are heterogeneous). This differs from a vector, where the elements all have to have the same type, as we saw in Section 5.7. The tradeoff is that most vectorized functions do not work with lists. You can make an ordinary list with the list function: x &lt;- list(1, c(&quot;hi&quot;, &quot;bye&quot;)) class(x) ## [1] &quot;list&quot; typeof(x) ## [1] &quot;list&quot; For ordinary lists, the type and the class are both list. In a later lesson, we’ll learn how to get and set list elements, and more about when and why to use lists. You’ve already seen one list, the my.data data frame: class(my.data) ## [1] &quot;data.frame&quot; typeof(my.data) ## [1] &quot;list&quot; Under the hood, data frames are lists, and each column is a list element. Because the class is data.frame rather than list, R treats data frames differently from ordinary lists. For example, R prints data frames differently from ordinary lists. 5.9.2 Implicit Coercion R’s types fall into a natural hierarchy of expressiveness: Each type on the right is more expressive than the ones to its left. For example, with the convention that FALSE is 0 and TRUE is 1, we can represent any logical value as an integer. In turn, we can represent any integer as a double, and any double as a complex number. By writing the number out, we can also represent any complex number as a string. The point is that no information is lost as we follow the arrows from left to right along the types in the hierarchy. In fact, R will automatically and silently convert from types on the left to types on the right as needed. This is called implicit coercion. As an example, consider what happens if we add a logical value to a number: TRUE + 2 ## [1] 3 R automatically converts the TRUE to the numeric value 1, and then carries out the arithmetic as usual. We’ve already seen implicit coercion at work once before, when we learned the c function. Since the elements of a vector all have to have the same type, if you pass several different types to c, then R tries to use implicit coercion to make them the same: x &lt;- c(TRUE, &quot;hi&quot;, 1, 1+3i) class(x) ## [1] &quot;character&quot; x ## [1] &quot;TRUE&quot; &quot;hi&quot; &quot;1&quot; &quot;1+3i&quot; Implicit coercion is strictly one-way; it never occurs in the other direction. If you want to coerce a type on the right to one on the left, you can do it explicitly with one of the as.TYPE functions. For instance, the as.numeric (or as.double) function coerces to numeric: as.numeric(&quot;3.1&quot;) ## [1] 3.1 There are a few types that fall outside of the hierarchy entirely, like functions. Implicit coercion doesn’t apply to these. If you try to use these types where it doesn’t make sense to, R generally returns an error: sin + 3 ## Error in sin + 3: non-numeric argument to binary operator If you try to use these types as elements of a vector, you get back a list instead: x &lt;- c(1, 2, sum) class(x) ## [1] &quot;list&quot; Understanding how implicit coercion works will help you avoid bugs, and can also be a time-saver. "],["control-structures.html", "6 Control Structures 6.1 If Statement 6.2 Relationship Operators 6.3 If Else Statement 6.4 ifelse Statement 6.5 The switch Statement 6.6 The which Statement", " 6 Control Structures Control Structures are functions in computer programming the evaluate conditions (like, for example, the value of a variable) and change the way code behaves based upon evaluated values. For example, you might to perform one function if the value stored in the variable x is greater than 5 and a different function if it is less than less than 5. The Wikiversit Control Structures page contains a good, general description of control structures that is not programming language specific. The information that follows provides examples of the most frequetly used R control structures and how to implement them. For more complete documentation on control strcutures in R run the following help command: ?Control 6.1 If Statement The “If Statement” is the most basic of the R control structures. It tests whether a particular condition is true. For example, the below statement tests whether the value of the variable x is greater than 5. If it is, the code prints the phrase “Yay!” to screen. If it is not, the code does nothing: x &lt;- 7 if (x &gt; 5) { print(&quot;Yay!&quot;) } Note, the general syntax in the example is: control_statement (condition) { #code to execute condition is true } While you will occasionally see variations in how control structures are present, this is a fairly universal syntax across computer programming languages. The specific control structure being invoked is followed by the condition to be tested. Any actions to be performed if the condition evaluates to TRUE are place between curly brackets {} following the condition. 6.2 Relationship Operators The most common conditions evaluate whether one value is equal to ( x == y), equal to or greater than (x =&gt; y), equal to or lesser than (x &lt;= y), greater than (x &gt; y), or lesser than (x &lt; y) another value. Another common task is to test whether a BOOLEAN value is TRUE or FALSE. The syntax for this evaluation is: if (*x*) { #do something} Control structures in R also have a negation symbol which allows you to specify a negative condition. For example, the conditional statement in the following code evaluates to TRUE (meaning any code placed between the curly brackets will be executed) if the x IS NOT EQUAL to 5: if (x !=5) { #do something} 6.3 If Else Statement The “If Else” statement is similar to the “If Statement,” but it allows you specify one code path to execute if the conditional evaluates to TRUE and another to execute if the conditional evaluates to FALSE: x &lt;- 7 if (x &gt; 5) { print(&quot;Yay!&quot;) } else { print(&quot;Boo!&quot;) } 6.4 ifelse Statement R also offers a combined if/else syntax for quick execution of small code chunks: x &lt;- 12 ifelse(x &lt;= 10, &quot;x less than 10&quot;, &quot;x greater than 10&quot;) 6.5 The switch Statement The switch statement provides a mechanism for selecting between multiple possible conditions. For example, the following code returns one of several possible values from a list based upon the value of a variable: x &lt;- 3 switch(x,&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;) Note: if you pass switch a value that exceeds the number of elements in the list R will not compute a reply. 6.6 The which Statement The which statement is not a true conditional statement, but it provides a very useful way to test the values of a dataset and tell you which elements match a particular condition. In the example below, we load the R IRIS dataset and find out which rows have a Petal.Length greater than 1.4: data(&quot;iris&quot;) rows &lt;- which(iris$Petal.Length &gt; 1.4) note: you can see all of the R. build in datasets with the data() command. "],["iterating-loops.html", "7 Iterating (Loops) 7.1 For i in x Loops 7.2 While Loops 7.3 Repeat Loops 7.4 Break and Next 7.5 Iterating Data.Frame Rows in R 7.6 lapply()", " 7 Iterating (Loops) In computer programming iteration is a specific type of control structure that repeatedly runs a specified operation either for a set numbe of iterations or untul some condition is met. For example, you might want your code to peform the same math operation on all of the numbers stored in a vector of values; or perhaps you want the computer to look through a list until it finds the first entry with a value greater than 10; or, maybe you just want the computer to sound an alarm exactly 5 times. Each of these is a type of iteration or “Loop” as they are also commonly called. 7.1 For i in x Loops The most common type of loop is the “For i in x” loop which interates through each value (i) in a list (x) and does something with each value. For example, assume that x is a vector containing the following four names names: Sue, John, Heather, George, and that we want to print each of these names to screen. We can do so with the followig code: x &lt;- c(&quot;Sue&quot;, &quot;John&quot;, &quot;Heather&quot;, &quot;George&quot;) for (i in x) { print(i) } In the first line of code, we create our vecctor of names (x). Next we begin our “For i in x loop”, which has the following general syntax, which is similar to that of the conditional statements you’ve already mastered: for (condition) {} Beginning with the first element of the vector x, which in our case is “Sue”, for each iteration of the for loop the value of the corresponding element in x is assiged to the variable i and then i can be acted upon in the code icnluded between the curly brackets of the function call. In our case we simply tell the conputer to print the value of i to the sreen. Witgh each iteration, the next value in our vector is assigned to i and is subsequently printed to screen, resulting in the following output: [1] &quot;Sue&quot; [1] &quot;John&quot; [1] &quot;Heather&quot; [1] &quot;George&quot; In addition to acting on vectors or lists, For loops can also be coded to simply execute a chunk of code a designated number of times. For example, the following code will print “Hello World!” to screen exactly 10 times: for (i in 1:10) { print(&quot;Hello World!&quot; } 7.2 While Loops Unlike For loops, which iterate a defined number of times based on the length of a list of range of values provided in the method declaration, While loops continue to iterate infinitely as long as (while) a defined condition is met. For example, assume you have a boolean variable x the value of which is TRUE. You might want to write code that performs some function repeatly until the value of x is switched to FALSE. A good example of this is a case where your program asks the user to enter data, which can then be evaluated for correctness before the you allow the program to move on in its execution. In the example below, we ask the user to tell us the secret of the universe. If the user answeres with the correct answer (42), the code moves on. But if the user provides and incorrect answer, the code iterates back to the beginning of the loop and asks for input again. response &lt;- 0 while (response!=42) { response &lt;- as.integer(readline(prompt=&quot;What is the answer to the Ultimate Question of Life, the Universe, and Everything? &quot;)); } 7.3 Repeat Loops Like While loops, Repeat loops continue to iterate until a specified condition is met; but with Repeat loops that condition is defined not as an argument to the function but is a specific call to “break” that appears in the functions executable code. In the example below we assign the value 1 to a variable i and then loop through code that prints and then iterates the value of i until it reaches 10, at which time we forceably exit the loop: i &lt;- 1 repeat { print(i) i = i+1 if (i &gt; 10){ break } } 7.4 Break and Next In the previous section we saw the use of the break statement to force an exit from a repeat loop based on a conditional evaluation in an if statement. Break can actually be used inside any conditional (for, while, repeat) in order to force the end of iteration. This can be useful in a variety of contexts where you want to test for multiple conditions as a means of stopping iteration. The next command is similar to break in that it can be used inside any iteration structure to force R to skip execution of the iteration code for particular cases only. For example, we use next below to iterate through the nunbers 1 to 10 and print all values to screen EXCEPT the value 5: for (i in 1:10) { if (i == 5){ next } print(i) } 7.5 Iterating Data.Frame Rows in R In the section on for loops above, we learned that you can easily iterate across all values of a list using a “for i in x” loop. Working with R data.frames adds a bit of complexity to this process. Because R was developed as a language for statistial analysis, which always involves the comparison of multiple observations of the same variable (for example, all of the weights recroded across all patients), the default behavior of the “for i in x” loop when applied to data.frames is to iterate across columns (variables) rather than rows (observations). Consider the following example: for (i in iris) { print(i) } If you run the above code, in the first iteration R will assign the vector of values contained in the firt column (Sepal.Length) to i, in the second iteration it will assign vectore of values contained in the second column (Sepal.Width) to i, etc. Iterating through the data columns of a data.frame is useful for many (if not most) operations. However, there are time when we want to iterate through data one observation at a time. To accomplish this, we nee do specifically direct R to move through the data.frame by row, as follows: for (i in 1:nrow(iris)) { thisrow &lt;- iris[i,] print(thisrow) } 7.6 lapply() R has a built-in class of functions known as the apply family that provide a shorthand for iterating through collections of data. These behave like a for loop, but require much less actual code to accomplish. The lapply function iterates across lists, such as vectors. When you invoke lapply it applies a defined operation to each item in the subitted list and returns a list of equal length that contains the results of this calculation. In the code below, we assign the values 1 through 10 to a vector and then use lapply to subtract 1 from each item in the vector and finally print the results to screen: v &lt;- c(1:10) results &lt;- lapply(v, function(x) (x-1)) print(results) We could accomplish the exact same thing with the following for loop v &lt;- c(1:10) for (i in v) { x &lt;- i - 1 print(x) } The basic syntax of lapply is: lapply(list, function) where “list” is some list object supplied and “function” is pre-defined chunk of code that will be exectuted. You’ll learn more about functions in a future lesson. "],["functions.html", "8 Functions 8.1 Learning objectives 8.2 What is a function? 8.3 What is the basic syntax of a function in R? 8.4 Building and calling functions 8.5 Saving functions and sourcing them from another file 8.6 Saving functions and calling them from another file", " 8 Functions 8.1 Learning objectives After this lecture, you should be able to: explain what a function is read and understand the basic syntax of a function in R use this syntax to call a function use this syntax to build your own function test your function install packages in R load libraries in R 8.2 What is a function? Why build code several or a hundred times when you can build it once and then call and run it as many times as you want? The answer is, don’t! A function allows you to perform an action multiple times in R by calling it and applying it in similar contexts. For instance, if you build a function that checks the class of all vectors in a dataframe, you can name this function and then apply it to do the same operation with any other dataframe. Or, if you build a function that graphs the correlation between two numeric vectors and exports this graph to a .png file, you can call this same function and apply it to two other vectors, again and again as needed. Functions can greatly increase the efficiency of your programming, and allow you to create flexible and customized solutions. 8.3 What is the basic syntax of a function in R? The basic syntax of a function in R, or the way it should be written so that R recognizes it and applies it do perform actions, is usually stated as follows: function_name &lt;- function(argument_1, argument_2, ...) { Function body } What this does not demonstrate is that there are actually two steps to a function: building it, and applying it. We will look at both steps in the following code from DataCamp: 8.4 Building and calling functions 8.4.1 Step 1: Building the function The code chunk builds the function, setting “myFirstFun” as the name, or variable, to which they have assigned the function. The function itself runs from the word “function” down through the closing curly brace. myFirstFun&lt;-function(n) { # Compute the square of integer `n` n*n } What is an argument? In the above example, “(n)” is the argument. R looks for this argument (in this case, “n”) in the body of the function, which in this case is n*n. When we run the above script, the function is saved as an object into the global environment so that it can be called elsewhere, as demonstrated in the code chunks below. The function has no effect unless you apply it. Until that happens, the function will do nothing but wait to be called. 8.4.2 Step 2: Calling the function The code chunk below calls “myFirstFun(n)” and tells R to assign the results of the operation the function performs (n*n) to the variable “u”. But if we run this code as it is (with “n” in the parentheses), we will get an error (unless we have previously assigned “n” as a variable with a value that will accept the operation to be performed — so “n” needs to be a number in this case so that it can be multiplied). We do not actually want to perform the function on the letter “n” but rather, on a number that we will insert in the place of “n.” We can apply this function by setting “n” as a number, such as 2, in the example below. # Call the function with argument `n` u &lt;- myFirstFun(2) # Call `u` u Once we have changed “n” to a number, R then performs this operation and saves the result to a new variable “u”. We can then ask R to tell us what “u” is, and R returns or prints the results of the function, which in this case, is the number 4 (2*2). The image below shows the results we get if we attempt to run the function without changing the argument “n” to a number (giving us an error), and the results when we change “n” to the number “2” which assigns the result of the function (4) to “u”, or the number “3” which assigns the result of the function (now 9) to “u”. It is important to understand that “n” is an argument of the function “myFirstFun.” R does not consider “n” a variable, but it acts like a variable because it can change as you call the function into different contexts. To R, “u” and “myFirstFun” are variables because they are names to which values and other content are assigned. 8.4.3 Example function with one argument Step 1: Build the function In the code below, we will build a function that groups age, as a number, into a category based on conditions we set using an if…else statement. #build function with one argument (variable) categorize_age &lt;- function(age) { if(age &lt; 26){ category &lt;- &quot;25 and under&quot; } else if(age &gt; 25 &amp; age &lt; 56) { category &lt;- &quot;26 to 55&quot; } else { category &lt;- &quot;56 and over&quot; } return(category) } Step 2: Call the function in one or more contexts. In the code below, we will call the function we built above and apply it to two different values for age. Just as we saw in the example above where we inserted the numbers 2 or 3 in place of “n”, we will insert the values of age we want to use in place of the word “age” to call the new function we have built. #run check_class function on two different values for age categorize_age(15) ## [1] &quot;25 and under&quot; categorize_age(70) ## [1] &quot;56 and over&quot; 8.4.4 Example function with more than one argument A function works similarly when it has two or more arguments. Let’s use the class data we collected at the start of term, my.data, generated below. pets &lt;- c(&quot;woof&quot;, &quot;woof&quot;, &quot;cat&quot;, &quot;woof&quot;, &quot;woof&quot;, &quot;cat&quot;, &quot;woof&quot;, &quot;woof&quot;, &quot;woof&quot;, &quot;woof&quot;, &quot;woof&quot;) place &lt;- c(&quot;Temple&quot;, &quot;Yakitori&quot;, &quot;Panera&quot;, &quot;Yakitori&quot;, &quot;Guads&quot;, &quot;Home&quot;, &quot;Tea List&quot;, &quot;Raising Canes&quot;, &quot;Pachamama&quot;, &quot;Lazi Cow&quot;, &quot;Wok of Flame&quot;) time.min &lt;- c(5, 4, 4, 12, 10, 2, 3, 4, 4, 5, 19) distance.mi &lt;- c(0.9, 0.6, 0.8, 0.6, 2, 100, 0.6, 0.7, 0.8, 1, 3.7) major &lt;- c(&quot;chicanx studies&quot;, &quot;human development&quot;, &quot;economics&quot;, &quot;undeclared&quot;, &quot;psychology&quot;, &quot;MMM&quot;, &quot;psychology&quot;, &quot;undeclared&quot;, &quot;human development&quot;, &quot;undeclared&quot;, &quot;GG&quot;) my.data &lt;- data.frame(place, distance.mi, time.min, major, pets) Now let’s say we only want to look at the first vector or column in the dataframe my.data. We would write a line of code that looks like this: my.data[1] ## place ## 1 Temple ## 2 Yakitori ## 3 Panera ## 4 Yakitori ## 5 Guads ## 6 Home ## 7 Tea List ## 8 Raising Canes ## 9 Pachamama ## 10 Lazi Cow ## 11 Wok of Flame But if we wanted to create a function that looks at any column/vector in any dataframe, we could write a function that looks like this: #build function with two arguments (variable) one_column &lt;- function(data, x) { data[x] } Note: if we want to tell a user what kind of input we want to include, we could instead do something like function(dataset, column_position) or function(dataset, column_name). Once we have run the above function (telling R to save it to the global environment), we would then call this new function, which we have named one_column, and apply it to various dataframes, and telling R which column or vector in each dataframe we want to view. #run one_column function on two different dataframes one_column(my.data, 1) ## place ## 1 Temple ## 2 Yakitori ## 3 Panera ## 4 Yakitori ## 5 Guads ## 6 Home ## 7 Tea List ## 8 Raising Canes ## 9 Pachamama ## 10 Lazi Cow ## 11 Wok of Flame one_column(my.data, 2) ## distance.mi ## 1 0.9 ## 2 0.6 ## 3 0.8 ## 4 0.6 ## 5 2.0 ## 6 100.0 ## 7 0.6 ## 8 0.7 ## 9 0.8 ## 10 1.0 ## 11 3.7 8.5 Saving functions and sourcing them from another file You can save the functions you build to a separate file, and then load these in to another script in the future. For example, I can paste the function I wrote above into a new script. #build function with two arguments (variable) one_column &lt;- function(data, x) { data[x] } Then I could save this script with any file name, typically something like “functions.r”. This “functions.r” script can then be loaded into any other script with the source() function. To load your pre-saved functions into a new script, you can call the “function.R” file at the top of the new script: # Open a new script, then paste and run the following line source(&quot;functions.r&quot;) The above code will allow you to call functions that are saved in these libraries and in the functions.r file. 8.6 Saving functions and calling them from another file You can save the functions you build to a separate file, and then load these as a source. For example, I might save my functions to an R script, called “functions.r”. I can then load these sources along with my packages into my R environment. Note: Although we loaded libraries as we went through this lesson, the best practice is to run your packages and source files at the very beginning of your new R script, as shown in the example that follows. library(dplyr) library(wakefield) library(rlang) library(ggforce) source(&quot;functions.r&quot;) The above code will allow you to call functions that are saved in these libraries and in the functions.r file. "],["working-with-files-and-packages.html", "9 Working with Files and Packages 9.1 Learning Objectives 9.2 Setup 9.3 Exploring Files 9.4 Reading and Writing Files in R 9.5 Packages", " 9 Working with Files and Packages This lesson will focus on working with data files in R. It will reinforce understanding of the command line, as well as RStudio. And, it will demonstrate finding and loading packages in R. 9.1 Learning Objectives After this lecture you should be able to: Identify file extensions Identify some common data science file formats Read csv files into R Save data to disk as RDS Download packages from CRAN Read excel files into R 9.2 Setup To follow along, download this zip file from the following url: https://datalab.ucdavis.edu/adventures-in-datascience/best_in_show.zip Navigate to where you want to save your work: cd ~/Documents/ Next, make a directory: mkdir files_in_r cd files_in_r Copy the downloaded zip file into that directory: cp ~/Downloads/best_in_show.zip . Unzip the file: unzip best_in_show.zip Navigate to the newly created directory cd best_in_show 9.3 Exploring Files When working with files, its important to gather lots of information, and constantly test assumptions that you may have. This process is a key part of programming and of working in the command line. Lets start by seeing what we have, which we do with the ls command: ls Remember that ls can be modified with flags, for example, to see all the files including hidden ones, use the -a flag: ls -a You can see more information about the files with the -l flag: ls -l Modifiers can be combined for ls: ls -la You can use du -h to see the disk usage (file size) of a given file: du -h dogs.csv -h refers to human readable as by default du displays the size in block units. Being aware of the size of a file early on can help debug issues with running out of disk space, as well as issues down the line in the analysis process. For example, reading in too many too large files into R can create issues for you by overloading your system’s RAM (your computer’s working memory). You can view the disk usage for all the files in the directory with the wildcard symbol *: du -h * 9.3.1 File Extensions Most of the time, you can guess the format of a file by looking at its extension, the characters (usually three) after the last dot . in the filename. For example, the extension .jpg or .jpeg indicates a JPEG image file. Some operating systems hide extensions by default, but you can find instructions to change this setting online by searching for “show file extensions” and your operating system’s name. The extension is just part of the file’s name, so it should be taken as a hint about the file’s format rather than a guarantee. 9.3.2 Text Files A text file is one that contains human-readable lines of text. You can check this by opening the file with a text editor such as Microsoft Notepad or macOS TextEdit. Many file formats use text in order to make the format easier to work with. On the command line, you can get information about the type of a file by using the file command followed by the name of a file: file dogs.csv Note that file uses a series of tests (learn more by reading man file), to determine the file type and may not always perfectly report the type of the file. The output of file is the filename followed by a colon and then a description of the file type. In this case, the output tells us that dogs.csv is a CSV text file. A comma-separated values (CSV) file records tabular data using one line per row, with commas separating columns. From the command line we can read text files with vim: vim dogs.csv To see the type of all the files in the directory you can use the wildcard * operator: file * 9.3.3 Binary Files A binary file is one that’s not human-readable. You can’t just read off the data if you open a binary file in a text editor, but they have a number of other advantages. Compared to text files, binary files are often faster to read and take up less storage space (bytes). For demonstrations sake, see what happens when you try to use vim to ‘read’ a binary data file: vim dogs.rds Notice that the editor displays data but it isn’t human readable, it looks like a bunch of random symbols with potentially the occasional recognizable word. 9.3.4 Common Data File Types Name Extension Tabular? Text? Comma-separated Values .csv Yes Yes Tab-separated Values .tsv Yes Yes Fixed-width File .fwf Yes Yes Microsoft Excel .xlsx Yes No Microsoft Excel 1993-2007 .xls Yes No Apache Arrow .feather Yes No R Data .rds Sometimes No R Data .rda Sometimes No Plaintext .txt Sometimes Yes Extensible Markup Language .xml No Yes JavaScript Object Notation .json No Yes 9.4 Reading and Writing Files in R R has many functions for working with file systems, reading and writing files. 9.4.1 The Working Directory The working directory is the starting point R uses for relative paths. Think of the working directory as the directory R is currently “at” or watching. The function getwd returns the absolute path for the current working directory, as a string. It doesn’t require any arguments: getwd() On your computer, the output from getwd will likely be different. This is a very useful function for getting your bearings when you write relative paths. If you write a relative path and it doesn’t work as expected, the first thing to do is check the working directory. The related setwd function changes the working directory. It takes one argument: a path to the new working directory. Here’s an example: setwd(&quot;..&quot;) # Now check the working directory. getwd() Generally, you should avoid using calls to setwd in your R scripts and R Markdown files. Calling setwd makes your code more difficult to understand, and can always be avoided by using appropriate relative paths. If you call setwd with an absolute path, it also makes your code less portable to other computers. It’s fine to use setwd interactively (in the R console), but avoid making your saved code dependent on it. When working in RStudio, you can set the working directory at the start of your session in session -&gt; Set Working Directory -&gt; To Source File Location Another function that’s useful for dealing with the working directory and file system is list.files. The list.files function returns the names of all of the files and directories inside of a directory. It accepts a path to a directory as an argument, or assumes the working directory if you don’t pass a path. For instance: # List files and directories in ~/. list.files(&quot;~/&quot;) # List files and directories in the working directory. list.files() If you call list.files with an invalid path or an empty directory, the output is character(0): list.files(&quot;/this/path/is/fake/&quot;) Later on, we’ll learn about what character(0) means more generally. 9.4.2 Reading a CSV File Let’s go ahead and read the dogs.csv file we extracted from the zip file at the start. R provides a very easy built-in function for reading CSV files, and a variety of other formats for text files containing tabular data. To read a csv file into R, use read.csv: dogs = read.csv(&#39;dogs.csv&#39;) 9.4.2.1 Inspecting the Data Note that when we used read.csv, we assumed that the file extension was appropriate. We can feel confident about this because we examined the file earlier. We can also feel confident that it will fit into memory as we saw it was relatively small. Whenever you import data into R, it is crucial to check that things went as expected. To check things went according to our expections, look at the output of the read.csv function, which we saved into dogs. Let’s see what the output is. We can check what the object is with the class function. class(dogs) We can see that the read.csv function returned a dataframe. This makes sense because dataframes represent tabular data, and csv files contain tabular data. We can get more information with the str function. str concisely gives information about the content of an R object: str(dogs) Let’s check the dimensions of our dataset: dim(dogs) Recall we can access the number of rows with: nrow(dogs) And the number of columns: ncol(dogs) To display the first rows from the dataset, use head: head(dogs) And to display the last rows from the dataset, use tail: tail(dogs) 9.4.3 Reader Functions for Tabular Data The read.csv function is a shortcut for read.table, a general purpose function for reading tabular data from plain-text files into R. The read.csv function acts just like read.table, but with the arguments sep = \",\" and header = TRUE. Table of R’s read functions (https://rstudio-education.github.io/hopr/dataio.html) Function Defaults Use read.table sep = ” “, header = FALSE General-purpose read function read.csv sep = “,”, header = TRUE Comma-separated-variable (CSV) files read.delim sep = “, header = TRUE Tab-delimited files read.csv2 sep = “;”, header = TRUE, dec = “,” CSV files with European decimal format read.delim2 sep = “, header = TRUE, dec =”,” Tab-delimited files with European decimal format 9.4.4 RDS You can save any R object, such as a data frame, as an RDS file. RDS files are a great option for storing data that is intended to be loaded into R. Data saved as RDS can be quickly and accurately loaded out of and back into R without losing any information. This isn’t always the case when saving data in plain text formats such as CSV. Any R-related metadata associated with the object you are saving will be maintainted in the RDS format. This is useful in the case of data frames if your data contains factors, or dates, or other specific class attributes that won’t be represented in a csv. You would need to reproduce the process for parsing the data into R. Additionally, RDS files often times take significantly less disk space to save, as they are a compressed format. RDS files in general are faster to read. However, its important to keep in mind that RDS files are meant to be used only in R. If you save data as an RDS, you are assuming that however is using that data will have access to and an understanding of R. As a result, its common to use the RDS format for saving intermediary data in a project. While when exporting results to a collaborator, or the internet you would most likely want to use a commonly used plain-text format such as CSV. Use saveRDS to save our data as an rds file with the rds file extension. saveRDS(dogs, &quot;./outputs/dogs.rds&quot;) It’s easy to load RDS files in R with readRDS: dogs = readRDS(&quot;./outputs/dogs.rds&quot;) 9.4.5 Writing a csv We just saved and read the dogs data as an RDS file, and we can practice saving data in other forms, such as a comma separated values (csv) file. Because we will be re-using the class survey data from the first week, let’s go ahead and save this data frame as a csv in your working directory. First, you will want to create a folder called “data” in your working directory. You can do this in your console with the dir.create() function (this is like the mkdir command used in the command line). (Hint: make sure you are in your class working directory). You can run the following in your console: dir.create(&quot;data&quot;) You can also use a point-and-click method by finding the New Folder button in the bottom right pane of RStudio, under the Files tab. Next, let’s manually create the my.data data frame once more, by copying and pasting the code below. pets &lt;- c(&quot;woof&quot;, &quot;woof&quot;, &quot;cat&quot;, &quot;woof&quot;, &quot;woof&quot;, &quot;cat&quot;, &quot;woof&quot;, &quot;woof&quot;, &quot;woof&quot;, &quot;woof&quot;, &quot;woof&quot;) place &lt;- c(&quot;Temple&quot;, &quot;Yakitori&quot;, &quot;Panera&quot;, &quot;Yakitori&quot;, &quot;Guads&quot;, &quot;Home&quot;, &quot;Tea List&quot;, &quot;Raising Canes&quot;, &quot;Pachamama&quot;, &quot;Lazi Cow&quot;, &quot;Wok of Flame&quot;) time.min &lt;- c(5, 4, 4, 12, 10, 2, 3, 4, 4, 5, 19) distance.mi &lt;- c(0.9, 0.6, 0.8, 0.6, 2, 100, 0.6, 0.7, 0.8, 1, 3.7) major &lt;- c(&quot;chicanx studies&quot;, &quot;human development&quot;, &quot;economics&quot;, &quot;undeclared&quot;, &quot;psychology&quot;, &quot;MMM&quot;, &quot;psychology&quot;, &quot;undeclared&quot;, &quot;human development&quot;, &quot;undeclared&quot;, &quot;GG&quot;) my.data &lt;- data.frame(place, distance.mi, time.min, major, pets) Now that we have a data frame called my.data, we can use the write.csv() function to save this data frame as a csv in our data folder. Let’s call it class_survey.csv. write.csv(my.data, &quot;data/class_survey.csv&quot;, row.names = F) Now this data will be available to us for future use without having to copy and paste anymore. 9.4.6 Excel Files in R Excel is very popular in the data analysis world. Millions of people use Excel to input, clean, analyze, and store data. R doesn’t provide a built-in function to load Excel files. Fortunately, members of the R community share code for a variety of tasks, including loading Excel files. 9.5 Packages Lots of the most useful parts of R do not come preloaded when you install R. Packages bundle together code, documentation and data. It’s easy to share, and easy to include in your own code. Users have contributed thousands of R packages which can be found online. You can think of a package as one or more functions that are related to a specific task, that you can include in your code. Packages need to be installed on your system and then loaded into your R session. 9.5.1 CRAN Comprehensive R Archive Network (CRAN) is the main website that makes R packages accessible. 9.5.1.1 readxl readxl is a package written to provide functions for working with Excel files in R. 9.5.2 Using Packages To use an R package, it first needs to be installed on your system, and then loaded into the R session. 9.5.2.1 Installing Packages You can install packages from CRAN onto your system using install.packages. It will search for the package on CRAN, and download the code onto your computer in a place that R can access. To install the readxl package, we pass the name to install.packages: install.packages(&quot;readxl&quot;) 9.5.2.2 Loading Packages Even if the package is on your system, it is not automatically loaded into R. Every time you restart R you will need to reload each package that your script uses. Do so with library at the top of your script for each package that you will use. This signals to you and anyone else that uses your script which packages are required to run the code, and will stop the execution of the script if any of the packages are not found. To load in the readxl package we installed in the previous step, use library: library(&quot;readxl&quot;) This will load in all the functions, data, and documentation from the readxl library, so we can now access them in our R session. To see all the packages installed you can run library without any arguments: library() This displays all the installed libraries as well the path R is searching to find them. 9.5.3 Load Excel Data With readxl, we can list all the sheets in an excel spreadsheet: sheets = excel_sheets(&quot;./data/dogs.xlsx&quot;) We can then load the data with read_excel: data = read_xlsx(&quot;./data/dogs.xlsx&quot;) "],["factors-special-values-and-indexing.html", "10 Factors, Special Values, and Indexing 10.1 Learning Objectives 10.2 Factors 10.3 Special Values 10.4 Indexing", " 10 Factors, Special Values, and Indexing This lesson covers what factors are, R’s special values, and R’s various indexing operators. 10.1 Learning Objectives After this lesson, you should be able to: Recognize categorical data Explain what factors are and when to use them Explain the purpose of R’s special values, especially missing values Explain the four different types of indexes and how to use them Explain why the [[ operator is necessary and how to use it Explain the syntax to subset a data frame 10.2 Factors A feature in a data set is categorical if it measures a qualitative category. Some examples of categories are: Music genres rock, blues, alternative, folk, pop Colors red, green, blue, yellow Answers yes, no Months January, February, and so on In some cases, a feature can be interpreted as categorical or quantitative. For instance, months can be interpreted as categories, but they can also be interpreted as numbers. There’s not necessarily a “correct” interpretation; each can be useful for different kinds of analyses. R uses the class factor to represent categorical data. For instance, in the dogs data set, the group column is a factor: class(dogs$group) Visualizations and statistical models sometimes treat factors differently than other data types, so it’s important to think about whether you want R to interpret data as categorical. When you load a data set, R usually can’t tell which features are categorical. That means identifying and converting the categorical features is up to you. For beginners, it can be difficult to understand whether a feature is categorical or not. The key is to think about whether you want to use the feature to divide the data into groups. For example, if we want to know how many songs are in the rock genre, we first need to divide the songs by genre, and then count the number of songs in each group (or at least the rock group). As a second example, months recorded as numbers can be categorical or not, depending on how you want to use them. You might want to treat them as categorical (for example, to compute max rainfall in each month) or you might want to treat them as numbers (for example, to compute the number of months time between two events). The bottom line is that you have to think about what you’ll be doing in the analysis. In some cases, you might treat a feature as categorical only for part of the analysis. You can use the factor function to convert a vector into a factor: colors = c(&quot;red&quot;, &quot;green&quot;, &quot;red&quot;, &quot;blue&quot;) colors = factor(colors) colors ## [1] red green red blue ## Levels: blue green red Notice that factors are printed differently than strings. The categories of a factor are called levels. You can list the levels with the levels function: levels(colors) ## [1] &quot;blue&quot; &quot;green&quot; &quot;red&quot; Factors remember all possible levels even if you take a subset: colors[1:3] ## [1] red green red ## Levels: blue green red This is another way factors are different from strings. Factors “remember” all possible levels even if they aren’t present. This ensures that if you plot a factor, the missing levels will still be represented on the plot. You can make a factor forget levels that aren’t present with the droplevels function: droplevels(colors[1:3]) ## [1] red green red ## Levels: green red 10.3 Special Values R has four special values to represent missing or invalid data. 10.3.1 Missing Values The value NA is called the missing value. Most of the time, missing values originate from how the data were collected (as opposed to computer errors). As an example, imagine the data came from a survey, and respondents chose not to answer some questions. In the data set, their answers for those questions might be recorded as NA. Of course, there are sometimes exceptions where missing values are the result of a computation. When you see missing values in a data set, you should think carefully about what the cause might be. Sometimes documentation or other parts of the data set provide clues. The missing value is a chameleon: it can be a logical, integer, numeric, complex, or character value. By default, the missing value is logical, and the other types occur through coercion (5.9.2): class(NA) ## [1] &quot;logical&quot; class(c(1, NA)) ## [1] &quot;numeric&quot; class(c(&quot;hi&quot;, NA, NA)) ## [1] &quot;character&quot; The missing value is also contagious: it represents an unknown quantity, so using it as an argument to a function usually produces another missing value. The idea is that if the inputs to a computation are unknown, generally so is the output: NA - 3 ## [1] NA mean(c(1, 2, NA)) ## [1] NA As a consequence, testing whether an object is equal to the missing value with == doesn’t return a meaningful result: 5 == NA ## [1] NA NA == NA ## [1] NA You can use the is.na function instead: is.na(5) ## [1] FALSE is.na(NA) ## [1] TRUE is.na(c(1, NA, 3)) ## [1] FALSE TRUE FALSE Missing values are a feature that sets R apart from most other programming languages. 10.3.2 Not a Number The value NaN, called not a number, represents a quantity that’s undefined mathematically. For instance, dividing 0 by 0 is undefined: 0 / 0 ## [1] NaN class(NaN) ## [1] &quot;numeric&quot; NaN can be numeric or complex. You can use the is.nan function to test whether a value is NaN: is.nan(c(10.1, log(-1), 3)) ## Warning in log(-1): NaNs produced ## [1] FALSE TRUE FALSE 10.3.3 Null The value NULL represents a quantity that’s undefined in R. Most of the time, NULL indicates the absence of a result. For instance, vectors don’t have dimensions, so the dim function returns NULL for vectors: dim(c(1, 2)) ## NULL class(NULL) ## [1] &quot;NULL&quot; typeof(NULL) ## [1] &quot;NULL&quot; Unlike the other special values, NULL has its own unique type and class. You can use the is.null function to test whether a value is NULL: is.null(&quot;null&quot;) ## [1] FALSE is.null(NULL) ## [1] TRUE 10.3.4 Infinity The value Inf represents infinity, and can be numeric or complex. You’re most likely to encounter it as the result of certain computations: 13 / 0 ## [1] Inf class(Inf) ## [1] &quot;numeric&quot; You can use the is.infinite function to test whether a value is infinite: is.infinite(3) ## [1] FALSE is.infinite(c(-Inf, 0, Inf)) ## [1] TRUE FALSE TRUE 10.4 Indexing The way to get and set elements of a data structure is by indexing. Sometimes this is also called subsetting or (element) extraction. Indexing is a fundamental operation in R, key to reasoning about how to solve problems with the language. We first saw indexing in Section 5.7.1, where we used [, the indexing or square bracket operator, to get and set elements of vectors. We saw indexing again in Section 5.8, where we used $, the dollar sign operator, to get and set data frame columns. The indexing operator [ is R’s primary operator for indexing. It works in four different ways, depending on the type of the index you use: An empty index selects all elements A numeric index selects elements by position A character index selects elements by name A logical index selects elements for which the index is TRUE Let’s explore each in more detail. We’ll use this vector as an example, to keep things concise: x = c(a = 10, b = 20, c = 30, d = 40, e = 50) x ## a b c d e ## 10 20 30 40 50 Even though we’re using a vector here, the indexing operator works with almost all data structures, including factors, lists, matrices, and data frames. We’ll look at unique behavior for some of these later on. 10.4.1 All Elements The first way to use [ to select elements is to leave the index blank. This selects all elements: x[] ## a b c d e ## 10 20 30 40 50 This way of indexing is rarely used for getting elements, since it’s the same as entering the variable name without the indexing operator. Instead, its main use is for setting elements. Suppose we want to set all the elements of x to 5. You might try writing this: x = 5 x ## [1] 5 Rather than setting each element to 5, this sets x to the scalar 5, which is not what we want. Let’s reset the vector and try again, this time using the indexing operator: x = c(a = 10, b = 20, c = 30, d = 40, e = 50) x[] = 5 x ## a b c d e ## 5 5 5 5 5 As you can see, now all the elements are 5. So the indexing operator is necessary to specify that we want to set the elements rather than the whole variable. Let’s reset x one more time, so that we can use it again in the next example: x = c(a = 10, b = 20, c = 30, d = 40, e = 50) 10.4.2 By Position The second way to use [ is to select elements by position. This happens when you use an integer or numeric index. We already saw the basics of this in Section 5.7.1. The positions of the elements in a vector (or other data structure) correspond to numbers starting from 1 for the first element. This way of indexing is frequently used together with the sequence operator : to get ranges of values. For instance, let’s get the 2nd through 4th elements of x: x[2:4] ## b c d ## 20 30 40 You can also use this way of indexing to set specific elements or ranges of elements. For example, let’s set the 3rd and 5th elements of x to 9 and 7, respectively: x[c(3, 5)] = c(9, 7) x ## a b c d e ## 10 20 9 40 7 When getting elements, you can repeat numbers in the index to get the same element more than once. You can also use the order of the numbers to control the order of the elements: x[c(2, 1, 2, 2)] ## b a b b ## 20 10 20 20 Finally, if the index contains only negative numbers, the elements at those positions are excluded rather than selected. For instance, let’s get all elements except the 1st and 5th: x[-c(1, 5)] ## b c d ## 20 9 40 When you index by position, the index should always be all positive or all negative. Using a mix of positive and negative numbers causes R to emit error rather than returning elements, since it’s unclear what the result should be: x[c(-1, 2)] ## Error in x[c(-1, 2)]: only 0&#39;s may be mixed with negative subscripts 10.4.3 By Name The third way to use [ is to select elements by name. This happens when you use a character vector as the index, and only works with named data structures. Like indexing by position, you can use indexing by name to get or set elements. You can also use it to repeat elements or change the order. Let’s get elements a, c, d, and a again from the vector x: y = x[c(&quot;a&quot;, &quot;c&quot;, &quot;d&quot;, &quot;a&quot;)] y ## a c d a ## 10 9 40 10 Element names are generally unique, but if they’re not, indexing by name gets or sets the first element whose name matches the index: y[&quot;a&quot;] ## a ## 10 Let’s reset x again to prepare for learning about the final way to index: x = c(a = 10, b = 20, c = 30, d = 40, e = 50) 10.4.4 By Condition The fourth and final way to use [ is to select elements based on a condition. This happens when you use a logical vector as the index. The logical vector should have the same length as what you’re indexing, and will be recycled (that is, repeated) if it doesn’t. Congruent Vectors To understand indexing by condition, we first need to learn about congruent vectors. Two vectors are congruent if they have the same length and they correspond element-by-element. For example, suppose you do a survey that records each respondent’s favorite animal and age. These are two different vectors of information, but each person will have a response for both. So you’ll have two vectors that are the same length: animal = c(&quot;dog&quot;, &quot;cat&quot;, &quot;iguana&quot;) age = c(31, 24, 72) The 1st element of each vector corresponds to the 1st person, the 2nd to the 2nd person, and so on. These vectors are congruent. Notice that columns in a data frame are always congruent! Back to Indexing When you index by condition, the index should generally be congruent to the object you’re indexing. Elements where the index is TRUE are kept and elements where the index is FALSE are dropped. If you create the index from a condition on the object, it’s automatically congruent. For instance, let’s make a condition based on the vector x: is_small = x &lt; 25 is_small ## a b c d e ## TRUE TRUE FALSE FALSE FALSE The 1st element in the logical vector is_small corresponds to the 1st element of x, the 2nd to the 2nd, and so on. The vectors x and is_small are congruent. It makes sense to use is_small as an index for x, and it gives us all the elements less than 25: x[is_small] ## a b ## 10 20 Of course, you can also avoid using an intermediate variable for the condition: x[x &gt; 10] ## b c d e ## 20 30 40 50 If you create index some other way (not using the object), make sure that it’s still congruent to the object. Otherwise, the subset returned from indexing might not be meaningful. You can also use indexing by condition to set elements, just as the other ways of indexing can be used to set elements. For instance, let’s set all the elements of x that are greater than 10 to the missing value NA: x[x &gt; 10] = NA x ## a b c d e ## 10 NA NA NA NA 10.4.5 Logic All of the conditions we’ve seen so far have been written in terms of a single test. If you want to use more sophisticated conditions, R provides operators to negate and combine logical vectors. These operators are useful for working with logical vectors even outside the context of indexing. Negation The NOT operator ! converts TRUE to FALSE and FALSE to TRUE: x = c(TRUE, FALSE, TRUE, TRUE, NA) x ## [1] TRUE FALSE TRUE TRUE NA !x ## [1] FALSE TRUE FALSE FALSE NA You can use ! with a condition: y = c(&quot;hi&quot;, &quot;hello&quot;) !(y == &quot;hi&quot;) ## [1] FALSE TRUE The NOT operator is vectorized. Combinations R also has operators for combining logical values. The AND operator &amp; returns TRUE only when both arguments are TRUE. Here are some examples: FALSE &amp; FALSE ## [1] FALSE TRUE &amp; FALSE ## [1] FALSE FALSE &amp; TRUE ## [1] FALSE TRUE &amp; TRUE ## [1] TRUE c(TRUE, FALSE, TRUE) &amp; c(TRUE, TRUE, FALSE) ## [1] TRUE FALSE FALSE The OR operator | returns TRUE when at least one argument is TRUE. Let’s see some examples: FALSE | FALSE ## [1] FALSE TRUE | FALSE ## [1] TRUE FALSE | TRUE ## [1] TRUE TRUE | TRUE ## [1] TRUE c(TRUE, FALSE) | c(TRUE, TRUE) ## [1] TRUE TRUE Be careful: everyday English is less precise than logic. You might say: I want all subjects with age over 50 and all subjects that like cats. But in logic this means: (subject age over 50) OR (subject likes cats) So think carefully about whether you need both conditions to be true (AND) or at least one (OR). Rarely, you might want exactly one condition to be true. The XOR (eXclusive OR) function xor() returns TRUE when exactly one argument is TRUE. For example: xor(FALSE, FALSE) ## [1] FALSE xor(TRUE, FALSE) ## [1] TRUE xor(TRUE, TRUE) ## [1] FALSE The AND, OR, and XOR operators are vectorized. Short-circuiting The second argument is irrelevant in some conditions: FALSE &amp; is always FALSE TRUE | is always TRUE Now imagine you have FALSE &amp; long_computation(). You can save time by skipping long_computation(). A short-circuit operator does exactly that. R has two short-circuit operators: &amp;&amp; is a short-circuited &amp; || is a short-circuited | These operators only evaluate the second argument if it is necessary to determine the result. Here are some of these: TRUE &amp;&amp; FALSE ## [1] FALSE TRUE &amp;&amp; TRUE ## [1] TRUE TRUE || TRUE ## [1] TRUE c(TRUE, FALSE) &amp;&amp; c(TRUE, TRUE) ## [1] TRUE For the final expression, notice R only combines the first element of each vector. The others are ignored. In other words, the short-circuit operators are not vectorized! Because of this, generally you _should not use*_ the short-circuit operators for indexing. Their main use is in writing conditions for control structures (6) and loops (7). 10.4.6 Indexing Lists Lists are a container for other types of R objects. When you select an element from a list, you can either keep the container (the list) or discard it. The indexing operator [ almost always keeps containers. As an example, let’s get some elements from a small list: x = list(first = c(1, 2, 3), second = sin, third = c(&quot;hi&quot;, &quot;hello&quot;)) y = x[c(1, 3)] y ## $first ## [1] 1 2 3 ## ## $third ## [1] &quot;hi&quot; &quot;hello&quot; class(y) ## [1] &quot;list&quot; The result is still a list. Even if we get just one element, the result of indexing a list with [ is a list: class(x[1]) ## [1] &quot;list&quot; Sometimes this will be exactly what we want. But what if we want to get the first element of x so that we can use it in a vectorized function? Or in a function that only accepts numeric arguments? We need to somehow get the element and discard the container. The solution to this problem is the extraction operator [[, which is also called the double square bracket operator. The extraction operator is the primary way to get and set elements of lists and other containers. Unlike the indexing operator [, the extraction operator always discards the container: x[[1]] ## [1] 1 2 3 class(x[[1]]) ## [1] &quot;numeric&quot; The tradeoff is that the extraction operator can only get or set one element at a time. Note that the element can be a vector, as above. Because it can only get or set one element at a time, the extraction operator can only index by position or name. Blank and logical indexes are not allowed. The final difference between the index operator [ and the extraction operator [[ has to do with how they handle invalid indexes. The index operator [ returns NA for invalid vector elements, and NULL for invalid list elements: c(1, 2)[10] ## [1] NA x[10] ## $&lt;NA&gt; ## NULL On the other hand, the extraction operator [[ raises an error for invalid elements: x[[10]] ## Error in x[[10]]: subscript out of bounds The indexing operator [ and the extraction operator [[ both work with any data structure that has elements. However, you’ll generally use the indexing operator [ to index vectors, and the extraction operator [[ to index containers (such as lists). 10.4.7 Indexing Data Frames For two-dimensional objects, like matrices and data frames, you can pass the indexing operator [ or the extraction operator [[ a separate index for each dimension. The rows come first: DATA[ROWS, COLUMNS] For instance, let’s get the first 3 rows and all columns of the dogs data: dogs[1:3, ] ## breed group datadog popularity_all popularity lifetime_cost ## 1 Border Collie herding 3.64 45 39 20143 ## 2 Border Terrier terrier 3.61 80 61 22638 ## 3 Brittany sporting 3.54 30 30 22589 ## intelligence_rank longevity ailments price food_cost grooming kids ## 1 1 12.52 2 623 324 weekly low ## 2 30 14.00 0 833 324 weekly high ## 3 19 12.92 0 618 466 weekly medium ## megarank_kids megarank size weight height ## 1 1 29 medium NA 20 ## 2 2 1 small 13.5 NA ## 3 3 11 medium 35.0 19 As we saw in Section 10.4.1, leaving an index blank means all elements. As another example, let’s get the 3rd and 5th row, and the 2nd and 4th column: dogs[c(3, 5), c(2, 4)] ## group popularity_all ## 3 sporting 30 ## 5 sporting 130 Mixing several different ways of indexing is allowed. So for example, we can get the same above, but use column names instead of positions: dogs[c(3, 5), c(&quot;breed&quot;, &quot;longevity&quot;)] ## breed longevity ## 3 Brittany 12.92 ## 5 Welsh Springer Spaniel 12.49 For data frames, it’s especially common to index the rows by condition and the columns by name. For instance, let’s get the breed, popularity, and weight columns for all rows with toy dogs: result = dogs[dogs$group == &quot;toy&quot;, c(&quot;breed&quot;, &quot;popularity&quot;, &quot;weight&quot;)] head(result) ## breed popularity weight ## 8 Papillon 33 NA ## 13 Affenpinscher 84 NA ## 16 Chihuahua 14 5.5 ## 28 Maltese 23 5.0 ## 29 Pomeranian 17 5.0 ## 30 Shih Tzu 11 12.5 The Drop Parameter If you use two-dimensional indexing with [ to select exactly one column, you get a vector: result = dogs[1:3, 2] class(result) ## [1] &quot;factor&quot; The container is dropped, even though the indexing operator [ usually keeps containers. This also occurs for matrices. You can control this behavior with the drop parameter: result = dogs[1:3, 2, drop = FALSE] class(result) ## [1] &quot;data.frame&quot; The default is drop = TRUE. "],["strings-and-regular-expressions.html", "11 Strings and Regular Expressions 11.1 Printing Output 11.2 Escape Sequences 11.3 Character Encodings 11.4 The Tidyverse 11.5 The stringr Package 11.6 Regular Expressions", " 11 Strings and Regular Expressions After this lesson, you should be able to: Print strings with cat Read and write escape sequences and raw strings With the stringr package: Split strings on a pattern Replace parts of a string that match a pattern Extract parts of a string that match a pattern Read and write regular expressions, including: Anchors ^ and $ Character classes [] Quantifiers ?, *, and + Groups () 11.1 Printing Output The cat function prints a string in the R console. If you pass multiple arguments, they will be concatenated: cat(&quot;Hello&quot;) ## Hello cat(&quot;Hello&quot;, &quot;Nick&quot;) ## Hello Nick Pitfall 1: Printing a string is different from returning a string. The cat function only prints (and always returns NULL). For example: f = function() { cat(&quot;Hello&quot;) } x = f() ## Hello x ## NULL If you just want to concatenate some strings (but not necessarily print them), use paste instead of cat. The paste function returns a string. The str_c function in stringr (a package we’ll learn about later in this lesson) can also concatenate strings. Pitfall 2: Remember to print strings with the cat function, not the print function. The print function prints R’s representation of an object, the same as if you had entered the object in the console without calling print. For instance, print prints quotes around strings, whereas cat does not: print(&quot;Hello&quot;) ## [1] &quot;Hello&quot; cat(&quot;Hello&quot;) ## Hello 11.2 Escape Sequences In a string, an escape sequence or escape code consists of a backslash followed by one or more characters. Escape sequences make it possible to: Write quotes or backslashes within a string Write characters that don’t appear on your keyboard (for example, characters in a foreign language) For example, the escape sequence \\n corresponds to the newline character. Notice that the cat function translates \\n into a literal new line, whereas the print function doesn’t: x = &quot;Hello\\nNick&quot; cat(x) ## Hello ## Nick print(x) ## [1] &quot;Hello\\nNick&quot; As another example, suppose we want to put a literal quote in a string. We can either enclose the string in the other kind of quotes, or escape the quotes in the string: x = &#39;She said, &quot;Hi&quot;&#39; cat(x) ## She said, &quot;Hi&quot; y = &quot;She said, \\&quot;Hi\\&quot;&quot; cat(y) ## She said, &quot;Hi&quot; Since escape sequences begin with backslash, we also need to use an escape sequence to write a literal backslash. The escape sequence for a literal backslash is two backslashes: x = &quot;\\\\&quot; cat(x) ## \\ There’s a complete list of escape sequences for R in the ?Quotes help file. Other programming languages also use escape sequences, and many of them are the same as in R. 11.2.1 Raw Strings A raw string is a string where escape sequences are turned off. Raw strings are especially useful for writing regular expressions, which we’ll do later in this lesson. Raw strings begin with r\" and an opening delimiter (, [, or {. Raw strings end with a matching closing delimiter and quote. For example: x = r&quot;(quotes &quot; and backslashes \\)&quot; cat(x) ## quotes &quot; and backslashes \\ Raw strings were added to R in version 4.0 (April 2020), and won’t work correctly in older versions. 11.3 Character Encodings Computers store data as numbers. In order to store text on a computer, we have to agree on a character encoding, a system for mapping characters to numbers. For example, in ASCII, one of the most popular encodings in the United States, the character a maps to the number 97. Many different character encodings exist, and sharing text used to be an inconvenient process of asking or trying to guess the correct encoding. This was so inconvenient that in the 1980s, software engineers around the world united to create the Unicode standard. Unicode includes symbols for nearly all languages in use today, as well as emoji and many ancient languages (such as Egyptian hieroglyphs). Unicode maps characters to numbers, but unlike a character encoding, it doesn’t dictate how those numbers should be mapped to bytes (sequences of ones and zeroes). As a result, there are several different character encodings that support and are synonymous with Unicode. The most popular of these is UTF-8. In R, we can write Unicode characters with the escape sequence \\U followed by the number for the character in base 16. For instance, the number for a in Unicode is 97 (the same as in ASCII). In base 16, 97 is 61. So we can write an a as: x = &quot;\\U61&quot; # or &quot;\\u61&quot; x ## [1] &quot;a&quot; Unicode escape sequences are usually only used for characters that are not easy to type. For example, the cat emoji is number 1f408 (in base 16) in Unicode. So the string \"\\U1f408\" is the cat emoji. Note that being able to see printed Unicode characters also depends on whether the font your computer is using has a glyph (image representation) for that character. Many fonts are limited to a small number of languages. The NerdFont project patches fonts commonly used for programming so that they have better Unicode coverage. Using a font with good Unicode coverage is not essential, but it’s convenient if you expect to work with many different natural languages or love using emoji. 11.3.0.1 Character Encodings in Text Files Most of the time, R will handle character encodings for you automatically. However, if you ever read or write a text file (including CSV and other formats) and the text looks like gibberish, it might be an encoding problem. This is especially true on Windows, the only modern operating system that does not (yet) use UTF-8 as the default encoding. Encoding problems when reading a file can usually be fixed by passing the encoding to the function doing the reading. For instance, the code to read a UTF-8 encoded CSV file on Windows is: read.csv(&quot;my_data.csv&quot;, fileEncoding = &quot;UTF-8&quot;) Other reader functions may use a different parameter to set the encoding, so always check the documentation. On computers where the native language is not set to English, it can also help to set R’s native language to English with Sys.setlocale(locale = \"English\"). Encoding problems when writing a file are slightly more complicated to fix. See this blog post for thorough explanation. 11.4 The Tidyverse The Tidyverse is a popular collection of packages for doing data science in R. The packages are made by many of the same people that make RStudio. They provide alternatives to R’s built-in tools for: Manipulating strings (package stringr) Making visualizations (package ggplot2) Reading files (package readr) Manipulating data frames (packages dplyr, tidyr, tibble) And more Think of the Tidyverse as a different dialect of R. Sometimes the syntax is different, and sometimes ideas are easier or harder to express concisely. Whether to use base R or the Tidyverse is mostly subjective. As a result, the Tidyverse is somewhat polarizing in the R community. It’s useful to be literate in both, since both are popular. One advantage of the Tidyverse is that the packages are usually well-documented. For example, there are documentation websites and cheat sheets for most Tidyverse packages. 11.5 The stringr Package The rest of this lesson uses stringr, the Tidyverse package for string processing. R also has built-in functions for string processing. The main advantage of stringr is that all of the functions use a common set of parameters, so they’re easier to learn and remember. The first time you use stringr, you’ll have to install it with install.packages (the same as any other package). Then you can load the package with the library function: # install.packages(&quot;stringr&quot;) library(stringr) The typical syntax of a stringr function is: str_NAME(string, pattern, ...) Where: NAME describes what the function does string is the string to search within or transform pattern is the pattern to search for ... is additional, function-specific arguments For example, the str_detect function detects whether the pattern appears within the string: str_detect(&quot;hello&quot;, &quot;el&quot;) ## [1] TRUE str_detect(&quot;hello&quot;, &quot;ol&quot;) ## [1] FALSE Most of the stringr functions are vectorized: str_detect(c(&quot;hello&quot;, &quot;goodbye&quot;, &quot;lo&quot;), &quot;lo&quot;) ## [1] TRUE FALSE TRUE There are a lot of stringr functions. The remainder of this lesson focuses on three that are especially important, as well as some of their variants: str_split_fixed str_replace str_match You can find a complete list of stringr functions with examples in the documentation or cheat sheet. 11.5.1 Splitting Strings The str_split function splits the string at each position that matches the pattern. The characters that match are thrown away. For example, suppose we want to split a sentence into words. Since there’s a space between each word, we can use a space as the pattern: x = &quot;The students in this class are great!&quot; result = str_split(x, &quot; &quot;) result ## [[1]] ## [1] &quot;The&quot; &quot;students&quot; &quot;in&quot; &quot;this&quot; &quot;class&quot; &quot;are&quot; &quot;great!&quot; The str_split function always returns a list with one element for each input string. Here the list only has one element because x only has one element. We can get the first element with: result[[1]] ## [1] &quot;The&quot; &quot;students&quot; &quot;in&quot; &quot;this&quot; &quot;class&quot; &quot;are&quot; &quot;great!&quot; We have to use the double square bracket [[ operator here because x is a list (for a vector, we could use the single square bracket operator instead). Notice that in the printout for result, R gives us a hint that we should use [[ by printing [[1]]. To see why the function returns a list, consider what happens if we try to split two different sentences at once: x = c(x, &quot;Are you listening?&quot;) result = str_split(x, &quot; &quot;) result[[1]] ## [1] &quot;The&quot; &quot;students&quot; &quot;in&quot; &quot;this&quot; &quot;class&quot; &quot;are&quot; &quot;great!&quot; result[[2]] ## [1] &quot;Are&quot; &quot;you&quot; &quot;listening?&quot; Each sentence has a different number of words, so the vectors in the result have different lengths. So a list is the only way to store both. The str_split_fixed function is almost the same as str_split, but takes a third argument for the maximum number of splits to make. Because the number of splits is fixed, the function can return the result in a matrix instead of a list. For example: str_split_fixed(x, &quot; &quot;, 3) ## [,1] [,2] [,3] ## [1,] &quot;The&quot; &quot;students&quot; &quot;in this class are great!&quot; ## [2,] &quot;Are&quot; &quot;you&quot; &quot;listening?&quot; The str_split_fixed function is often more convenient than str_split because the nth piece of each input string is just the nth column of the result. For example, suppose we want to get the area code from some phone numbers: phones = c(&quot;717-555-3421&quot;, &quot;629-555-8902&quot;, &quot;903-555-6781&quot;) result = str_split_fixed(phones, &quot;-&quot;, 3) result[, 1] ## [1] &quot;717&quot; &quot;629&quot; &quot;903&quot; 11.5.2 Replacing Parts of Strings The str_replace function replaces the pattern the first time it appears in the string. The replacement goes in the third argument. For instance, suppose we want to change the word \"dog\" to \"cat\": x = c(&quot;dogs are great, dogs are fun&quot;, &quot;dogs are fluffy&quot;) str_replace(x, &quot;dog&quot;, &quot;cat&quot;) ## [1] &quot;cats are great, dogs are fun&quot; &quot;cats are fluffy&quot; The str_replace_all function replaces the pattern every time it appears in the string: str_replace_all(x, &quot;dog&quot;, &quot;cat&quot;) ## [1] &quot;cats are great, cats are fun&quot; &quot;cats are fluffy&quot; We can also use the str_replace and str_replace_all functions to delete part of a string by setting the replacement to the empty string \"\". For example, suppose we want to delete the comma: str_replace(x, &quot;,&quot;, &quot;&quot;) ## [1] &quot;dogs are great dogs are fun&quot; &quot;dogs are fluffy&quot; In general, stringr functions with the _all suffix affect all matches. Functions without _all only affect the first match. We’ll learn about str_match at the end of the next section. 11.6 Regular Expressions The stringr functions (including the ones we just learned) use a special language called regular expressions or regex for the pattern. The regular expressions language is also used in many other programming languages besides R. A regular expression can describe a complicated pattern in just a few characters, because some characters, called metacharacters, have special meanings. Letters and numbers are never metacharacters. They’re always literal. Here are a few examples of metacharacters (we’ll look at examples in the subsequent sections): Metacharacter Meaning . any single character (wildcard) \\ escape character (in both R and regex) ^ beginning of string $ end of string [ab] 'a' or 'b' [^ab] any character except 'a' or 'b' ? previous character appears 0 or 1 times * previous character appears 0 or more times + previous character appears 1 or more times () make a group More metacharacters are listed on the stringr cheatsheet, or in ?regex. The str_view function is especially helpful for testing regular expressions. It opens a browser window with the first match in the string highlighted. We’ll use it in the subsequent regex examples. The RegExr website is also helpful for testing regular expressions; it provides an interactive interface where you can write regular expressions and see where they match a string. 11.6.1 The Wildcard The regex wildcard character is . and matches any single character. For example: x = &quot;dog&quot; str_view(x, &quot;d.g&quot;) By default, regex searches from left to right: str_view(x, &quot;.&quot;) 11.6.2 Escape Sequences Like R, regular expressions can contain escape sequences that begin with a backslash. These are computed separately and after R escape sequences. The main use for escape sequences in regex is to turn a metacharacter into a literal character. For example, suppose we want to match a literal dot .. The regex for a literal dot is \\.. Since backslashes in R strings have to be escaped, the R string for this regex is \"\\\\.. Then the regex works: str_view(&quot;this.string&quot;, &quot;\\\\.&quot;) The double backslash can be confusing, and it gets worse if we want to match a literal backslash. We have to escape the backslash in the regex (because backslash is the regex escape character) and then also have to escape the backslashes in R (because backslash is also the R escape character). So to match a single literal backslash in R, the code is: str_view(&quot;this\\\\that&quot;, &quot;\\\\\\\\&quot;) Raw strings are helpful here, because they make the backslash literal in R strings (but still not in regex). We can use raw strings to write the above as: str_view(r&quot;(this\\that)&quot;, r&quot;(\\\\)&quot;) You can turn off regular expressions entirely in stringr with the fixed function: str_view(x, fixed(&quot;.&quot;)) It’s good to turn off regular expressions whenever you don’t need them, both to avoid mistakes and because they take longer to compute. 11.6.3 Anchors By default, a regex will match anywhere in the string. If you want to force a match at specific place, use an anchor. The beginning of string anchor is ^. It marks the beginning of the string, but doesn’t count as a character in the match. For example, suppose we want to match an a at the beginning of the string: x = c(&quot;abc&quot;, &quot;cab&quot;) str_view(x, &quot;a&quot;) str_view(x, &quot;^a&quot;) It doesn’t make sense to put characters before ^, since no characters can come before the beginning of the string. Likewise, the end of string anchor is $. It marks the end of the string, but doesn’t count as a character in the match. 11.6.4 Character Classes In regex, square brackets [ ] create a character class. A character class counts as one character, but that character can be any of the characters inside the square brackets. The square brackets themselves don’t count as characters in the match. For example, suppose we want to match a c followed by either a or t: x = c(&quot;ca&quot;, &quot;ct&quot;, &quot;cat&quot;, &quot;cta&quot;) str_view(x, &quot;c[ta]&quot;) You can use a dash - in a character class to create a range. For example, to match letters p through z: str_view(x, &quot;c[p-z]&quot;) Ranges also work with numbers and capital letters. To match a literal dash, place the dash at the end of the character class (instead of between two other characters), as in [abc-]. Most metacharacters are literal when inside a character class. For example, [.] matches a literal dot. A hat ^ at the beginning of the character class negates the class. So for example, [^abc] matches any one character except for a, b, or c: str_view(&quot;abcdef&quot;, &quot;[^abc]&quot;) 11.6.5 Quantifiers Quantifiers are metacharacters that affect how many times the preceeding character must appear in a match. The quantifier itself doesn’t count as a character in the match. For example, the ? quantifier means the preceeding character can appear 0 or 1 times. In other words, ? makes the preceeding character optional. For example: x = c(&quot;abc&quot;, &quot;ab&quot;, &quot;ac&quot;, &quot;abbc&quot;) str_view(x, &quot;ab?c&quot;) The * quantifier means the preceeding character can appear 0 or more times. In other words, * means the preceeding character can appear any number of times or not at all. str_view(x, &quot;ab*c&quot;) The + quantifier means the preceeding character must appear 1 or more times. Quantifiers are greedy, meaning they always match as many characters as possible. 11.6.6 Groups In regex, parentheses create a group. Groups can be affected by quantifiers, making it possible to repeat a pattern (rather than just a character). The parentheses themselves don’t count as characters in the match. For example: x = c(&quot;cats, dogs, and frogs&quot;, &quot;cats and frogs&quot;) str_view(x, &quot;cats(, dogs,)? and frogs&quot;) 11.6.7 Extracting Matches Groups are espcially useful with the stringr functions str_match and str_match_all. The str_match function extracts the overall match to the pattern, as well as the match to each group. So you can use str_match to split a string in more complicated ways than str_split, or to extract specifc pieces of a string. For example, suppose we want to split an email address: str_match(&quot;naulle@ucdavis.edu&quot;, &quot;([^@]+)@(.+)[.](.+)&quot;) ## [,1] [,2] [,3] [,4] ## [1,] &quot;naulle@ucdavis.edu&quot; &quot;naulle&quot; &quot;ucdavis&quot; &quot;edu&quot; "],["data-structures.html", "12 Data Structures 12.1 Tabular Data 12.2 Tree / Document Data Structures", " 12 Data Structures Merriam-Webster’s Dictionary defines data as: factual information (such as measurements or statistics) used as a basis for reasoning, discussion, or calculation information in digital form that can be transmitted or processed information output by a sensing device or organ that includes both useful and irrelevant or redundant information and must be processed to be meaningful Several key principals are introduced in the above definition: data is an intermediary step leading towards some form of analysis or or presentation, not typically an end in itself data comes in multiple formats, both digital and analogue data can be collected by both humans and machines not all data in a given dataset is necessarily meaningful, correct nor useful Data Scientists (as differentiated from statisticians or computer scientists, for example) are expert in understanding the nature of data itself and the steps necessary to assess the suitability of a given data set for answering specific research questions and the work required to properly prepare data for successful analysis. In the broadest terms, we call this process data forensics. The first step in the data forensics process is understanding the format(s) through which data are stored and transferred. 12.1 Tabular Data Tabular Data is the most ubiquitous form of data storage and the one most familiar to most users. Tabular data consists of organizing data in a table of rows and columns. Traditionally, each column in the table represents a Field of Variable and each row represents an observation or entity. For example, the table below shows a tabular organization of a subset of the mtcars dataset: Table 1.1: mpg cyl disp hp Mazda RX4 21.0 6 160.0 110 Mazda RX4 Wag 21.0 6 160.0 110 Datsun 710 22.8 4 108.0 93 Hornet 4 Drive 21.4 6 258.0 110 Hornet Sportabout 18.7 8 360.0 175 Valiant 18.1 6 225.0 105 Duster 360 14.3 8 360.0 245 Merc 240D 24.4 4 146.7 62 Merc 230 22.8 4 140.8 95 Merc 280 19.2 6 167.6 123 12.2 Tree / Document Data Structures Another popular form of data structure is the Tree structure, sometimes referred to as a Document based data structure. Tree data structures present data in a hierarchical tree-like structure in which all items related back to a single, root node. A “Family Tree” is a good example of tree structured data: The mtcars data from the above table can also be represented using a tree structure: The above image visually depicts the mtcars data as a tree, which works well for a human reader but is no parsable by the computer. There are a variety of ways to represent tree data as a computer file (or data stream) so that it can be read and parsed by the computer. In this class, we will cover two of the most popular formats: XML and JSON. 12.2.1 Structuring Data as XML XML is stands for Extensible Markup Language. Markup languages have been around since the 1960’s and were originally developed as a means to adding structured information to an existing unstructured text. In the days of analogue text preparation, professional editors typically used a blue or red pencil to make notes on typed manuscripts. The use of a specially collored pen or pencil for “marking up” documents, as the procedure was known in the industry, easily allowed subsequent readers to distinguish between editorial comment and formatting notes placed on typed manuscripts from the texts themselves. Computerized markup languages were developed as a means of allowing data specialists to markup a text in a manner that would allow the computer to distinguish between textual content and meta-information (information about the text) about the text when both types of information appear in the same file. XML is the most widely used form of markup today. In fact, nearly every webpage that you have ever viewed is actually an XML document that contains both content to be displayed and instructions for the computer on how to display that content embedded in the file using XML Tags, which are simply instructions contained with the special charcters “&lt;” and “&gt;”. For example, consider the following short email text: To: Tavi From: Jonna Subject: Meeting Date: Thursday, February 4, 2021 at 2:46 PM Don&#39;t forget about meeting with Sarah next week, 2pm in room 242. Thanks, Jonna This email contains quite a bit of structured email (sender, receiver, date/time, etc.), but there is no easy way for the computer easily extract this structure. We can solve this problem by using XML to embed information about the structure directly in the document as follows: &lt;head&gt; &lt;to&gt;Tavi&lt;/to&gt; &lt;from&gt;Jonna&lt;/from&gt; &lt;subject&gt;Meeting&lt;/subject&gt; &lt;datetime&gt; &lt;dayofweek&gt;Thursday&lt;/dayofweek&gt; &lt;month&gt;February&lt;/month&gt; &lt;day&gt;4&lt;/day&gt; &lt;year&gt;2021&lt;/year&gt; &lt;time&gt;2:46 PM&lt;/time&gt; &lt;/datetime&gt; &lt;/head&gt; &lt;body&gt; Don&#39;t forget about meeting with Sarah next week, 2pm in room 242. Thanks, &lt;signature&gt;Jonna&lt;/signuature&gt; &lt;/body&gt; By using XML, we are able to identify specific information in the email in a way that the computer is a capable of parsing. This allows us to use computational methods to easily extract information in bulk from many emails and it also allows us to program a computer program, such as an email client, to organize and properly display all of the parts of the email. The above XML example illustrates several important aspects of XML: All XML tags are enclosed in “&lt;” and “&gt;” symbols. There are 2 primary types of tags, opening tags, which designate the beginning character that is defined by the tag, and closing tags, which designate the end of the portion of the text to be associated with the opening tag. Closing tags are always indicated by slash character where tag is the name of the opening tag that is being closed. Tags be be embedded within each other in a tree-like structure. However, any tags opened within a tag must be closed before parent tag can be closed. For example, &lt;name&gt;&lt;first&gt;John&lt;/first&gt; &lt;last&gt;Doe&lt;/last&gt;&lt;/name&gt; is valid, but &lt;name&gt;&lt;first&gt;John&lt;/first&gt; &lt;last&gt;Doe&lt;/name&gt;&lt;/last&gt; is not valid. While XML was oringinally developed as a means of embedding meta information about a text directly in a text, it also quickly evolved into a stand-alone means of representing tree-structured data for exchange between computer systems. To this end, many computer applications use XML to store, share, and retrieve data. For exmaple, we can represent the data in our truncated mtcars dataset as XML as follows: &lt;cars&gt; &lt;make id=&quot;mazda&quot;&gt; &lt;model id=&quot;RX4&quot;&gt; &lt;mpg&gt;21.0&lt;/mpg&gt; &lt;cyl&gt;6&lt;/cyl&gt; &lt;disp&gt;160.0&lt;/disp&gt; &lt;hp&gt;110&lt;/hp&gt; &lt;/model&gt; &lt;model id=&quot;RX4 Wag&quot;&gt; &lt;mpg&gt;21.0&lt;/mpg&gt; &lt;cyl&gt;6&lt;/cyl&gt; &lt;disp&gt;160.0&lt;/disp&gt; &lt;hp&gt;110&lt;/hp&gt; &lt;/model&gt; &lt;/make&gt; &lt;make id=&quot;Datsun&quot;&gt; &lt;model id=&quot;710&quot;&gt; &lt;mpg&gt;22.8&lt;/mpg&gt; &lt;cyl&gt;4&lt;/cyl&gt; &lt;disp&gt;108.0&lt;/disp&gt; &lt;hp&gt;93&lt;/hp&gt; &lt;/model&gt; &lt;/make&gt; &lt;make id=&quot;Hornet&quot;&gt; &lt;model id=&quot;4 Drive&quot;&gt; &lt;mpg&gt;21.4&lt;/mpg&gt; &lt;cyl&gt;6&lt;/cyl&gt; &lt;disp&gt;258.0&lt;/disp&gt; &lt;hp&gt;110&lt;/hp&gt; &lt;/model&gt; &lt;model id=&quot;Sportabout&quot;&gt; &lt;mpg&gt;18.7&lt;/mpg&gt; &lt;cyl&gt;8&lt;/cyl&gt; &lt;disp&gt;360.0&lt;/disp&gt; &lt;hp&gt;175&lt;/hp&gt; &lt;/model&gt; &lt;/make&gt; &lt;make id=&quot;Valiant&quot;&gt; &lt;model id=&quot;valiant&quot;&gt; &lt;mpg&gt;18.1&lt;/mpg&gt; &lt;cyl&gt;6&lt;/cyl&gt; &lt;disp&gt;225.0&lt;/disp&gt; &lt;hp&gt;105&lt;/hp&gt; &lt;/model&gt; &lt;/make&gt; &lt;make id=&quot;Duster&quot;&gt; &lt;model id=&quot;360&quot;&gt; &lt;mpg&gt;14.3&lt;/mpg&gt; &lt;cyl&gt;8&lt;/cyl&gt; &lt;disp&gt;360.0&lt;/disp&gt; &lt;hp&gt;245&lt;/hp&gt; &lt;/model&gt; &lt;/make&gt; &lt;make id=&quot;Merc&quot;&gt; &lt;model id=&quot;240D&quot;&gt; &lt;mpg&gt;24.4&lt;/mpg&gt; &lt;cyl&gt;4&lt;/cyl&gt; &lt;disp&gt;146.7&lt;/disp&gt; &lt;hp&gt;62&lt;/hp&gt; &lt;/model&gt; &lt;model id=&quot;230&quot;&gt; &lt;mpg&gt;22.8&lt;/mpg&gt; &lt;cyl&gt;4&lt;/cyl&gt; &lt;disp&gt;140.8&lt;/disp&gt; &lt;hp&gt;95&lt;/hp&gt; &lt;/model&gt; &lt;model id=&quot;280&quot;&gt; &lt;mpg&gt;19.2&lt;/mpg&gt; &lt;cyl&gt;6&lt;/cyl&gt; &lt;disp&gt;167.6&lt;/disp&gt; &lt;hp&gt;123&lt;/hp&gt; &lt;/model&gt; &lt;/make&gt; &lt;/cars&gt; For an XML dataset to be technically valid, the tags used to markup the dataset must themselves be defined according to a schema, another XML document that defines all tags that can be used in marking up a dataset and the allowable tree structure of the markup (for example, which tags can be parents of which other tags, etc.). You do not need to understand, or even know, the schema being used to present data in order to read and parse an XML document. However, schemas are extremely useful (and often necessary) for building applications that perform advanced processing of XML documents, such as web browsers, emial clients, etc. For more information on XML and XML Schemas see the w3schools XML Tutorial at https://www.w3schools.com/xml/. 12.2.2 Structuring Data as JSON XML provides an excellent framework for encoding, saving, and transfering all kinds of data, and it was the dominant mode of transfering data across the internet for many years. However, XML has an Achilles’ Heel from the data transfer perspective: a lack of sparcity. If you look closely at the mtcars dataset XML example above, you will note that the markup accounts for more of the total characters in the document than the data itself. In a world where data is regularly being exchanged in real time across the network, the use of XML can result in the necessity to exchange a lot more data to accomplish the same task. This adds both time and cost to every data transaction. JavaScript Object Notation (JSON) was developed as a standard to address this problem and provides a sparse framework for representing data that introduces minimal, non-data elements into the overal data structure. JSON uses a key/value pair structure to represent data elements: &quot;model&quot;:&quot;RX4&quot; Individual data elements are then grouped to reflect more complex data structures: {&quot;model&quot;: {&quot;id&quot;: &quot;2&quot;, &quot;hp&quot;: &quot;120&quot;}} The example below shows the subsetted mtcars dataset represented as JSON. Note the use of the “[” character to indicated repeated elements in the data:] b = mice_pot\\(percent_of_act[ mice_pot\\)group == 3] "],["check-for-equal-variances---these-are-close-enough.html", "13 check for equal variances - these are close enough 13.1 [1] 689.4729 13.2 Regression 13.3 Model selection 13.4 Cross-validation", " 13 check for equal variances - these are close enough var(a) 13.1 [1] 689.4729 ```r var(b) ## [1] 429.4551 # confirm equal variances with a boxplot boxplot(a, b) # check whether the high-THC mice movement is Normal # (we already checked for the medium-dose mice) qqnorm(b) # two pop test t.test(a, b, var.equal=TRUE) ## ## Two Sample t-test ## ## data: a and b ## t = 2.7707, df = 20, p-value = 0.0118 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 7.014608 49.754345 ## sample estimates: ## mean of x mean of y ## 99.05235 70.66787 13.1.1 Hypothesis tests for non-normal data Just as with the confidence intervals, there is a bootstrap hypothesis test that can be used where the data are not normal. There are other options, too, with clever derivations. The one I’ll show you is the Wilcoxon test, which is based on the ranks of the data. Since we’e already seen that the barnacles per square meter data are not normal, I will illustrate testing the null hypothesis that \\(\\mu_0\\) = 300 barnacles per square meter. This is a one-population test, and a two-sided alternative. # wilcoxon test for 300 barnacles per square meter wilcox.test( barnacles$per_m ) ## ## Wilcoxon signed rank test with continuity correction ## ## data: barnacles$per_m ## V = 3916, p-value = 3.797e-16 ## alternative hypothesis: true location is not equal to 0 13.2 Regression Regression is a mathematical tool that allows you to estimate how some response variable is related to some predictor variable(s). There are methods that handle continuous or discrete responses of many different distributions, but we are going to focus on linear regression here. Linear regression means that the relationship between the predictor variable(s) and the response is a linear one. To illustrate, we’ll create a plot of the relationship between the waist measurement and bmi of 81 adults: # plot the relationship between the waist_cm and bmi variables with(adipose, plot(waist_cm, bmi), bty=&#39;n&#39; ) The relationship between the two is apparently linear (you can imagine drawing a straight line through the data). The general mathematical form of a linear regression line is \\[ y = a + \\beta x + \\epsilon \\] Here, the response variable (e.g., BMI) is called \\(y\\) and the predictor (e.g. waist measurement) is \\(x\\). The coefficient \\(\\beta\\) indicates how much the response changes for a change in the predictors (e.g., the expected change in BMI with a 1cm change in waist measurement). Variable \\(a\\) denotes the intercept, which is a constant offset that aligns the mean of \\(y\\) with the mean of \\(x\\). Finally, \\(\\epsilon\\) is the so-called residual error in the relationship. It represents the variation in the response that is not due to the predictor(s). 13.2.1 Fitting a regression line The R function to fit the model is called lm(). Let’s take a look at an example: # fit the linear regression BMI vs waist_cm fit = lm( bmi ~ waist_cm, data=adipose ) # plot the fitted regression: begin with the raw data with( adipose, plot(waist_cm, bmi, bty=&#39;n&#39;) ) #now plot the fitted regression line (in red) abline( coef(fit)[[1]], coef(fit)[[2]], col=&#39;red&#39; ) 13.2.2 Assumptions and diagnostics “Fitting” a linear regression model involves estimating \\(a\\) and \\(\\beta\\) in the regression equation. You can can do this fitting procedure using any data, but the results won’t be reliable unless some conditions are met. The conditions are: Observations are independent. The linear model is correct. The residual error is Normally distributed. The variance of the residual error is constant for all observations. The first of these conditions can’t be checked - it has to do with the design of the experiment. The rest can be checked, though, and I’ll take them in order. 13.2.2.1 Checking that the linear model is correct In the cast of a simple linear regression model (one predictor variable), you can check this by plotting the predictor against the response and looking for a linear trend. If you have more than one predictor variable, then you need to plot the predictions against the response to look for a linear trend. We’ll see an example by adding height as a predictor for BMI (in addition to waist measurement). # linear model for BMI using waist size and height as predictors fit2 = lm( bmi ~ waist_cm + stature_cm, data=adipose ) # plot the fitted versus the predicted values plot( fit2$fitted.values, adipose$bmi, bty=&#39;n&#39; ) 13.2.2.2 Checking that the residuals are normally distributed We have already learned about the QQ plot, which shows visually whether some values are Normally distributed. In order to depend upon the fit from a linear regression model, we need to see that the residuals are Normally distributed, and we use the QQ plot to check. 13.2.2.3 Checking that the vaiance is constant In an earlier part, we saw that the variance is the average of the squared error. But that would just be a single number, when we want to see if there is a trend. So like the QQ plot, you’l plot the residuals and use your eyeball to discern whether there is a trend in the residuals or if they are approximately constant - this is called the scale-location plot. The QQ plot and scale-location plot are both created by plotting the fitted model object # set up the pattern of the panels layout( matrix(1:4, 2, 2) ) # make the diagnostic plots plot( fit ) The “Residuals vs. Fitted” plot is checking whether the linear model is correct. There should be no obvious pattern if the data are linear (as is the casre here). The Scale-Location plot will have no obvios pattern if the variance of the residuals is constant, as is the case here (you might see a slight pattern in the smoothed red line but it isn’t obvious). And the QQ plot will look like a straight line if the residuals are from a Normal distribution, as is the case here. So this model is good. The fourth diagnostic plot is the Residuals vs. Leverage plot, which is used to identify influential outliers. We won’t get into that here. 13.2.3 Functions for inspecting regression fits When you fit a linear regression model, you are estimating the parameters of the regression equation. In order to see those estimates, use the summary() function on the fitted model object. # get the model summary summary( fit2 ) ## ## Call: ## lm(formula = bmi ~ waist_cm + stature_cm, data = adipose) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.1290 -1.0484 -0.2603 1.2661 5.2572 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.38196 3.82700 3.758 0.000329 *** ## waist_cm 0.29928 0.01461 20.491 &lt; 2e-16 *** ## stature_cm -0.08140 0.02300 -3.539 0.000680 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.724 on 78 degrees of freedom ## Multiple R-squared: 0.844, Adjusted R-squared: 0.84 ## F-statistic: 211 on 2 and 78 DF, p-value: &lt; 2.2e-16 Here you can see that the average marginal effect of one additional centimeter of waist measurement is to increase BMI by 0.3 and an additional centimeter of height is associated with a change to BMI of -0.08. You can get the coefficients from the fitted model object using the coef() function, and there are some other functions that allow you to generate the values shown in the summary table. # get the coefficients of the fitted regression beta = coef( fit2 ) round( beta, 2 ) ## (Intercept) waist_cm stature_cm ## 14.38 0.30 -0.08 Get the variance-covariance matrix: round( vcov( fit2 ), 4) # compare the square root of the diagonals of the variance-covariance matrix # to the standard errors are reported in the summary table: se = sqrt( diag( vcov(fit2) )) # here are the standard errors: round( se, 3 ) ## (Intercept) waist_cm stature_cm ## 3.827 0.015 0.023 # calculate the t-statistics for the regression coefficients # (compare these to the t-statistics reorted in the summary table) t_stats = beta / se # show the t-statistics: round( t_stats, 2 ) ## (Intercept) waist_cm stature_cm ## 3.76 20.49 -3.54 # calculate the p-values: pval = 2 * pt( abs(t_stats), df=78, lower.tail=FALSE ) round(pval, 4) ## (Intercept) waist_cm stature_cm ## 3e-04 0e+00 7e-04 # this is the residual standard error: sd( fit2$residuals ) * sqrt(80 / 78) ## [1] 1.72357 # R-squared is the proportion of variance # explained by the regression model round( 1 - var(fit2$residuals) / var(adipose$bmi), 3 ) ## [1] 0.844 13.2.4 A model that fails diagnostics We’ve seen a model that has good diagnostics. Now let’s look at one that doesn’t. This time, we’ll use linear regression to make a model of the relationship between waist measurement and the visceral adipose tissue fat (measured in grams). The visceral adipose tissue fat is abbreviated vat in the data. First, since the model uses a single predictor variable, let’s look at the relationship with a pair plot. # plot the relationship between waist_cm and vat with( adipose, plot( waist_cm, vat, bty=&#39;n&#39; )) The plot is obviously not showing a linear relationship, which will violate one of the conditions for linear regression. Also, you can see that there is less variance of vat among the observations that have smaller waist measurements. So that will violate the assumption that the residual variance has no relationship to the fitted values. To see how these will show up in the diagnostic plots, we need to fit the linear regression model. # estimate the model for vat fit_vat = lm( vat ~ waist_cm, data = adipose ) # there is no problem creating the summary table: summary( fit_vat ) ## ## Call: ## lm(formula = vat ~ waist_cm, data = adipose) ## ## Residuals: ## Min 1Q Median 3Q Max ## -996.25 -265.96 -61.87 191.24 1903.46 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3604.196 334.241 -10.78 &lt;2e-16 *** ## waist_cm 51.353 3.937 13.04 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 479 on 79 degrees of freedom ## Multiple R-squared: 0.6829, Adjusted R-squared: 0.6789 ## F-statistic: 170.2 on 1 and 79 DF, p-value: &lt; 2.2e-16 # show the diagnostic plots layout( matrix(1:4, 2, 2) ) plot( fit_vat ) There is obviously a curved pattern in the Residuals vs. Fitted plot, and in the Scale vs. Location plot. Residuals vs. Fitted shows a fan-shaped pattern, too, which reflects the increasing variance among the greater fitted values. The QQ plot is not a straight line, although the difference is not as obvious. In particular, the upper tail of residuals is heavier than expected. Together, all of these are indications that we may need to do a log transformation of the response. A log transformation helps to exaggerate the differences between smaller numbers (make the lower tail heavier) and collapse some difference among larger numbers (make the upper tail less heavy). # fit a regression model where the response is log-transformed fit_log = lm( log(vat) ~ waist_cm, data=adipose ) # plot the diagnostics for the log-transformed model plot( fit_log ) The diagnostics do not look good after the log transformation, but now the problem is the opposite: a too-heavy lower tail and residual variance decreases as the fitted value increases. Perhaps a better transformation is something in between the raw data and the log transform. Try a square-root transformation. # fit a model where the vat is square root transformed fit_sqrt = lm( sqrt(vat) ~ waist_cm, data=adipose ) # plot the diagnostics for the log-transformed model plot( fit_sqrt ) These look acceptable for real-world data. 13.2.5 Predictions and variability There are two scales of uncertainty for a regression model: uncertainty in the fitted relationship, and the uncertainty of a predicted outcome. The uncertainty of a prediction is always greater because it is calculated by adding the uncertainty of the fitted line to the uncertainty of a single data point around that fitted line. We can illustrate using the example of the model we just created to relate the waist measurement to the square root of vat. For this block of code, we’ll need the mvtnorm library to be loaded. # import mvtnorm. install it if necessary. library( mvtnorm ) # draw the data on the transformed scale with( adipose, plot(waist_cm, sqrt(vat), bty=&#39;n&#39;) ) # plot the fitted regression line abline( coef(fit_sqrt)[[1]], coef(fit_sqrt)[[2]], col=&#39;red&#39; ) # plot 100 samples from the distributon of the regression line. for (i in 1:100) { cc = rmvnorm( n=1, mean=coef(fit_sqrt), sigma=vcov(fit_sqrt) ) abline( cc[[1]], cc[[2]], col=grey(0.8)) } Clearly, the variability of the data points is greater than the variability of the fitted line (that’s why they lie outside the envelope of the fitted lines). We can extract a confidence interval for fitted values or predictions with the predict() function. # draw the data on the transformed scale with( adipose, plot(waist_cm, sqrt(vat), bty=&#39;n&#39;) ) # plot the fitted regression line abline( coef(fit_sqrt)[[1]], coef(fit_sqrt)[[2]], col=&#39;red&#39; ) # define some waist measurements where we&#39;ll construct confidence intervals pred_pts = data.frame( waist_cm = c(70, 85, 110) ) # calculate the 90% CI at each of the pred_pts ff = predict(fit_sqrt, pred_pts, interval=&quot;confidence&quot;, level=0.9) pp = predict(fit_sqrt, pred_pts, interval=&quot;prediction&quot;, level=0.9) # convert the confidence intervals to data.frames ff = as.data.frame(ff) pp = as.data.frame(pp) # add the three confidence intervals to the plots # (offset them a bit for clarity in the plot) for (i in 1:3) { lines( x=rep( pred_pts$waist_cm[[i]] - 0.5, 2), y=c( ff$lwr[[i]], ff$upr[[i]] ), col=&#39;blue&#39;, lwd=2 ) lines( x=rep( pred_pts$waist_cm[[i]] + 0.5, 2), y=c( pp$lwr[[i]], pp$upr[[i]] ), col=&#39;orange&#39;, lwd=2 ) } # add a legend legend(c(&quot;90% CI (fitted values)&quot;, &quot;90% CI (predicted values)&quot;), x=&quot;topleft&quot;, lwd=2, col=c(&quot;blue&quot;, &quot;orange&quot;), bty=&#39;n&#39;) One thing to notice about the confidence intervals is that the interval is smallest (so the precision of the estimation is greatest) at the mean of the predictor variable. This is a general rule of fitting regression. 13.3 Model selection Choosing how to represent your data is a common task in statistics. The most common target is to choose the representation (or model) that does the best job of predicting new data. We set this target because if we have a representation that predicts the future, then we can say it must accurately represent the process that generates the data. 13.4 Cross-validation Unfortunately, we rarely have information about the future, so there isn’t new data to predict. One way to do prediction with the available data is to break it into a training part and a testing part. You make represent the training part with a model, and then use it to predict the left-out testing part. If you then swap the to parts and repeat the process, you’ll have a prediction for every data point. This would be called two-fold cross valdation because the data was broken into two parts. It’s more common to break the data into more than two parts - typically five or ten or one per data point. Then one part is taken as the testing part and all the others go into the training part. The result is five-fold or ten-fold, or leave-one-out cross validation. Let’s use cross-validation to do model selection. The model this time is a representation of the number of births per day in 1978 in the United States. # plot the data gf_point( births ~ day_of_year, color = ~wknd, data=Births78 ) # make models with two through ten knots in the spline for day_of_year bmod2 = lm( births ~ wknd + ns(day_of_year, 2), data=Births78 ) bmod4 = lm( births ~ wknd + ns(day_of_year, 4), data=Births78 ) bmod6 = lm( births ~ wknd + ns(day_of_year, 6), data=Births78 ) bmod8 = lm( births ~ wknd + ns(day_of_year, 8), data=Births78 ) bmod10 = lm( births ~ wknd + ns(day_of_year, 10), data=Births78 ) # plot the 2 and 10 knot models mod_plot(bmod2, births~day_of_year + wknd) + geom_point(mapping=aes(x=day_of_year, y=births, color=wknd), data=Births78) mod_plot(bmod10, births~day_of_year + wknd) + geom_point(mapping=aes(x=day_of_year, y=births, color=wknd), data=Births78) # cross-validate to choose the best model mod_cv( bmod2, bmod4, bmod6, bmod8, bmod10, k=nrow(Births78), ntrials=1 ) ## mse model ## 1 190815.9 bmod2 ## 2 143305.1 bmod4 ## 3 104875.7 bmod6 ## 4 106094.2 bmod8 ## 5 107130.5 bmod10 # plot the data mod_plot(bmod6, births~day_of_year + wknd) + geom_point(mapping=aes(x=day_of_year, y=births, color=wknd), data=Births78) Cross-validation suggests that six knots is the ideal number, because it has the smallest mean-squared error (mse). The resulting model looks good, too. "],["data-forensics-and-cleaning-structured-data.html", "14 Data Forensics and Cleaning: Structured Data 14.1 Introduction 14.2 Tidy Data 14.3 Data Types 14.4 Special Values 14.5 Outliers", " 14 Data Forensics and Cleaning: Structured Data After this lesson, you should be able to: Cleaning: Explain what it means for a data set to be “tidy” Pivot columns in a data set to make it tidy Separate values in a column that contains multiple values per cell Convert columns to appropriate data types Forensics: Locate and count missing values in a data set Explain what it means for a value to be an “outlier” Locate and count outliers in a data set 14.1 Introduction This lesson focuses on how to identify, diagnose, and fix potential problems in tabular data sets. There are several different kinds of problems that can arise: Structural (data are transposed or rotated) Data types (some columns have the wrong types) Missing values Outliers (extreme values) We’ll see examples of each of these. 14.2 Tidy Data The Tidyverse is so named because many functions in Tidyverse packages require data frames that are in tidy form. Before we see the requirements for a data set to be tidy, we need to define or review some terminology from statistics. A feature (also called a covariate or a variable) is measurement of something, usually across multiple subjects. For example, we might decide to measure the heights of everyone in the class. Each person in the class is a subject, and the height measurement is a feature. Features don’t have to be quantitative. If we also asked each person their favorite color, then favorite color would be another feature in our data set. Features are usually, but not always, the columns in a tabular data set. An observation is a set of features measured for a single subject or at a single time. So in the preceding example, the combined height and favorite color measurement for one student is one observation. Observations are usually, but not always, the rows in a tabular data set. Now we can define what it means to be tidy. A tabular data set is tidy if and only if: Each observation has its own row. Each feature has its own column. Each value has its own cell. These rules ensure that all of the values are visually organized and are easy to access with R’s built-in indexing operations. For instance, the $ operator gets a column, and in a tidy data set, columns are features. The rules also reflect the way statisticians traditionally arrange tabular data sets. Let’s look at some examples of tidy and untidy data sets. The tidyr package provides examples, and as we’ll see later, it also provides functions to make untidy data sets tidy. As usual, we first need to load the package: # install.packages(&quot;tidyr&quot;) library(tidyr) Let’s start with an example of tidy data. This data set is included in the tidyr package and records the number of tuberculosis cases across several different countries and years: table1 ## # A tibble: 6 × 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 When you first look at a data set, think about what the observations are and what the features are. If the data set comes with documentation, it may help you figure this out. Since this data set is a tidy data set, we already know each row is an observation and each column is a feature. Features in a data set tend to take one of two roles. Some features are identifiers that describe the observed subject. These are usually not what the researcher collecting the data is trying to find out. For example, in the tuberculosis data set, the country and year columns are identifiers. Other features are measurements. These are usually the reason the researcher collected the data. For the tuberculosis data set, the cases and population columns are measurements. Thinking about whether features are identifiers or measurements can be helpful when you need to use tidyr to rearrange a data set. 14.2.1 Columns into Rows Tidy data rule 1 says each observation must have its own row. Here’s a table that breaks rule 1: table4a ## # A tibble: 3 × 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 All of the numbers measure the same thing: cases. To make the data tidy, we must rotate the 1999 and 2000 column names into rows, one for each value in the columns. The new columns are year and cases. This process means less columns (generally) and more rows, so the data set becomes longer. We can use the pivot_longer function to rotate columns into rows. We need to specify: Columns to rotate as cols. Name(s) of new identifier column(s) as names_to. Name(s) of new measuerment column(s) as values_to. Here’s the code: pivot_longer(table4a, -country, names_to = &quot;year&quot;, values_to = &quot;cases&quot;) ## # A tibble: 6 × 3 ## country year cases ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 ## 2 Afghanistan 2000 2666 ## 3 Brazil 1999 37737 ## 4 Brazil 2000 80488 ## 5 China 1999 212258 ## 6 China 2000 213766 14.2.1.1 How to Pivot Longer without tidyr You also can do this without tidyr: Subset columns to separate 1999 and 2000 into two data frames. Add a year column to each. Rename the 1999 and 2000 columns to cases. Stack the two data frames with rbind. # Step 1 df99 = table4a[-3] df00 = table4a[-2] # Step 2 df99$year = &quot;1999&quot; df00$year = &quot;2000&quot; # Step 3 names(df99)[2] = &quot;cases&quot; names(df00)[2] = &quot;cases&quot; # Step 4 rbind(df99, df00) ## # A tibble: 6 × 3 ## country cases year ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Afghanistan 745 1999 ## 2 Brazil 37737 1999 ## 3 China 212258 1999 ## 4 Afghanistan 2666 2000 ## 5 Brazil 80488 2000 ## 6 China 213766 2000 14.2.2 Rows into Columns Tidy data rule 2 says each feature must have its own column. Let’s look at a table that breaks rule 2: table2 ## # A tibble: 12 × 4 ## country year type count ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 cases 745 ## 2 Afghanistan 1999 population 19987071 ## 3 Afghanistan 2000 cases 2666 ## 4 Afghanistan 2000 population 20595360 ## 5 Brazil 1999 cases 37737 ## 6 Brazil 1999 population 172006362 ## 7 Brazil 2000 cases 80488 ## 8 Brazil 2000 population 174504898 ## 9 China 1999 cases 212258 ## 10 China 1999 population 1272915272 ## 11 China 2000 cases 213766 ## 12 China 2000 population 1280428583 Here the count column contains two different features: cases and population. To make the data tidy, we must rotate the count values into columns, one for each type value. New columns are cases and population. This process means less rows and more columns, so the data set becomes wider. We can use pivot_wider to rotate rows into columns. We need to specify: Column names to rotate as names_from. Measurements to rotate as values_from. Here’s the code: pivot_wider(table2, names_from = type, values_from = count) ## # A tibble: 6 × 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 14.2.2.1 How to Pivot Wider without tidyr You can also do this without tidyr: Subset rows to separate cases and population values. Remove the type column from each. Rename the count column to cases and population. Merge the two subsets by matching country and year. # Step 1 cases = table2[table2$type == &quot;cases&quot;, ] pop = table2[table2$type == &quot;population&quot;, ] # Step 2 cases = cases[-3] pop = pop[-3] # Step 3 names(cases)[3] = &quot;cases&quot; names(pop)[3] = &quot;population&quot; # Step 4 tidy = cbind(cases, pop[3]) This code uses the cbind function to merge the two subsets, but it would be better to use the merge function. The cbind function does not use identifier columns to check that the rows in each subset are from the same observations. Run vignette(\"pivot\") for more examples of how to use tidyr. 14.2.3 Separating Values Tidy data rule 3 says each value must have its own cell. Here’s a table that breaks rule 3: table3 ## # A tibble: 6 × 3 ## country year rate ## * &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745/19987071 ## 2 Afghanistan 2000 2666/20595360 ## 3 Brazil 1999 37737/172006362 ## 4 Brazil 2000 80488/174504898 ## 5 China 1999 212258/1272915272 ## 6 China 2000 213766/1280428583 Cells in the rate column contain two values: cases and population. These are two different features, so to make the data set tidy, we need to separate them into two different columns. So how can we separate the rate column? The rate column is a character vector (you can check this with str(table3)), so we can use the string processing functions in the stringr package. In particular, we can use the str_split_fixed function: library(stringr) columns = str_split_fixed(table3$rate, fixed(&quot;/&quot;), 2) Now we have a character matrix where the values are in separate columns. Now we need to combine these with the original data frame. There are several ways to approach this, but to be safe, let’s make a new data frame rather than overwrite the original. First we make a copy of the original: tidy_tb = table3 Next, we need to assign each column in the character matrix to a column in the tidy_tb data frame. Since the columns contain numbers, we can also use the as.numeric function to convert them to the correct data type: tidy_tb$cases = as.numeric(columns[, 1]) tidy_tb$population = as.numeric(columns[, 2]) Extracting values, converting to appropriate data types, and then combining everything into a single data frame is an extremely common pattern in data science. Using stringr functions is the most general way to separate out values in a column, but the tidyr package also provides a function separate specifically for the case we just worked through. Either package is appropriate for solving this problem. 14.3 Data Types Another problem that can arise with a data set is the data types of the columns. Recall that R’s most common data types are: character complex numeric integer logical For each of these data types, there’s a corresponding as. function to convert to that data type. For instance, as.character converts an object to a string: x = 3.1 class(x) ## [1] &quot;numeric&quot; y = as.character(x) y ## [1] &quot;3.1&quot; class(y) ## [1] &quot;character&quot; It’s also a good idea to convert categorical columns into factors with the factor function, and to convert columns of dates into dates (more about this in the next section). Let’s look at some examples using a data set collected from the classified advertisements website Craigslist. The data set contains information from ads for rentals in the Sacramento area. First we need to load the data set: cl = read.csv(&quot;data/cl_rentals.csv&quot;) Now we can use the str function to check the classes of the columns: str(cl) ## &#39;data.frame&#39;: 2987 obs. of 20 variables: ## $ title : chr &quot;$1,125 / 1br - 550ft2 - 1Bedroom Prime Location -2520 S Limited Access/Gated $1125 Avail Now (2520 S St)&quot; &quot;$1,449 / 1br - 680ft2 - 1x1 with washer &amp; dryer in unit! Move in ready! (The Phoenix/Sacramento/Folsom/SF)&quot; &quot;$1,449 / 1br - 680ft2 - 1x1 with washer &amp; dryer in unit! Move in ready! (The Phoenix/Sacramento/Folsom/SF)&quot; &quot;$1,479 / 1br - 680ft2 - 1x1 with washer &amp; dryer in unit! Move in ready! (The Phoenix/Sacramento/Folsom/SF)&quot; ... ## $ text : chr &quot;QR Code Link to This Post\\n \\n \\n* SEE MY OTHER MIDTOWN 1 bedroom apts-text for web site\\n*An&quot;| __truncated__ &quot;QR Code Link to This Post\\n \\n \\n Lease our 1x1 Apartment with Che starting at $1449+ Pric&quot;| __truncated__ &quot;QR Code Link to This Post\\n \\n \\n Lease our 1x1 Apartment with Che starting at $1449+ Pric&quot;| __truncated__ &quot;QR Code Link to This Post\\n \\n \\n Lease our 1x1 Apartment with Che starting at $1479+ Pric&quot;| __truncated__ ... ## $ latitude : num 38.6 38.6 38.6 38.6 38.6 ... ## $ longitude : num -121 -121 -121 -121 -121 ... ## $ city : chr &quot;2520 S St&quot; &quot;The Phoenix/Sacramento/Folsom/SF&quot; &quot;The Phoenix/Sacramento/Folsom/SF&quot; &quot;The Phoenix/Sacramento/Folsom/SF&quot; ... ## $ date_posted : chr &quot;2021-02-04 15:03:12&quot; &quot;2021-03-02 12:41:17&quot; &quot;2021-03-02 13:26:17&quot; &quot;2021-03-03 10:02:05&quot; ... ## $ date_updated: chr &quot;2021-03-03 08:41:39&quot; NA NA NA ... ## $ price : int 1125 1449 1449 1479 1414 1441 1615 1660 1877 1611 ... ## $ deleted : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ sqft : int 550 680 680 680 680 680 816 816 916 916 ... ## $ bedrooms : int 1 1 1 1 1 1 2 2 2 2 ... ## $ bathrooms : num 1 1 1 1 1 1 1 1 2 2 ... ## $ pets : chr NA &quot;both&quot; &quot;both&quot; &quot;both&quot; ... ## $ laundry : chr &quot;shared&quot; &quot;in-unit&quot; &quot;in-unit&quot; &quot;in-unit&quot; ... ## $ parking : chr &quot;off-street&quot; &quot;covered&quot; &quot;covered&quot; &quot;covered&quot; ... ## $ craigslist : chr &quot;sacramento&quot; &quot;sacramento&quot; &quot;sacramento&quot; &quot;sacramento&quot; ... ## $ shp_place : chr &quot;Sacramento&quot; &quot;Sacramento&quot; &quot;Sacramento&quot; &quot;Sacramento&quot; ... ## $ shp_city : chr &quot;Sacramento&quot; &quot;Sacramento&quot; &quot;Sacramento&quot; &quot;Sacramento&quot; ... ## $ shp_state : chr &quot;CA&quot; &quot;CA&quot; &quot;CA&quot; &quot;CA&quot; ... ## $ shp_county : chr &quot;Sacramento&quot; &quot;Sacramento&quot; &quot;Sacramento&quot; &quot;Sacramento&quot; ... Some of the columns have the wrong types. For instancce, the pets, laundry, and parking columns all contain categorical data, so they should be factors. Let’s convert these: cl$pets = factor(cl$pets) cl$laundry = factor(cl$laundry) cl$parking = factor(cl$parking) There’s another way we could’ve done this that uses only two lines of code, no matter how many columns there are: cols = c(&quot;pets&quot;, &quot;laundry&quot;, &quot;parking&quot;) cl[cols] = lapply(cl[cols], factor) You can use whichever approach is more convenient and makes more sense to you. If there were other columns to convert, we’d go through the same steps with the appropriate conversion function. The read.csv function does a good job at identifying columns of numbers, so it’s rarely necessary to convert columns of numbers manually. However, you may have to do this for data you got some other way (rather than loading a file). For instance, it’s common to make these conversions when scraping data from the web. 14.3.1 Dates The as.Date function converts times and dates to R’s Date class. This is data type allows us to do computations on dates, such as sorting by date or finding the number of days between two dates. How does as.Date work? We can use it to convert just about any date. The syntax is: as.Date(x, format) The parameter x is a string to convert to a date. The parameter format is a string that explains how the date is formatted. In the format string, a percent sign % followed by a character is called a specification and has special meaning. The most useful are: Specification Description January 29, 2015 %Y 4-digit year 2015 %y 2-digit year 15 %m 2-digit month 01 %B full month name January %b short month name Jan %d day of month 29 %% literal % % You can find a complete list in ?strptime. Let’s look at some examples: as.Date(&quot;January 29, 2015&quot;, &quot;%B %d, %Y&quot;) as.Date(&quot;01292015&quot;, &quot;%m%d%Y&quot;) x = c(&quot;Dec 13, 98&quot;, &quot;Dec 12, 99&quot;, &quot;Jan 1, 16&quot;) class(x) y = as.Date(x, &quot;%b %d, %y&quot;) class(y) y # You can do arithmetic on dates. y[2] - y[1] Now let’s convert the date_posted column in the Craigslist data. It’s always a good idea to test your format string before saving the results back into the data frame: dates = as.Date(cl$date_posted, &quot;%Y-%m-%d %H:%M:%S&quot;) head(dates) ## [1] &quot;2021-02-04&quot; &quot;2021-03-02&quot; &quot;2021-03-02&quot; &quot;2021-03-03&quot; &quot;2021-03-04&quot; ## [6] &quot;2021-03-04&quot; The as.Date function returns NA if conversion failed, so in this case it looks like the dates were converted correctly. Now we can save the dates back into the data frame. We can also do the same thing for the other column of dates, date_updated: cl$date_posted = dates dates = as.Date(cl$date_updated, &quot;%Y-%m-%d %H:%M:%S&quot;) head(dates) # some NAs here because the column already contained NAs ## [1] &quot;2021-03-03&quot; NA NA NA NA ## [6] NA cl$date_updated = dates 14.4 Special Values R has four special values: NA, which represents a missing value. Inf, which represents an infinite value. NaN, read as “not a number,” which represents a value that’s not defined mathematically. For example: 0 / 0 or sqrt(-1). NULL, which represents a value that’s not defined in R. Any R vector can contain a missing value NA. Only numeric and complex vectors can contain Inf and NaN values. Vectors can’t contain NULL values, but lists can. In a data frame, each column is a vector, so you’ll generally only have to deal with NA, Inf, and NaN. Missing values are what you’re most likely to encounter. Missing values represent unknown information. Using an unknown value in a computation produces an unknown result, so we say that missing values are contagious. Here’s an example: NA + 3 ## [1] NA Because of this property, testing equality on missing values with == returns a missing value! So if we want to check whether an object is the missing value, we have to use the is.na function instead: is.na(3) ## [1] FALSE is.na(NA) ## [1] TRUE There are analogous functions is.infinite, is.nan, and is.null for checking whether an object is one of the other special values. The first time you work with a data set, it’s a good idea to check for special values. If too much data is missing, it might not be possible to produce useful visualizations and statistics. We can use is.na together with the table function to check how many values are missing in a column. Let’s try it with some of the columns in the Craigslist data: table(is.na(cl$parking)) ## ## FALSE ## 2987 table(is.na(cl$sqft)) ## ## FALSE TRUE ## 2640 347 Some people prefer to use is.na with the sum function to count missing values, so you may see that as well. The summary function is another way to count missing values, but keep in mind that it only shows the missing values for some data types: summary(cl) ## title text latitude longitude ## Length:2987 Length:2987 Min. :33.99 Min. :-123.2 ## Class :character Class :character 1st Qu.:38.55 1st Qu.:-121.5 ## Mode :character Mode :character Median :38.59 Median :-121.4 ## Mean :38.59 Mean :-121.5 ## 3rd Qu.:38.67 3rd Qu.:-121.3 ## Max. :40.19 Max. : -76.5 ## NA&#39;s :3 NA&#39;s :3 ## city date_posted date_updated price ## Length:2987 Min. :2021-01-30 Min. :2021-02-27 Min. : 1 ## Class :character 1st Qu.:2021-02-24 1st Qu.:2021-03-02 1st Qu.: 1471 ## Mode :character Median :2021-03-01 Median :2021-03-03 Median : 1730 ## Mean :2021-02-26 Mean :2021-03-02 Mean : 1764 ## 3rd Qu.:2021-03-03 3rd Qu.:2021-03-03 3rd Qu.: 1975 ## Max. :2021-03-04 Max. :2021-03-04 Max. :15630 ## NA&#39;s :1801 NA&#39;s :35 ## deleted sqft bedrooms bathrooms pets ## Mode :logical Min. : 200.0 Min. :0.000 Min. :1.00 both:2511 ## FALSE:2987 1st Qu.: 681.0 1st Qu.:1.000 1st Qu.:1.00 cats: 46 ## Median : 801.0 Median :2.000 Median :1.00 dogs: 31 ## Mean : 881.5 Mean :1.529 Mean :1.36 none: 385 ## 3rd Qu.: 1000.0 3rd Qu.:2.000 3rd Qu.:2.00 NA&#39;s: 14 ## Max. :88900.0 Max. :7.000 Max. :4.00 ## NA&#39;s :347 NA&#39;s :10 NA&#39;s :10 ## laundry parking craigslist shp_place ## hookup : 18 covered :1872 Length:2987 Length:2987 ## in-unit:2030 garage : 430 Class :character Class :character ## none : 21 none : 30 Mode :character Mode :character ## shared : 918 off-street: 482 ## street : 169 ## valet : 4 ## ## shp_city shp_state shp_county ## Length:2987 Length:2987 Length:2987 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## Finally, you can use is.na with the which function to get the specific positions of elements that are missing in a vector or column: which(is.na(cl$sqft)) ## [1] 14 17 18 19 29 36 37 46 47 48 69 70 73 79 83 ## [16] 96 97 166 178 189 227 228 229 252 286 293 294 298 303 376 ## [31] 378 410 418 487 490 493 498 499 500 515 600 614 626 653 664 ## [46] 680 681 685 686 687 705 706 708 709 713 720 721 724 733 736 ## [61] 737 740 741 742 744 745 746 750 848 849 852 865 866 878 879 ## [76] 880 881 882 883 884 885 886 887 888 889 890 934 1060 1061 1062 ## [91] 1090 1092 1095 1102 1103 1104 1106 1107 1108 1109 1114 1115 1116 1117 1118 ## [106] 1121 1129 1130 1133 1134 1152 1155 1157 1158 1159 1166 1172 1175 1182 1184 ## [121] 1186 1208 1236 1237 1259 1270 1275 1276 1283 1302 1315 1320 1321 1322 1323 ## [136] 1328 1336 1359 1360 1365 1366 1377 1380 1383 1403 1404 1417 1426 1427 1430 ## [151] 1433 1434 1475 1476 1491 1492 1493 1494 1496 1497 1508 1546 1547 1548 1550 ## [166] 1551 1552 1561 1565 1589 1590 1592 1593 1600 1636 1637 1638 1657 1658 1659 ## [181] 1663 1666 1667 1684 1691 1727 1731 1738 1750 1781 1790 1800 1832 1833 1835 ## [196] 1836 1845 1846 1871 1897 1902 1913 1917 1959 1960 2014 2017 2019 2031 2033 ## [211] 2036 2038 2040 2114 2115 2118 2119 2120 2124 2147 2148 2152 2163 2164 2165 ## [226] 2184 2189 2207 2212 2214 2238 2239 2249 2258 2259 2302 2318 2329 2330 2349 ## [241] 2350 2351 2352 2353 2357 2360 2361 2378 2398 2399 2412 2418 2423 2424 2441 ## [256] 2444 2445 2448 2460 2481 2484 2485 2490 2491 2497 2508 2515 2516 2517 2527 ## [271] 2528 2534 2539 2554 2558 2571 2572 2574 2575 2581 2584 2587 2611 2648 2649 ## [286] 2650 2651 2661 2662 2856 2857 2858 2859 2860 2864 2865 2866 2870 2871 2872 ## [301] 2878 2879 2882 2883 2886 2887 2888 2889 2890 2895 2896 2898 2901 2905 2906 ## [316] 2907 2909 2910 2911 2912 2915 2919 2920 2941 2944 2945 2951 2952 2953 2954 ## [331] 2959 2960 2961 2962 2963 2964 2965 2966 2967 2968 2972 2975 2976 2977 2978 ## [346] 2979 2980 cl[14, ] ## title ## 14 $1,250 / 2br - Amazing location: Midtown 1 Bedroom Apt. ## text ## 14 QR Code Link to This Post\\n \\n \\nEntrances at front and rear gates are locked for added privacy. This property also offers an enclosed garage with 5 rental parking spaces for residents (all are rented at this moment, but you will be offered the space when it becomes available). Street parking is easy to find, and permits through the city are free and easy to obtain with proof of residence. ## latitude longitude city date_posted date_updated price deleted sqft bedrooms ## 14 NA NA &lt;NA&gt; 2021-03-03 &lt;NA&gt; 1250 FALSE NA 2 ## bathrooms pets laundry parking craigslist shp_place shp_city shp_state ## 14 1 none hookup garage sacramento &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## shp_county ## 14 &lt;NA&gt; 14.4.1 Reasoning about Missing Values If your data contains missing values, it’s important to think about why the values are missing. Statisticians use two different terms to describe why data is missing: missing at random (MAR) missing not at random (MNAR) - causes bias! When values are missing at random, the cause for missingness is not related to any of the other features. This is rare in practice. For example, if people in a food survey accidentally overlook some questions. When values are missing not at random, the cause for missingness depends on other features. These features may or may not be in the data set. Think of this as a form of censorship. For example, if people in a food survey refuse to report how much sugar they ate on days where they ate junk food, data is missing not at random. Values MNAR can bias an analysis. The default way to handle missing values in R is to ignore them. This is just a default, not necessarily the best or even an appropriate way to deal with them. You can remove missing values from a data set by indexing: cl_no_sqft_na = cl[!is.na(cl$sqft), ] head(cl_no_sqft_na) ## title ## 1 $1,125 / 1br - 550ft2 - 1Bedroom Prime Location -2520 S Limited Access/Gated $1125 Avail Now (2520 S St) ## 2 $1,449 / 1br - 680ft2 - 1x1 with washer &amp; dryer in unit! Move in ready! (The Phoenix/Sacramento/Folsom/SF) ## 3 $1,449 / 1br - 680ft2 - 1x1 with washer &amp; dryer in unit! Move in ready! (The Phoenix/Sacramento/Folsom/SF) ## 4 $1,479 / 1br - 680ft2 - 1x1 with washer &amp; dryer in unit! Move in ready! (The Phoenix/Sacramento/Folsom/SF) ## 5 $1,414 / 1br - 680ft2 - 1x1 with washer &amp; dryer in unit! Move in ready! (The Phoenix/Sacramento/Folsom/SF) ## 6 $1,441 / 1br - 680ft2 - 1x1 with washer &amp; dryer in unit! Move in ready! (The Phoenix/Sacramento/Folsom/SF) ## text ## 1 QR Code Link to This Post\\n \\n \\n* SEE MY OTHER MIDTOWN 1 bedroom apts-text for web site\\n*An upstairs apt @ 2520 S is coming available 3/18/21\\n*I have 4 apts coming avail in midtown\\n*New flooring in lower apt and redone hardwood flooring in upper unit\\n*1 Bedroom lower unit in 20 unit complex (2-10 unit buildings-courtyard in middle) with manager on site\\n*Gated front and back\\n*9 parking spots in back\\n*Laundry on site with new washers and dryers (coin op)\\n*Owner pays water/sewer/garbage\\n*Wall heat and window air\\n*New paint and new Pergo-type wood flooring \\n*Updated lighting\\n*Nicely maintained building and grounds\\n*$500 deposit\\n*Non-Smoking/vaping Complex\\n*Long time Mgr on Site\\n*No dogs\\n*Pictures of a like unit\\n*Text/call showing Wes show contact info\\n to get copy of video\\n*You need to make 3X rent, have good rental history and credit score of 600 or greater to qualify-no dogs. ## 2 QR Code Link to This Post\\n \\n \\n Lease our 1x1 Apartment with Che starting at $1449+ Prices Subject to change daily! \\n Apartment Features: \\nPrivate Balconies &amp; Patios \\nLuxury Vinyl Wood Flooring \\nDesigner Interior Paint \\nUpgraded cabinets and countertops \\nModern white doors and trim \\nModern white base boards \\nEnergy efficient appliances \\nSpacious bathroom \\nTONS of closet and storage space \\nSac State Shuttle-including Sac State Shuttle App! PET FRIENDLY - MEOW! WOOF! \\n\\nUpgraded Fitness Centers \\nTanning Booth \\nHydro Massage Table \\nResident Business Center \\nResort-Style Pool Furniture \\nSparkling Waterfall Wall \\nFire pit locations \\nBeach Area for lounging \\nOutdoor Recreation Area \\nSand Volleyball Court \\nGrilling Locations Call today to schedule your tour ... show contact info\\n -or- \\nStop in to visit Mon - Sat 10:00a - 5:00p for more info! :D \\n\\n\\nCommunity Amenities: \\nComplete fitness center \\nFour sparkling pools \\nLighted tennis courts \\nFull basketball court \\nWhirlpool Jacuzzi \\n24 hour Emergency Maintenance Hornet Shuttle Access to Sacramento State University \\n Easy Access to Public Transportation (lightrail and bus) \\n Easy Access to the American River Trail And so much more! \\n\\n Check us out online @ www.thephoenixsacramento.com TODAY! \\n Connect with us: \\nFacebook = The Phoenix Apartment Living \\nTwitter = @thephoenixlife Prices and availability are subject to change. Offered prices are starting prices for base rent only. Other charges, conditions, fees, and terms may apply. Equal housing opportunity provider ## 3 QR Code Link to This Post\\n \\n \\n Lease our 1x1 Apartment with Che starting at $1449+ Prices Subject to change daily! \\n Apartment Features: \\nPrivate Balconies &amp; Patios \\nLuxury Vinyl Wood Flooring \\nDesigner Interior Paint \\nUpgraded cabinets and countertops \\nModern white doors and trim \\nModern white base boards \\nEnergy efficient appliances \\nSpacious bathroom \\nTONS of closet and storage space \\nSac State Shuttle-including Sac State Shuttle App! PET FRIENDLY - MEOW! WOOF! \\n\\nUpgraded Fitness Centers \\nTanning Booth \\nHydro Massage Table \\nResident Business Center \\nResort-Style Pool Furniture \\nSparkling Waterfall Wall \\nFire pit locations \\nBeach Area for lounging \\nOutdoor Recreation Area \\nSand Volleyball Court \\nGrilling Locations Call today to schedule your tour ... show contact info\\n -or- \\nStop in to visit Mon - Sat 10:00a - 5:00p for more info! :D \\n\\n\\nCommunity Amenities: \\nComplete fitness center \\nFour sparkling pools \\nLighted tennis courts \\nFull basketball court \\nWhirlpool Jacuzzi \\n24 hour Emergency Maintenance Hornet Shuttle Access to Sacramento State University \\n Easy Access to Public Transportation (lightrail and bus) \\n Easy Access to the American River Trail And so much more! \\n\\n Check us out online @ www.thephoenixsacramento.com TODAY! \\n Connect with us: \\nFacebook = The Phoenix Apartment Living \\nTwitter = @thephoenixlife Prices and availability are subject to change. Offered prices are starting prices for base rent only. Other charges, conditions, fees, and terms may apply. Equal housing opportunity provider ## 4 QR Code Link to This Post\\n \\n \\n Lease our 1x1 Apartment with Che starting at $1479+ Prices Subject to change daily! \\n Apartment Features: \\nPrivate Balconies &amp; Patios \\nLuxury Vinyl Wood Flooring \\nDesigner Interior Paint \\nUpgraded cabinets and countertops \\nModern white doors and trim \\nModern white base boards \\nEnergy efficient appliances \\nSpacious bathroom \\nTONS of closet and storage space \\nSac State Shuttle-including Sac State Shuttle App! PET FRIENDLY - MEOW! WOOF! \\n\\nUpgraded Fitness Centers \\nTanning Booth \\nHydro Massage Table \\nResident Business Center \\nResort-Style Pool Furniture \\nSparkling Waterfall Wall \\nFire pit locations \\nBeach Area for lounging \\nOutdoor Recreation Area \\nSand Volleyball Court \\nGrilling Locations Call today to schedule your tour ... show contact info\\n -or- \\nStop in to visit Mon - Sat 10:00a - 5:00p for more info! :D \\n\\n\\nCommunity Amenities: \\nComplete fitness center \\nFour sparkling pools \\nLighted tennis courts \\nFull basketball court \\nWhirlpool Jacuzzi \\n24 hour Emergency Maintenance Hornet Shuttle Access to Sacramento State University \\n Easy Access to Public Transportation (lightrail and bus) \\n Easy Access to the American River Trail And so much more! \\n\\n Check us out online @ www.thephoenixsacramento.com TODAY! \\n Connect with us: \\nFacebook = The Phoenix Apartment Living \\nTwitter = @thephoenixlife Prices and availability are subject to change. Offered prices are starting prices for base rent only. Other charges, conditions, fees, and terms may apply. Equal housing opportunity provider ## 5 QR Code Link to This Post\\n \\n \\n Lease our 1x1 Apartment with Juliet starting at $1414+ Prices Subject to change daily! \\n Apartment Features: \\nPrivate Balconies &amp; Patios \\nLuxury Vinyl Wood Flooring \\nDesigner Interior Paint \\nUpgraded cabinets and countertops \\nModern white doors and trim \\nModern white base boards \\nEnergy efficient appliances \\nSpacious bathroom \\nTONS of closet and storage space \\nSac State Shuttle-including Sac State Shuttle App! PET FRIENDLY - MEOW! WOOF! \\n\\nUpgraded Fitness Centers \\nTanning Booth \\nHydro Massage Table \\nResident Business Center \\nResort-Style Pool Furniture \\nSparkling Waterfall Wall \\nFire pit locations \\nBeach Area for lounging \\nOutdoor Recreation Area \\nSand Volleyball Court \\nGrilling Locations Call today to schedule your tour ... show contact info\\n -or- \\nStop in to visit Mon - Sat 10:00a - 5:00p for more info! :D \\n\\n\\nCommunity Amenities: \\nComplete fitness center \\nFour sparkling pools \\nLighted tennis courts \\nFull basketball court \\nWhirlpool Jacuzzi \\n24 hour Emergency Maintenance Hornet Shuttle Access to Sacramento State University \\n Easy Access to Public Transportation (lightrail and bus) \\n Easy Access to the American River Trail And so much more! \\n\\n Check us out online @ www.thephoenixsacramento.com TODAY! \\n Connect with us: \\nFacebook = The Phoenix Apartment Living \\nTwitter = @thephoenixlife Prices and availability are subject to change. Offered prices are starting prices for base rent only. Other charges, conditions, fees, and terms may apply. Equal housing opportunity provider ## 6 QR Code Link to This Post\\n \\n \\n Lease our 1x1 Apartment with Che starting at $1441+ Prices Subject to change daily! \\n Apartment Features: \\nPrivate Balconies &amp; Patios \\nLuxury Vinyl Wood Flooring \\nDesigner Interior Paint \\nUpgraded cabinets and countertops \\nModern white doors and trim \\nModern white base boards \\nEnergy efficient appliances \\nSpacious bathroom \\nTONS of closet and storage space \\nSac State Shuttle-including Sac State Shuttle App! PET FRIENDLY - MEOW! WOOF! \\n\\nUpgraded Fitness Centers \\nTanning Booth \\nHydro Massage Table \\nResident Business Center \\nResort-Style Pool Furniture \\nSparkling Waterfall Wall \\nFire pit locations \\nBeach Area for lounging \\nOutdoor Recreation Area \\nSand Volleyball Court \\nGrilling Locations Call today to schedule your tour ... show contact info\\n -or- \\nStop in to visit Mon - Sat 10:00a - 5:00p for more info! :D \\n\\n\\nCommunity Amenities: \\nComplete fitness center \\nFour sparkling pools \\nLighted tennis courts \\nFull basketball court \\nWhirlpool Jacuzzi \\n24 hour Emergency Maintenance Hornet Shuttle Access to Sacramento State University \\n Easy Access to Public Transportation (lightrail and bus) \\n Easy Access to the American River Trail And so much more! \\n\\n Check us out online @ www.thephoenixsacramento.com TODAY! \\n Connect with us: \\nFacebook = The Phoenix Apartment Living \\nTwitter = @thephoenixlife Prices and availability are subject to change. Offered prices are starting prices for base rent only. Other charges, conditions, fees, and terms may apply. Equal housing opportunity provider ## latitude longitude city date_posted date_updated ## 1 38.5728 -121.4675 2520 S St 2021-02-04 2021-03-03 ## 2 38.5511 -121.4068 The Phoenix/Sacramento/Folsom/SF 2021-03-02 &lt;NA&gt; ## 3 38.5511 -121.4068 The Phoenix/Sacramento/Folsom/SF 2021-03-02 &lt;NA&gt; ## 4 38.5511 -121.4068 The Phoenix/Sacramento/Folsom/SF 2021-03-03 &lt;NA&gt; ## 5 38.5511 -121.4068 The Phoenix/Sacramento/Folsom/SF 2021-03-04 &lt;NA&gt; ## 6 38.5511 -121.4068 The Phoenix/Sacramento/Folsom/SF 2021-03-04 &lt;NA&gt; ## price deleted sqft bedrooms bathrooms pets laundry parking craigslist ## 1 1125 FALSE 550 1 1 &lt;NA&gt; shared off-street sacramento ## 2 1449 FALSE 680 1 1 both in-unit covered sacramento ## 3 1449 FALSE 680 1 1 both in-unit covered sacramento ## 4 1479 FALSE 680 1 1 both in-unit covered sacramento ## 5 1414 FALSE 680 1 1 both in-unit covered sacramento ## 6 1441 FALSE 680 1 1 both in-unit covered sacramento ## shp_place shp_city shp_state shp_county ## 1 Sacramento Sacramento CA Sacramento ## 2 Sacramento Sacramento CA Sacramento ## 3 Sacramento Sacramento CA Sacramento ## 4 Sacramento Sacramento CA Sacramento ## 5 Sacramento Sacramento CA Sacramento ## 6 Sacramento Sacramento CA Sacramento The na.omit function is less precise than indexing, because it removes rows that have a missing value in any column. This means lots of information gets lost. Another way to handle missing values is to impute, or fill in, the values with estimates based on other data in the data set. We won’t get into the details of how to impute missing values here, since it is a fairly deep subject. Generally it is safe to impute MAR values, but not MNAR values. 14.5 Outliers An outlier is an anomalous or extreme value in a data set. We can picture this as a value that’s far away from most of the other values. Sometimes outliers are a natural part of the data set. In other situations, outliers can indicate errors in how the data were measured, recorded, or cleaned. There’s no specific definition for “extreme” or “far away”. A good starting point for detecting outliers is to make a plot that shows how the values are distributed. Box plots and density plots work especially well for this: library(ggplot2) ggplot(cl, aes(x = sqft)) + geom_boxplot() ## Warning: Removed 347 rows containing non-finite values (stat_boxplot). Statisticians tend to use the rule of thumb that any value more than 3 standard deviations away from the mean is an outlier. You can use the scale function to compute how many standard deviations the elements in a column are from their mean: z = scale(cl$sqft) head(z) ## [,1] ## [1,] -0.1910838 ## [2,] -0.1161393 ## [3,] -0.1161393 ## [4,] -0.1161393 ## [5,] -0.1161393 ## [6,] -0.1161393 which(z &lt;= -3 | 3 &lt;= z) ## [1] 1261 2461 Be careful to think about what your specific data set measures, as this definition isn’t appropriate in every situation. How can you handle outliers? First, try inspecting other features from the row to determine whether the outlier is a valid measurement or an error. When an outlier is valid, keep it. If the outlier interferes with a plot you want to make, you can adjust the x and y limits on plots as needed to “ignore” the outlier. Make sure to mention this in the plot’s title or caption. When an outlier is not valid, first try to correct it. For example: Correct with a different covariate from the same observation. Estimate with a mean or median of similar observations. This is another example of imputing values. For example, in the Craigslist data, we can use the text column to try to correct outliers: cat(cl$text[1261]) ## QR Code Link to This Post ## ## ## Villages of the Galleria ## 701 Gibson Drive, Roseville, CA, 95678 ## Want more information? Follow this link: ## http://rcmi.aptdetails.com/49u13n ## Call Now: show contact info ## ## Roseville&#39;s Premier Luxury Condominium Rentals ## This is a 1 Bedroom, 1 Bath, approximately 819 Sq. Ft. ## Signature Collection ## This collection of fully renovated homes is limited to a select few. These unique homes are renting quickly. ## The beautifully remodeled floor plan offers an entertainment style kitchen, gracious living area, formal dining room with access to your outdoor balcony, designer two tone paint with crown molding, spacious bathroom with relaxing oval bath tub, linen closet and large vanity. The bedroom offers a sliding glass door giving you additional access to the private balcony over looking the picturesque courtyard. ## Brand New Featured Interiors: ## Entertainment Style Kitchen ## • Beautiful warm espresso custom built cabinets with brush nickel hardware ## • Opulent Granite Countertops with backsplash ## • Satin finish under mount sink with disposal and upgraded Moen faucet and fixtures ## • Stainless steel appliances, spacious built in microwave, multi-cycle dishwasher and self-cleaning oven ## • Plant/décor cabinet ledge ## • Spacious pantry and personalized custom shelving in all cabinets ## • Attractive bright recessed lighting ## • Private in home personal laundry room with full size washer &amp; dryer ## Living and Dining ## • Hand laid tile resembling hard wood flooring ## • Designer two-tone paint with white accent crown molding and baseboards ## • Upgraded wooden style blinds ## • Dual pane windows featuring custom framed molding ## • Spacious coat closet ## Bath ## • Oval Roman soaking tub with surround Opulent granite walls ## • Warm espresso custom built cabinets with brush nickel hardware ## • Spacious linen closet and personal cabinet storage ## • Hand selected Opulent Granite countertops ## • Unique hand crafted above counter sink ## • Contemporary waterfall faucet ## • Custom wood-look framed mirror ## • White glass contemporary light fixture with brush nickel base ## • Upgraded brush nickel accents (towel bars and holders) ## • Hand laid tile resembling hard wood flooring ## Bedroom and Closet ## • Rich plush carpeting ## • Spacious walk in closets with personalized built in custom organizers and compartments ## Other Features and Amenities ## • Brilliant bright recessed lighting ## • Central heat and air ## • Private balcony or terrace ## • Pre-wired for high-speed internet, multi-line phone and cable ## • Brushed nickel hardware accents (door knobs, latches, deadbolts, locks, door knocker and light fixtures) ## • Covered parking ## • Additional patio storage ## Select Homes Offer ## • Private detached garage ## • Additional linen or storage space ## • Cozy Gas Fireplace with carved stone-look mantel and molding ## Style, sophistication, beautiful landscaping and stunning architecture accent the Villages of the ## Galleria apartment homes, located in dynamic Roseville, California. Villages of the Galleria is just minutes from the Galleria Mall and Fountains at Roseville and offers easy freeway access to downtown, Sacramento International Airport, Arco Arena and major employers such as NEC, Oracle and HP. Select from a variety of one, two or three bedroom floor plans. All apartment homes offer gracious living areas with designer two-tone paint, crown molding, large walk-in closets and in home full size washer and dryer. Enjoy the many fine conveniences offered, such as an expansive fitness center, executive business center and refreshing pool. Villages of the Galleria is the perfect place to call home. ## Features ## - Contemporary Recessed Lighting ## - Built-In Linen Closet in Bathroom * ## - Entertainment Style Kitchens ## - Private Balconies and Patios ## - Crown Molding Accents ## - Six-Panel Interior Doors ## - Custom Maple-Front Cabinetry ## - Nine-Foot Ceilings ## - Microwave ## - Pre-Wired for High Speed Internet ## - Private Garages * ## - Full Size Washer/Dryer ## - Pantry * ## - Oval Roman Soaking Tub * ## - Cozy Gas Fireplace with Mantel * ## - Covered Parking * ## - Spacious Walk-In Closet(s) * ## - Energy-Saving Multi-Cycle Dishwasher ## Community Amenities ## - Community Garden ## - Executive Business Center ## - Sand Volleyball ## - Close to Shopping ## - Beautiful Landscaped Court Yards ## - Playground ## - Fitness Center ## - Clubhouse ## - Open Air Cabanas ## - Easy Access to Freeways ## - Pool and Spa ## - Gated Community ## - Professional Onsite Management w/ 24-Hour Emergency Maintenance ## - Picnic Area with Barbecue ## Office Hours ## Monday - Friday 9:00 AM - 6:00 PM ## Saturday 10:00 AM - 5:00 PM ## Sunday 12:00 PM - 5:00 PM ## Pet Policy ## Maximum of 2 pets cats or dogs. No weight limit. Additional $25 rent per month and additional $500 deposit per pet. Inquire about our breed restrictions. ## Equal Housing Opportunity ## VJWLzl1wXG Based on the text, this apartment is 819 square feet, not 8190 square feet. So we can reassign the value: cl$sqft[1261] = 819 If other features don’t help with correction, try getting information from external sources. If you can’t correct the outlier but know it’s invalid, replace it with a missing value NA. "],["data-forensics-and-cleaning-unstructured-data.html", "15 Data Forensics and Cleaning: Unstructured Data 15.1 Preliminaries 15.2 From File Names to Metadata 15.3 Loading a Corpus 15.4 Preprocessing 15.5 Counting Terms 15.6 Text Mining Pipepline 15.7 Document Term Matrix 15.8 Corpus Analytics", " 15 Data Forensics and Cleaning: Unstructured Data 15.1 Preliminaries Lesson Objectives By the end of this lesson, you should: Be able to identify patterns in unstructured data Create metadata about a collection of documents Load and clean a collection of text files into R, which entails: Tokenizing words Determining and applying stop words Normalizing, lemmatizing, and stemming texts Creating a document-term matrix Getting high-level data about text documents (term frequencies, tf–idf scores) Understand how preprocessing steps impact analysis Packages install.packages(c(&quot;tidyverse&quot;, &quot;tokenizers&quot;, &quot;tm&quot;, &quot;cluster&quot;)) 15.2 From File Names to Metadata First, let’s get some information about a collection of files. input_dir &lt;- &quot;./IST8_text_corpus/&quot; fnames &lt;- list.files(input_dir) While we could start analyzing these files immediately, their names contain a lot of metadata, which could be helpful. We’ll need to structure this info first (yes, we’re structuring unstructured data so we can structure more unstructured data—welcome to data forensics!). Mercifully, whoever created these files had a convention in mind for giving them names. We can latch onto the patterns within this convention to make our own representation of the files’ metadata. Here’s the pattern: [LANGUAGE]_[YEAR]_[LASTNAME,FIRSTNAME]_[N OR G].txt Let’s use it to make a data frame. Using stringr in combination with regex patterns will be essential to do so. First, let’s break apart the strings on their underscores and transform that output into a data frame. library(stringr) C19_novels &lt;- str_split_fixed(fnames, &quot;_&quot;, 5) C19_novels &lt;- as.data.frame(C19_novels) This is already pretty close to a good data sheet for us, but we’ll want to refine it a little further. First, let’s name our columns. The letters in the file names are genre tags, which stand for either “gothic” or “not gothic,” so we’ll be sure to record them. colnames(C19_novels) &lt;- c(&quot;lang&quot;, &quot;year&quot;, &quot;author_name&quot;, &quot;title&quot;, &quot;genre&quot;) Now, let’s split author names into “first” and “last” and add them back to the data frame. author_names &lt;- str_split_fixed(C19_novels$author_name, &quot;,&quot;, 2) C19_novels$last_name &lt;- author_names[, 1] C19_novels$first_name &lt;- author_names[, 2] And for good measure, let’s remove the .txt extension in the genre tags and convert those tags to factors. C19_novels$genre &lt;- sapply(C19_novels$genre, function(x) str_remove_all(x, &quot;.txt&quot;)) C19_novels$genre &lt;- as.factor(C19_novels$genre) Finally, we’ll clean up, removing the author_names and lang columns and doing a bit of reordering. (Language could be useful in some instances, but we don’t need it for now, especially because these novels are all in English.) C19_novels &lt;- subset(C19_novels, select= -c(lang, author_name)) C19_novels &lt;- C19_novels[, c(4,5,2,1,3)] C19_novels ## last_name first_name title year genre ## 1 Beckford William Vathek 1786 G ## 2 Radcliffe Ann ASicilianRomance 1790 G ## 3 Radcliffe Ann TheMysteriesofUdolpho 1794 G ## 4 Lewis Matthew TheMonk 1795 G ## 5 Austen Jane SenseandSensibility 1811 N ## 6 Shelley Mary Frankenstein 1818 G ## 7 Scott Walter Ivanhoe 1820 N ## 8 Poe EdgarAllen TheNarrativeofArthurGordonPym 1838 N ## 9 Bronte Emily WutheringHeights 1847 G ## 10 Hawthorne Nathaniel TheHouseoftheSevenGables 1851 N ## 11 Gaskell Elizabeth NorthandSouth 1854 N ## 12 Collins Wilkie TheWomaninWhite 1860 N ## 13 Dickens Charles GreatExpectations 1861 N ## 14 James Henry PortraitofaLady 1881 N ## 15 Stevenson RobertLouis TreasureIsland 1882 N ## 16 Stevenson RobertLouis JekyllandHyde 1886 G ## 17 Wilde Oscar ThePictureofDorianGray 1890 G ## 18 Stoker Bram Dracula 1897 G Nice and tidy! 15.3 Loading a Corpus With our metadata structured, it’s time to load our files. files &lt;- lapply(paste0(input_dir, fnames), readLines) Loading our files like this will create a giant list of vectors, where each vector is a full text file. Those vectors are chunked by paragraph right now, but for our purposes it would be easier if each vector was a single stream of text (like the output of ocr(), if you’ll remember). We can collapse them together with paste(). files &lt;- lapply(files, function(x) paste(x, collapse=&quot; &quot;)) From here, we can wrap these files in a special “corpus” object, which the tm package enables (a corpus is a large collection of texts). A tm corpus works somewhat like a database. It has a section for “content”, which contains text data, as well as various metadata sections, which we can populate with additional information about our texts, if we wished. Taken together, these features make it easy to streamline workflows with text data. To make a corpus with tm, we call the Corpus() function, specifying with VectorSource() (because our texts are vectors): library(tm) corpus &lt;- Corpus(VectorSource(files)) Here’s a high-level glimpse at what’s in this object: corpus ## &lt;&lt;SimpleCorpus&gt;&gt; ## Metadata: corpus specific: 1, document level (indexed): 0 ## Content: documents: 18 Zooming in to metadata about a text in the corpus: corpus[[6]] ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 424017 Not much here so far, but we’ll add more later. Finally, we can get content from a text: str_sub(corpus[[6]]$content, 1, 500) ## [1] &quot;FRANKENSTEIN: OR, THE MODERN PROMETHEUS. BY MARY W. SHELLEY. PREFACE. The event on which this fiction is founded, has been supposed, by Dr. Darwin, and some of the physiological writers of Germany, as not of impossible occurrence. I shall not be supposed as according the remotest degree of serious faith to such an imagination; yet, in assuming it as the basis of a work of fancy, I have not considered myself as merely weaving a series of supernatural terrors. The event on which the interest &quot; In this last view, you can see that the text file is still formatted (at least we didn’t have to OCR it!). This formatting is unwieldy and worse, it makes it so we can’t really access the elements that comprise each novel. We’ll need to do more work to preprocess our texts before we can analyze them. 15.4 Preprocessing Part of preprocessing entails making decisions about the kinds of information we want to know about our data. Knowing what information we want often guides the way we structure data. Put another way: research questions drive preprocessing. 15.4.1 Tokenizing and Bags of Words For example, it’d be helpful to know how many words are in each novel, which might enable us to study patterns and differences between authors’ styles. To get word counts, we need to split the text vectors into individual words. One way to do this would be to first strip out everything in each novel that isn’t an alphabetic character or a space. Let’s grab one text to experiment with. frankenstein &lt;- corpus[[6]]$content frankenstein &lt;- str_replace_all(frankenstein, &quot;[^A-Za-z]&quot;, &quot; &quot;) From here, it would be easy enough to count the words in a novel by splitting its vector on spaces, removing empty elements in the vector, and calling length() on the vector. The end result is what we call a bag of words. frankenstein &lt;- str_split(frankenstein, &quot; &quot;) frankenstein &lt;- lapply(frankenstein, function(x) x[x != &quot;&quot;]) length(frankenstein[[1]]) ## [1] 76015 And here are the first nine words (or “tokens”): frankenstein[[1]][1:9] ## [1] &quot;FRANKENSTEIN&quot; &quot;OR&quot; &quot;THE&quot; &quot;MODERN&quot; &quot;PROMETHEUS&quot; ## [6] &quot;BY&quot; &quot;MARY&quot; &quot;W&quot; &quot;SHELLEY&quot; 15.4.2 Text Normalization While easy, producing our bag of words this way is a bit clunky. And further, this process can’t handle contractions (“I’m”, “don’t”, “that’s”) or differences in capitalization. frankenstein[[1]][188:191] ## [1] &quot;Midsummer&quot; &quot;Night&quot; &quot;s&quot; &quot;Dream&quot; Should be: Midsummer Night&#39;s Dream And &quot;FRANKENSTEIN&quot;, &quot;Frankenstein&quot; Should be: &quot;Frankenstein&quot; Or, even better: frankenstein Typically, when we work with text data we want all of our words to be in the same case because this makes it easier to do things like counting operations. Remember that, to a computer, “Word” and “word” are two separate words, and if we want to count them together, we need to pick one version or the other. Making all words lowercase (even proper nouns) is the standard. Doing this is part of what’s called text normalization. (Other forms of normalization might entail handling orthographic differences between British and American English, like “color” and “colour”.) As for contractions, we have some decisions to make. On the one hand, it’s important to retain as much information as we can about the original text, so keeping “don’t” or “what’s” (which would be “don t” and “what s” in our current method) is important. One way corpus linguists handle these words is to lemmatize them. Lemmatizing involves removing inflectional endings to return words to their base form: car, cars, car’s, cars’ =&gt; car don’t =&gt; do This is a helpful step if what we’re primarily interested in is doing a high- level analysis of semantics. On the other hand, though, many words that feature contractions are high-frequency function words, which don’t have much meaning beyond the immediate context of a sentence or two. Words like “that’s” or “won’t” appear in huge numbers in text data, but they don’t carry much information in and of themselves—it may in fact be the case that we could get rid of them entirely… 15.4.3 Stop Words …and indeed this is the case! When structuring text data to study it at scale, it’s common to remove, or stop out, words that don’t have much meaning. This makes it much easier to identify significant (i.e. unique) features in a text, without having to swim through all the noise of “the” or “that,” which would almost always show up as the highest-occurring words in an analysis. But what words should we remove? Ultimately, this depends on your text data. We can usually assume that function words will be on our list of stop words, but it may be that you’ll have to add or subtract others depending on your data and, of course, your research question. The tm package has a good starting list. Let’s look at the first 100 words. head(stopwords(&quot;SMART&quot;), 100) ## [1] &quot;a&quot; &quot;a&#39;s&quot; &quot;able&quot; &quot;about&quot; &quot;above&quot; ## [6] &quot;according&quot; &quot;accordingly&quot; &quot;across&quot; &quot;actually&quot; &quot;after&quot; ## [11] &quot;afterwards&quot; &quot;again&quot; &quot;against&quot; &quot;ain&#39;t&quot; &quot;all&quot; ## [16] &quot;allow&quot; &quot;allows&quot; &quot;almost&quot; &quot;alone&quot; &quot;along&quot; ## [21] &quot;already&quot; &quot;also&quot; &quot;although&quot; &quot;always&quot; &quot;am&quot; ## [26] &quot;among&quot; &quot;amongst&quot; &quot;an&quot; &quot;and&quot; &quot;another&quot; ## [31] &quot;any&quot; &quot;anybody&quot; &quot;anyhow&quot; &quot;anyone&quot; &quot;anything&quot; ## [36] &quot;anyway&quot; &quot;anyways&quot; &quot;anywhere&quot; &quot;apart&quot; &quot;appear&quot; ## [41] &quot;appreciate&quot; &quot;appropriate&quot; &quot;are&quot; &quot;aren&#39;t&quot; &quot;around&quot; ## [46] &quot;as&quot; &quot;aside&quot; &quot;ask&quot; &quot;asking&quot; &quot;associated&quot; ## [51] &quot;at&quot; &quot;available&quot; &quot;away&quot; &quot;awfully&quot; &quot;b&quot; ## [56] &quot;be&quot; &quot;became&quot; &quot;because&quot; &quot;become&quot; &quot;becomes&quot; ## [61] &quot;becoming&quot; &quot;been&quot; &quot;before&quot; &quot;beforehand&quot; &quot;behind&quot; ## [66] &quot;being&quot; &quot;believe&quot; &quot;below&quot; &quot;beside&quot; &quot;besides&quot; ## [71] &quot;best&quot; &quot;better&quot; &quot;between&quot; &quot;beyond&quot; &quot;both&quot; ## [76] &quot;brief&quot; &quot;but&quot; &quot;by&quot; &quot;c&quot; &quot;c&#39;mon&quot; ## [81] &quot;c&#39;s&quot; &quot;came&quot; &quot;can&quot; &quot;can&#39;t&quot; &quot;cannot&quot; ## [86] &quot;cant&quot; &quot;cause&quot; &quot;causes&quot; &quot;certain&quot; &quot;certainly&quot; ## [91] &quot;changes&quot; &quot;clearly&quot; &quot;co&quot; &quot;com&quot; &quot;come&quot; ## [96] &quot;comes&quot; &quot;concerning&quot; &quot;consequently&quot; &quot;consider&quot; &quot;considering&quot; That looks pretty comprehensive so far, though the only way we’ll know whether it’s a good match for our corpus is to process our corpus with it. At first glance, the extra random letters in this list seem like they could be a big help, on the off chance there’s some noise from OCR. If you look at the first novel in the corpus, for example, there are a bunch of stray p’s, which is likely from a pattern for marking pages (“p. 7”): cat(str_sub(corpus[[1]]$content, 1, 1000)) ## VATHEK; AN ARABIAN TALE, BY WILLIAM BECKFORD, ESQ. p. 7VATHEK. Vathek, ninth Caliph [7a] of the race of the Abassides, was the son of Motassem, and the grandson of Haroun Al Raschid. From an early accession to the throne, and the talents he possessed to adorn it, his subjects were induced to expect that his reign would be long and happy. His figure was pleasing and majestic; but when he was angry, one of his eyes became so terrible [7b] that no person could bear to behold it; and the wretch upon whom it was fixed instantly fell backward, and sometimes expired. For fear, however, of depopulating his dominions, and making his palace desolate, he but rarely gave way to his anger. Being much addicted to women, and the pleasures of the table, he sought by his affability to procure agreeable companions; and he succeeded the better, p. 8as his generosity was unbounded and his indulgences unrestrained; for he was by no means scrupulous: nor did he think, with the Caliph Omar Ben A Our stop word list would take care of this. With it, we could return to our original collection of novels, split them on spaces as before, and filter out everything that’s stored in our stop_list variable. Before we did the filtering, though, we’d need to transform the novels into lowercase (which can be done with R’s tolower() function). 15.4.4 Tokenizers This whole process is ultimately straightforward so far, but it would be nice to collapse all its steps. Luckily, there are packages we can use to streamline our process. tokenizers has functions that split a text vector, turn words into lowercase forms, and remove stop words, all in a few lines of code. Further, we can combine these functions with a special tm_map() function in the tm package, which will globally apply our changes. library(tokenizers) cleaned_corpus &lt;- tm_map(corpus, function(x) tokenize_words(x, stopwords=stopwords(&quot;SMART&quot;), lowercase=TRUE, strip_punct=TRUE, strip_numeric=TRUE)) You may see a “transformation drops documents” warning after this. You can disregard it. It has to do with the way tm references text changes against a corpus’s metadata, which we’ve left blank. We can compare our tokenized output with the text data we had been working with earlier: list(untokenized=frankenstein[[1]][1:9], tokenized=cleaned_corpus[[6]]$content[1:5]) ## $untokenized ## [1] &quot;FRANKENSTEIN&quot; &quot;OR&quot; &quot;THE&quot; &quot;MODERN&quot; &quot;PROMETHEUS&quot; ## [6] &quot;BY&quot; &quot;MARY&quot; &quot;W&quot; &quot;SHELLEY&quot; ## ## $tokenized ## [1] &quot;frankenstein&quot; &quot;modern&quot; &quot;prometheus&quot; &quot;mary&quot; &quot;shelley&quot; From the title alone we can see how much of a difference tokenizing with stop words makes. And while we lose a bit of information by doing this, what we can is a much clearer picture of key words we’d want to further analyze. 15.4.5 Document Chunking and N-grams Finally, it’s possible to change the way we separate out our text data. Instead of tokenizing on words, we could use tokenizers to break apart our texts on paragraphs (tokenize_paragraphs()), sentences (tokenize_sentences), and more. There might be valuable information to be learned about the average sentence length of a novel, for example, so we might chunk it accordingly. We might also want to see whether a text contains repeated phrases, or if two or three words often occur in the same sequence. We could investigate this by adjusting the window around which we tokenize individual words. So far we’ve used the “unigram,” or a single word, as our basic unit of counting, but we could break our texts into “bigrams” (two word phrases), “trigrams” (three word phrases), or, well any sequence of n units. Generally, you’ll see these sequences referred to as n-grams: frankenstein_bigrams &lt;- tokenize_ngrams(corpus[[6]]$content, n=2, stopwords=stopwords(&quot;SMART&quot;)) Here, n=2 sets the n-gram window at two: frankenstein_bigrams[[1]][1:20] ## [1] &quot;frankenstein modern&quot; &quot;modern prometheus&quot; &quot;prometheus mary&quot; ## [4] &quot;mary shelley&quot; &quot;shelley preface&quot; &quot;preface event&quot; ## [7] &quot;event fiction&quot; &quot;fiction founded&quot; &quot;founded supposed&quot; ## [10] &quot;supposed dr&quot; &quot;dr darwin&quot; &quot;darwin physiological&quot; ## [13] &quot;physiological writers&quot; &quot;writers germany&quot; &quot;germany impossible&quot; ## [16] &quot;impossible occurrence&quot; &quot;occurrence supposed&quot; &quot;supposed remotest&quot; ## [19] &quot;remotest degree&quot; &quot;degree faith&quot; Note though that, for this function, we’d need to do some preprocessing on our own to remove numeric characters and punctuation; tokenize_ngrams() won’t do it for us. 15.5 Counting Terms Let’s return to our single word counts. Now that we’ve transformed our novels into bags of single words, we can start with some analysis. Simply counting the number of times a word appears in some data can tell us a lot about a text. The following steps should feel familiar: we did them with OCR. Let’s look at Wuthering Heights, which is our ninth text: library(tidyverse) wuthering_heights &lt;- table(cleaned_corpus[[9]]$content) wuthering_heights &lt;- data.frame(word=names(wuthering_heights), count=as.numeric(wuthering_heights)) wuthering_heights &lt;- arrange(wuthering_heights, desc(count)) head(wuthering_heights, 30) ## word count ## 1 heathcliff 422 ## 2 linton 348 ## 3 catherine 339 ## 4 mr 312 ## 5 master 185 ## 6 hareton 169 ## 7 answered 156 ## 8 till 151 ## 9 house 144 ## 10 door 133 ## 11 mrs 133 ## 12 joseph 130 ## 13 miss 129 ## 14 time 127 ## 15 back 121 ## 16 thought 118 ## 17 cathy 117 ## 18 good 117 ## 19 replied 117 ## 20 earnshaw 116 ## 21 eyes 116 ## 22 cried 114 ## 23 young 107 ## 24 day 106 ## 25 father 106 ## 26 asked 105 ## 27 make 105 ## 28 edgar 104 ## 29 night 104 ## 30 made 102 Looks good! The two main characters in this novel are named Heathcliff and Catherine, so it makes sense that these words would appear a lot. You can see, however, that we might want to fine tune our stop word list so that it removes “mr” and “mrs” from the text. Though again, it depends on our research question. If we’re exploring gender roles in nineteenth-century literature, we’d probably keep those words in. In addition to fine tuning stop words, pausing here at these counts would be a good way to check whether some other form of textual noise is present in your data, which you haven’t yet caught. There’s nothing like that here, but you might imagine how consistent OCR noise could make itself known in this view. 15.5.1 Term Frequency After you’ve done your fine tuning, it would be good to get a term frequency number for each word in this data frame. Raw counts are nice, but expressing those counts in proportion to the total words in a document will tell us more information about a word’s contribution to the document as a whole. We can get term frequencies for our words by dividing a word’s count by document length (which is the sum of all words in the document). wuthering_heights$term_frequency &lt;- sapply(wuthering_heights$count, function(x) (x/sum(wuthering_heights$count))) head(wuthering_heights, 30) ## word count term_frequency ## 1 heathcliff 422 0.009619549 ## 2 linton 348 0.007932709 ## 3 catherine 339 0.007727552 ## 4 mr 312 0.007112084 ## 5 master 185 0.004217101 ## 6 hareton 169 0.003852379 ## 7 answered 156 0.003556042 ## 8 till 151 0.003442066 ## 9 house 144 0.003282500 ## 10 door 133 0.003031754 ## 11 mrs 133 0.003031754 ## 12 joseph 130 0.002963368 ## 13 miss 129 0.002940573 ## 14 time 127 0.002894983 ## 15 back 121 0.002758212 ## 16 thought 118 0.002689827 ## 17 cathy 117 0.002667031 ## 18 good 117 0.002667031 ## 19 replied 117 0.002667031 ## 20 earnshaw 116 0.002644236 ## 21 eyes 116 0.002644236 ## 22 cried 114 0.002598646 ## 23 young 107 0.002439080 ## 24 day 106 0.002416285 ## 25 father 106 0.002416285 ## 26 asked 105 0.002393490 ## 27 make 105 0.002393490 ## 28 edgar 104 0.002370695 ## 29 night 104 0.002370695 ## 30 made 102 0.002325104 15.5.2 Plotting Term Frequency Let’s plot the top 50 words in Wuthering Heights. We’ll call fct_reorder() in the aes() field of ggplot to sort words in the descending order of their term frequency. library(ggplot2) ggplot(data=wuthering_heights[1:50, ], aes(x=fct_reorder(word, -term_frequency), y=term_frequency)) + geom_bar(stat=&quot;identity&quot;) + theme(axis.text.x=element_text(angle=90, vjust=0.5, hjust=1)) + labs(title=&quot;Top 50 words in Wuthering Heights&quot;, x=&quot;Word&quot;, y=&quot;Term Frequency&quot;) This is a good start for creating a high-level view of the novel, but further tuning might be in order. We’ve already mentioned “mrs” and “mr” as two words that we could cut out of the text. Another option would be to collapse these two words together into a base form by stemming them. Though this would overweight their base form (which in this case is “mr”) in terms of term frequency, it would also free up space to see other terms in the document. Other examples of stemming words would be transforming “fishing”, “fished”, and “fisher” all into “fish.” That said, like all preprocessing, lemmatizing words is an interpretive decision, which comes with its own consequences. Maybe it’s okay to transform “mr” and “mrs” into “mr” for some analyses, but it’s also the case that we’d be erasing potentially important gender differences in the text—and would do so by overweighting the masculine form of the word. Regardless of what you decide, it’s important to keep track of these decisions as you make them because they will impact the kinds of claims you make about your data later on. 15.5.3 Comparing Term Frequencies Across Documents Term frequency is helpful if we want to start comparing words across two texts. We can make some comparisons by transforming the above code into a function: term_table &lt;- function(text) { term_tab &lt;- table(text) term_tab &lt;- data.frame(word=names(term_tab), count=as.numeric(term_tab)) term_tab$term_frequency &lt;- sapply(term_tab$count, function(x) (x/sum(term_tab$count))) term_tab &lt;- arrange(term_tab, desc(count)) return(term_tab) } We already have a term table for Wuthering Heights. Let’s make one for Dracula. dracula &lt;- term_table(cleaned_corpus[[18]]$content) head(dracula, 30) ## word count term_frequency ## 1 time 387 0.007280458 ## 2 van 321 0.006038829 ## 3 helsing 299 0.005624953 ## 4 back 261 0.004910076 ## 5 room 231 0.004345699 ## 6 good 225 0.004232824 ## 7 lucy 225 0.004232824 ## 8 man 224 0.004214012 ## 9 dear 219 0.004119949 ## 10 mina 217 0.004082324 ## 11 night 217 0.004082324 ## 12 hand 209 0.003931823 ## 13 face 205 0.003856573 ## 14 door 201 0.003781323 ## 15 made 193 0.003630822 ## 16 poor 192 0.003612010 ## 17 sleep 190 0.003574385 ## 18 eyes 186 0.003499135 ## 19 looked 185 0.003480322 ## 20 friend 183 0.003442697 ## 21 great 182 0.003423884 ## 22 jonathan 182 0.003423884 ## 23 dr 178 0.003348634 ## 24 things 174 0.003273384 ## 25 make 163 0.003066446 ## 26 day 160 0.003010008 ## 27 professor 155 0.002915946 ## 28 count 153 0.002878320 ## 29 found 153 0.002878320 ## 30 thought 153 0.002878320 Now we can compare the relative frequency of a word across two novels: comparison_words &lt;- c(&quot;dark&quot;, &quot;night&quot;, &quot;ominous&quot;) for (i in comparison_words) { wh &lt;- list(wh=subset(wuthering_heights, word==i)) drac &lt;- list(drac=subset(dracula, word==i)) print(wh) print(drac) } ## $wh ## word count term_frequency ## 183 dark 32 0.0007294445 ## ## $drac ## word count term_frequency ## 90 dark 77 0.001448566 ## ## $wh ## word count term_frequency ## 29 night 104 0.002370695 ## ## $drac ## word count term_frequency ## 11 night 217 0.004082324 ## ## $wh ## word count term_frequency ## 7283 ominous 1 2.279514e-05 ## ## $drac ## word count term_frequency ## 7217 ominous 1 1.881255e-05 Not bad! We might be able to make a few generalizations from this, but to say anything definitively, we’ll need to scale our method. Doing so wouldn’t be easy with this setup as it stands now. While it’s true that we could write some functions to roll through these two data frames and systematically compare the words in each, it would take a lot of work to do so. Luckily, the tm package (which we’ve used to make our stop word list) features generalized functions for just this kind of thing. 15.6 Text Mining Pipepline Before going further, we should note that tm has its own functions for preprocessing texts. To send raw files directly through those functions, you’d call tm_map() in conjunction with these functions. You can think of tm_map() as a cognate to the apply() family. corpus_2 &lt;- Corpus(VectorSource(files)) corpus_2 &lt;- tm_map(corpus_2, removeNumbers) corpus_2 &lt;- tm_map(corpus_2, removeWords, stopwords(&quot;SMART&quot;)) corpus_2 &lt;- tm_map(corpus_2, removePunctuation) corpus_2 &lt;- tm_map(corpus_2, stripWhitespace) Note the order of operations here: because our stop words list takes into account punctuated words, like “don’t” or “i’m”, we want to remove stop words before removing punctuation. If we didn’t do this, removeWords() wouldn’t catch the un-punctuated “dont” or “im”. This won’t always be the case, since we can use different stop word lists, which may have a different set of terms, but in this instance, the order in which we preprocess matters. Preparing your text files like this would be fine, and indeed sometimes it’s preferable to sequentially step through each part of the preprocessing workflow. That said, tokenizers manages the order of operations above on its own and its preprocessing functions are generally a bit faster to run (in particular, removeWords() is quite slow in comparison to tokenize_words()). There is, however, one caveat to using tokenizers. It splits documents up to do text cleaning, but other functions in tm require non-split documents. If we use tokenizers, then, we need to do a quick workaround with paste(). cleaned_corpus &lt;- lapply(cleaned_corpus, function(x) paste(x, collapse=&quot; &quot;)) And then reformat that output as a corpus object: cleaned_corpus &lt;- Corpus(VectorSource(cleaned_corpus)) Ultimately, it’s up to you to decide what workflow makes sense. Personally, I (Tyler) like to do exploratory preprocessing steps with tokenizers, often with a sample set of all the documents. Then, once I’ve settled on my stop word list and so forth, I reprocess all my files with the tm-specific functions above. Regardless of what workflow you choose, preprocessing can take a while, so now would be a good place to save your data. That way, you can retrieve your corpus later on. saveRDS(cleaned_corpus, &quot;./data/C19_novels_cleaned.rds&quot;) Loading it back in is straightforward: cleaned_corpus &lt;- readRDS(&quot;./data/C19_novels_cleaned.rds&quot;) 15.7 Document Term Matrix The advantage of using a tm corpus is that it makes comparing data easier. Remember that, in our old workflow, looking at the respective term frequencies in two documents entailed a fair bit of code. And further, we left off before generalizing that code to the corpus as a whole. But what if we wanted to look at a term across multiple documents? To do so, we need to create what’s called a document-term matrix, or DTM. A DTM describes the frequency of terms across an entire corpus (rather than just one document). Rows of the matrix correspond to documents, while columns correspond to the terms. For a given document, we count the number of times that term appears and enter that number in the column in question. We do this even if the count is 0; key to the way a DTM works is that it’s a corpus-wide representation of text data, so it matters if a text does or doesn’t contain a term. Here’s a simple example with three documents: Document 1: “I like cats” Document 2: “I like dogs” Document 3: “I like both cats and dogs” Transforming these into a document-term matrix would yield: n_doc I like both cats and dogs 1 1 1 0 1 0 0 2 1 1 0 0 0 1 3 1 1 1 1 1 1 Representing texts in this way is incredibly useful because it enables us to easily discern similarities and differences in our corpus. For example, we can see that each of the above documents contain the words “I” and “like.” Given that, if we wanted to know what makes documents unique, we can ignore those two words and focus on the rest of the values. Now, imagine doing this for thousands of words! What patterns might emerge? Let’s try it on our corpus. We can transform a tm corpus object into a DTM by calling DocumentTermMatrix(). (Note: this is one of the functions in tm that requires non-split documents, so before you call it make sure you know how you’ve preprocessed your texts!) dtm &lt;- DocumentTermMatrix(cleaned_corpus) This object is quite similar to the one that results from Corpus(): it contains a fair bit of metadata, as well as an all-important “dimnames” field, which records the documents in the matrix and the entire term vocabulary. We access all of this information with the same syntax we use for data frames. Let’s look around a bit and get some high-level info. 15.8 Corpus Analytics Number of columns in the DTM (i.e. the vocabulary size): dtm$ncol ## [1] 34925 Number of rows in the DTM (i.e. the number of documents this matrix represents): dtm$nrow ## [1] 18 Right now, the document names are just a numbers in a vector: dtm$dimnames$Docs ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; &quot;14&quot; &quot;15&quot; ## [16] &quot;16&quot; &quot;17&quot; &quot;18&quot; But they’re ordered according to the sequence in which the corpus was originally created. This means we can use our metadata from way back when to associate a document with its title: dtm$dimnames$Docs &lt;- C19_novels$title dtm$dimnames$Docs ## [1] &quot;Vathek&quot; &quot;ASicilianRomance&quot; ## [3] &quot;TheMysteriesofUdolpho&quot; &quot;TheMonk&quot; ## [5] &quot;SenseandSensibility&quot; &quot;Frankenstein&quot; ## [7] &quot;Ivanhoe&quot; &quot;TheNarrativeofArthurGordonPym&quot; ## [9] &quot;WutheringHeights&quot; &quot;TheHouseoftheSevenGables&quot; ## [11] &quot;NorthandSouth&quot; &quot;TheWomaninWhite&quot; ## [13] &quot;GreatExpectations&quot; &quot;PortraitofaLady&quot; ## [15] &quot;TreasureIsland&quot; &quot;JekyllandHyde&quot; ## [17] &quot;ThePictureofDorianGray&quot; &quot;Dracula&quot; With this information associated, we can use inspect() to get a high-level view of the corpus. inspect(dtm) ## &lt;&lt;DocumentTermMatrix (documents: 18, terms: 34925)&gt;&gt; ## Non-/sparse entries: 145233/483417 ## Sparsity : 77% ## Maximal term length: 19 ## Weighting : term frequency (tf) ## Sample : ## Terms ## Docs back day eyes good great long made man thought time ## Dracula 261 160 186 225 182 147 193 224 153 387 ## GreatExpectations 244 216 180 256 198 173 300 307 238 373 ## Ivanhoe 77 138 100 298 111 154 151 235 46 182 ## NorthandSouth 184 257 197 316 179 211 234 270 332 423 ## PortraitofaLady 210 241 226 520 421 187 381 317 302 339 ## TheHouseoftheSevenGables 79 113 72 100 144 153 144 211 60 113 ## TheMonk 81 106 184 80 66 108 167 95 72 162 ## TheMysteriesofUdolpho 117 167 225 186 164 359 316 213 341 367 ## TheWomaninWhite 417 351 233 235 112 188 244 443 183 706 ## WutheringHeights 121 106 116 117 63 97 102 88 118 127 Of special note here is sparsity. Sparsity measures the amount of 0s in the data. This happens when a document does not contain a term that appears elsewhere in the corpus. In our case, of the 628,650 entries in this matrix, 80% of them are 0. Such is the way of working with DTMs: they’re big, expansive data structures that have a lot of empty space. We can zoom in and filter on term counts with findFreqTerms(). Here are terms that appear more than 1,000 times in the corpus: findFreqTerms(dtm, 1000) ## [1] &quot;answered&quot; &quot;appeared&quot; &quot;asked&quot; &quot;back&quot; &quot;day&quot; &quot;dear&quot; ## [7] &quot;death&quot; &quot;door&quot; &quot;eyes&quot; &quot;face&quot; &quot;father&quot; &quot;felt&quot; ## [13] &quot;found&quot; &quot;friend&quot; &quot;gave&quot; &quot;give&quot; &quot;good&quot; &quot;great&quot; ## [19] &quot;half&quot; &quot;hand&quot; &quot;hands&quot; &quot;head&quot; &quot;hear&quot; &quot;heard&quot; ## [25] &quot;heart&quot; &quot;hope&quot; &quot;kind&quot; &quot;knew&quot; &quot;lady&quot; &quot;leave&quot; ## [31] &quot;left&quot; &quot;life&quot; &quot;light&quot; &quot;long&quot; &quot;looked&quot; &quot;love&quot; ## [37] &quot;made&quot; &quot;make&quot; &quot;man&quot; &quot;men&quot; &quot;mind&quot; &quot;moment&quot; ## [43] &quot;morning&quot; &quot;mother&quot; &quot;night&quot; &quot;part&quot; &quot;passed&quot; &quot;people&quot; ## [49] &quot;person&quot; &quot;place&quot; &quot;poor&quot; &quot;present&quot; &quot;put&quot; &quot;replied&quot; ## [55] &quot;returned&quot; &quot;round&quot; &quot;side&quot; &quot;speak&quot; &quot;stood&quot; &quot;thing&quot; ## [61] &quot;thou&quot; &quot;thought&quot; &quot;till&quot; &quot;time&quot; &quot;told&quot; &quot;turned&quot; ## [67] &quot;voice&quot; &quot;woman&quot; &quot;words&quot; &quot;world&quot; &quot;young&quot; &quot;count&quot; ## [73] &quot;house&quot; &quot;madame&quot; &quot;room&quot; &quot;sir&quot; &quot;emily&quot; &quot;margaret&quot; ## [79] &quot;miss&quot; &quot;mrs&quot; &quot;isabel&quot; Using findAssocs(), we can also track which words rise and fall in usage alongside a given word. (The number in the third argument position of this function is a cutoff for the strength of a correlation.) Here’s “boat”: findAssocs(dtm, &quot;boat&quot;, .85) ## $boat ## thumping scoundrels midday direction ## 0.94 0.88 0.87 0.85 Here’s “writing” (there are a lot of terms, so we’ll limit to 15): writing &lt;- findAssocs(dtm, &quot;writing&quot;, .85) writing[[1]][1:15] ## letter copy disposal inquiries bedrooms hindrance ## 0.99 0.97 0.97 0.97 0.97 0.97 ## messages certificate distrust plainly drawings anonymous ## 0.97 0.97 0.96 0.96 0.96 0.96 ## ladyship plantation lodgings ## 0.96 0.96 0.96 15.8.1 Corpus Term Counts From here, it would be useful to get a full count of all the terms in the corpus. We can transform the DTM into a matrix and then a data frame. term_counts &lt;- as.matrix(dtm) term_counts &lt;- data.frame(sort(colSums(term_counts), decreasing=TRUE)) term_counts &lt;- cbind(newColName=rownames(term_counts), term_counts) colnames(term_counts) &lt;- c(&quot;term&quot;, &quot;count&quot;) As before, let’s plot the top 50 terms in these counts, but this time, they will cover the entire corpus: ggplot(data=term_counts[1:50, ], aes(x=fct_reorder(term, -count), y=count)) + geom_bar(stat=&quot;identity&quot;) + theme(axis.text.x=element_text(angle=90, vjust=0.5, hjust=1)) + labs(title=&quot;Top 50 words in 18 Nineteenth-Century Novels&quot;, x=&quot;Word&quot;, y=&quot;Count&quot;) This looks good, though the words here are all pretty common. In fact, many of them are simply the most common words in the English language. “Time” is the 64th-most frequent word in English; “make” is the 50th. As it stands, then, this graph doesn’t tell us very much about the specificity of our particular collection of texts; if we ran the same process on English novels from the twentieth century, we’d probably produce very similar output. 15.8.2 tf–idf Scores Given this, if we want to know what makes our corpus special, we need a measure of uniqueness for the terms it contains. One of the most common ways to do this is to get what’s called a tf–idf score (short for “term frequency—inverse document frequency”) for each term in our corpus. tf–idf is a weighting method. It increases proportionally to the number of times a word appears in a document but is importantly offset by the number of documents in the corpus that contain this term. This offset adjusts for common words across a corpus, pushing their scores down while boosting the scores of rarer terms in the corpus. Inverse document frequency can be expressed as: \\[\\begin{align*} idf_i = log(\\frac{n}{df_i}) \\end{align*}\\] Where \\(idf_i\\) is the idf score for term \\(i\\), \\(df_i\\) is the number of documents that contain \\(i\\), and \\(n\\) is the total number of documents. A tf-idf score can be calculated by the following: \\[\\begin{align*} w_i,_j = tf_i,_j \\times idf_i \\end{align*}\\] Where \\(w_i,_j\\) is the tf–idf score of term \\(i\\) in document \\(j\\), \\(tf_i,_j\\) is the term frequency for \\(i\\) in \\(j\\), and \\(idf_i\\) is the inverse document score. While it’s good to know the underlying equations here, you won’t be tested on the math specifically. And as it happens, tm has a way to perform the above math for each term in a corpus. We can implement tf–idf scores when making a document-term matrix: dtm_tfidf &lt;- DocumentTermMatrix(cleaned_corpus, control=list(weighting=weightTfIdf)) dtm_tfidf$dimnames$Docs &lt;- C19_novels$title To see what difference it makes, let’s plot the top terms in our corpus using their tf–idf scores. tfidf_counts &lt;- as.matrix(dtm_tfidf) tfidf_counts &lt;- data.frame(sort(colSums(tfidf_counts), decreasing=TRUE)) tfidf_counts &lt;- cbind(newColName=rownames(tfidf_counts), tfidf_counts) colnames(tfidf_counts) &lt;- c(&quot;term&quot;, &quot;tfidf&quot;) ggplot(data=tfidf_counts[1:50, ], aes(x=fct_reorder(term, -tfidf), y=tfidf)) + geom_bar(stat=&quot;identity&quot;) + theme(axis.text.x=element_text(angle=90, vjust=0.5, hjust=1)) + labs(title=&quot;Words with the 50-highest tf--idf scores in 18 Nineteenth-Century Novels&quot;, x=&quot;Word&quot;, y=&quot;TF-IDF&quot;) Lots of names! That makes sense: heavily weighted terms in these novels are going to be terms that are unique to each text. Main characters’ names are used a lot in novels, and the main character names in these novels are all unique. To see in more concrete way how tf–idf scores might make a difference in the way we analyze our corpus, we’ll do two last things. First, we’ll look again at term correlations, using the same words from above with findAssoscs(), but this time we’ll use tf–idf scores. Here’s “boat”: findAssocs(dtm_tfidf, &quot;boat&quot;, .85) ## $boat ## thumping shore bucket cables doo geese ## 0.95 0.93 0.92 0.92 0.92 0.92 ## pickled sea rudder gunwale scoundrels boats ## 0.92 0.91 0.91 0.91 0.91 0.90 ## keel sailed crew baffling biscuit bowsprit ## 0.90 0.89 0.89 0.89 0.89 0.89 ## hauling muskets ripped splash anchor oar ## 0.89 0.89 0.89 0.89 0.88 0.88 ## rattling sandy cook patted shipped beach ## 0.88 0.88 0.88 0.88 0.88 0.87 ## pistols seamen tobacco lee bulwarks hauled ## 0.87 0.87 0.87 0.87 0.87 0.87 ## inkling musket navigation rags steering island ## 0.87 0.87 0.87 0.87 0.87 0.86 ## bottle tumbled avast belay bilge broadside ## 0.86 0.86 0.86 0.86 0.86 0.86 ## cruising cutlasses diagonal furtively headway jupiter ## 0.86 0.86 0.86 0.86 0.86 0.86 ## mainland marlin midday monthly mutineers outnumbered ## 0.86 0.86 0.86 0.86 0.86 0.86 ## plumped riggers schooner schooners seaworthy swamping ## 0.86 0.86 0.86 0.86 0.86 0.86 ## tide&#39;s tiller tonnage towed yawed sail ## 0.86 0.86 0.86 0.86 0.86 0.85 ## ship tap loading sails aft berths ## 0.85 0.85 0.85 0.85 0.85 0.85 ## pinned ## 0.85 Here’s “writing”: findAssocs(dtm_tfidf, &quot;writing&quot;, .85) ## $writing ## hindrance messages disposal inquiries bedrooms ## 0.92 0.91 0.90 0.90 0.89 ## ladyship copy lodgings london unforeseen ## 0.88 0.87 0.87 0.87 0.87 ## drawings plantation explanations certificate dears ## 0.86 0.86 0.86 0.86 0.86 ## neighbourhood allowances ## 0.85 0.85 The semantics of these results have changed. For “boats”, we get much more terms related to sefaring. Most probably this is because only a few novels talk about boats so these terms correlate highly with one another. For “writing”, we’ve interestingly lost a lot of the words associated with writing in a strict sense (“copy”, “message”) but we’ve gained instead a list of terms that seem to situate us in where writing takes place in these novels, or what characters write about. So far though this is speculation; we’d have to look into this further to see whether the hypothesis holds. Finally, we can disaggregate our giant term count graph from above to focus more closely on the uniqueness of individual novels in our corpus. First, we’ll make a data frame from our tf–idf DTM. We’ll transpose the DTM so the documents are our variables (columns) and the corpus vocabulary terms are our observations (or rows). Don’t forget the t! tfidf_df &lt;- as.matrix(dtm_tfidf) tfidf_df &lt;- as.data.frame(t(tfidf_df)) colnames(tfidf_df) &lt;- C19_novels$title 15.8.3 Unique Terms in a Document With this data frame made, we can order our rows by the highest value for a given column. In other words, we can find out not only the top terms for a novel, but the top most unique terms in that novel. Here’s Dracula: rownames(tfidf_df[order(tfidf_df$Dracula, decreasing=TRUE)[1:50],]) ## [1] &quot;helsing&quot; &quot;mina&quot; &quot;lucy&quot; &quot;jonathan&quot; &quot;van&quot; ## [6] &quot;harker&quot; &quot;godalming&quot; &quot;quincey&quot; &quot;seward&quot; &quot;professor&quot; ## [11] &quot;morris&quot; &quot;lucy&#39;s&quot; &quot;harker&#39;s&quot; &quot;diary&quot; &quot;seward&#39;s&quot; ## [16] &quot;arthur&quot; &quot;renfield&quot; &quot;westenra&quot; &quot;whilst&quot; &quot;undead&quot; ## [21] &quot;tonight&quot; &quot;whitby&quot; &quot;dracula&quot; &quot;varna&quot; &quot;carfax&quot; ## [26] &quot;journal&quot; &quot;helsing&#39;s&quot; &quot;count&quot; &quot;count&#39;s&quot; &quot;hawkins&quot; ## [31] &quot;madam&quot; &quot;galatz&quot; &quot;jonathan&#39;s&quot; &quot;mina&#39;s&quot; &quot;pier&quot; ## [36] &quot;wolves&quot; &quot;tomorrow&quot; &quot;czarina&quot; &quot;telegram&quot; &quot;boxes&quot; ## [41] &quot;today&quot; &quot;holmwood&quot; &quot;hypnotic&quot; &quot;garlic&quot; &quot;vampire&quot; ## [46] &quot;phonograph&quot; &quot;transylvania&quot; &quot;cliff&quot; &quot;piccadilly&quot; &quot;slovaks&quot; Note here that some contractions have slipped through. Lemmatizing would take care of this, though we could also go back to the corpus object and add in another step with tm_map() and then make another DTM: cleaned_corpus &lt;- tm_map(cleaned_corpus, function(x) str_remove_all(x, &quot;\\\\&#39;s&quot;, &quot; &quot;)) We won’t bother to do this whole process now, but it’s a good example of how iterative the preprocessing workflow is. Here’s Frankenstein: rownames(tfidf_df[order(tfidf_df$Frankenstein, decreasing=TRUE)[1:50],]) ## [1] &quot;clerval&quot; &quot;justine&quot; &quot;elizabeth&quot; &quot;felix&quot; &quot;geneva&quot; ## [6] &quot;frankenstein&quot; &quot;safie&quot; &quot;cottagers&quot; &quot;dæmon&quot; &quot;ingolstadt&quot; ## [11] &quot;kirwin&quot; &quot;agatha&quot; &quot;victor&quot; &quot;ernest&quot; &quot;mont&quot; ## [16] &quot;krempe&quot; &quot;lacey&quot; &quot;waldman&quot; &quot;agrippa&quot; &quot;walton&quot; ## [21] &quot;mountains&quot; &quot;creator&quot; &quot;cottage&quot; &quot;sledge&quot; &quot;hovel&quot; ## [26] &quot;switzerland&quot; &quot;ice&quot; &quot;beaufort&quot; &quot;cornelius&quot; &quot;william&quot; ## [31] &quot;protectors&quot; &quot;moritz&quot; &quot;henry&quot; &quot;labours&quot; &quot;chamounix&quot; ## [36] &quot;glacier&quot; &quot;jura&quot; &quot;blanc&quot; &quot;endeavoured&quot; &quot;lake&quot; ## [41] &quot;leghorn&quot; &quot;monster&quot; &quot;rhine&quot; &quot;magistrate&quot; &quot;belrive&quot; ## [46] &quot;lavenza&quot; &quot;salêve&quot; &quot;saville&quot; &quot;strasburgh&quot; &quot;werter&quot; And here’s Sense and Sensibility: rownames(tfidf_df[order(tfidf_df$SenseandSensibility, decreasing=TRUE)[1:50],]) ## [1] &quot;elinor&quot; &quot;marianne&quot; &quot;dashwood&quot; &quot;jennings&quot; &quot;willoughby&quot; ## [6] &quot;lucy&quot; &quot;brandon&quot; &quot;barton&quot; &quot;ferrars&quot; &quot;colonel&quot; ## [11] &quot;mrs&quot; &quot;marianne&#39;s&quot; &quot;edward&quot; &quot;middleton&quot; &quot;elinor&#39;s&quot; ## [16] &quot;norland&quot; &quot;palmer&quot; &quot;steele&quot; &quot;dashwoods&quot; &quot;jennings&#39;s&quot; ## [21] &quot;willoughby&#39;s&quot; &quot;edward&#39;s&quot; &quot;delaford&quot; &quot;steeles&quot; &quot;cleveland&quot; ## [26] &quot;mama&quot; &quot;dashwood&#39;s&quot; &quot;lucy&#39;s&quot; &quot;brandon&#39;s&quot; &quot;fanny&quot; ## [31] &quot;allenham&quot; &quot;middletons&quot; &quot;devonshire&quot; &quot;combe&quot; &quot;ferrars&#39;s&quot; ## [36] &quot;sister&quot; &quot;morton&quot; &quot;miss&quot; &quot;margaret&quot; &quot;park&quot; ## [41] &quot;charlotte&quot; &quot;exeter&quot; &quot;magna&quot; &quot;berkeley&quot; &quot;harley&quot; ## [46] &quot;john&quot; &quot;middleton&#39;s&quot; &quot;parsonage&quot; &quot;beaux&quot; &quot;behaviour&quot; Names still rank high, but we can see in these results other words that indeed seem to be particular to each novel. With this data, we now have a sense of what makes each document unique in its relationship with all other documents in a corpus "],["data-forensics-and-cleaning-geospatial-data.html", "16 Data Forensics and Cleaning: Geospatial Data 16.1 Learning Objectives 16.2 What is Geospatial Data? 16.3 Geospatial Data Models 16.4 Data Structures Applied to Geospatial Data 16.5 Cleaning Geospatial Data 16.6 Conclusions 16.7 Optional Further Reading", " 16 Data Forensics and Cleaning: Geospatial Data 16.1 Learning Objectives This lecture is designed to introduce you to the basics of geospatial data. By the end of this lecture you will understand the main components of geospatial data are locations, attributes, and a coordinate reference system understand how geospatial data can be represented with different data models understand that the data structures we were already familiar with can be modified to contain spatial data. learn some common processes for cleaning our geospatial data 16.2 What is Geospatial Data? Geospatial data (also known as spatial data, GIS data, and other names) is information that can be attributed to a real-world location or can relate to each other in space. Technically, “geospatial” refers to locations on Earth, while “spatial” can be locations anywhere, including other planets or even ficticious places (like J.R.R. Tolkien’s hand-drawn maps for his novels), but quite often the terms are used interchangably. You use geospatial data every day on your smart phone through spatially-enabled apps like Google Maps, food delivery apps, fitness trackers, weather, or games like Pokemon Go. (geo)Spatial Data = Attributes + Locations Location = Coordinate Reference System (CRS) + Coordinates So… (geo)Spatial Data = Attributes + Coordinate Reference System (CRS) + Coordinates 16.2.1 Attributes Attributes are pieces of information about a location. For example, if I’m mapping gas stations, my attributes might be something like the price of gas, the address of the station, and the company that runs it (Shell, Arco, etc.). This isn’t the same thing as metadata, which is information about the entire dataset such as who made the data, when they made it, and how the data was created. 16.2.2 Coordinate Reference System The earth is generally round. Maps are generally flat, with a few exceptions. If you were to try to flatten out the earth, you would create some fairly major distortions. Next time you eat an orange or a tangerine, try taking off the peel and then try to create a flat solid sheet of peel from it. You’ll end up needing to cut it or smash it to get a flat surface. The same things happens to geospatial data when we try to translate it from a round globe to a flat map. But, there are ways to minimize distortions. A coordinate reference system (sometimes called a projection) is a set of mathematical formulas that translate measurements on a round globe to a flat piece of paper. The coordinate reference system also specifies the linear units of measure (i.e. feet, meters, decimal degrees, or something else) and a set of reference lines. For our purposes, we can think of coordinate reference systems coming in two flavors. One is geographic coordinate systems. For simplicity’s sake, we can think of these as coordinate reference systems that apply to latitude and longitude coordinates. Projected coordinate systems translate latitude and longitude coordinates into linear units from a specified baseline and aim to reduce some aspect of the distortion introduced in the round to flat translation. (I am very much simplifying this concept so we can learn the basics without getting overwhelmed.) To work with more than one digital spatial dataset, the coordinate reference systems must match. If they don’t match, you can transform your data into a different coordinate reference system. 16.2.3 Coordinates Coordinates are given in the distance (in the linear units specified in the CRS) from the baselines (specified in the CRS). Coordinates can be plotted just like coordinates on a graph (cartesian coordinate system). Sometimes we refer to these as X and Y, just like a graph, but sometimes you’ll hear people refer to the cardinal directions (north, south, east, and west). Let’s take a moment to talk about latitude and longitude. You’re probably at least a little familiar with latitude (Y) and longitude (X), but this is a special case that’s more complex than we probably initially realize. Latitude and longitude are angular measurements (with units in degrees) from a set of baselines - usually the Equator and the Greenwhich Meridian. We can plot latitude and longitude on a cartesian coordinate system, but this introduces major distortions increasing as you approach the poles. You never want to use straight latitude/longitude coordinates (commonly in North America, you’ll see data in the geographic coordinate reference system called WGS84) for an analysis. Always translate them into a projected coordinate system first. In addition, because the units are degrees, they are rather hard for us to interpret when we make measurements. How many degrees is it from the UC Davis campus to your appartment? It’s probably a very small fraction of a degree. Area measurements make even less sense. (What is a square degree and what does that look like?) Latitude/longitude coordinates are a great starting place, we just need to handle them correctly. 16.3 Geospatial Data Models Now we have an idea of what makes data spatial, but what does spatial data look like in a computer? There are two common data models for geospatial data: Vector and Raster. Data Model Geometry Example Vector Points Very small things, like cities at world scale . Lines Linear things, like roads at city scale . Polygons Larger things that take up space, like parks at a city scale Raster Grid Digital Photo A visual table of raster vs. vector data as continuous and discrete data. 16.3.1 Vector Data Vector data represents discrete objects in the real world with points, lines, and polygons in the dataset. If you were to draw a map to your house for a friend, you would typically use vector data - roads would be lines, a shopping center included as an important landmark might be a rectangle of sorts, and your house might be a point (perhaps represented by a star or a house icon). For this lecture, we will focus on point data. 16.3.2 Raster Data Raster data represents continuous fields or discrete objects on a grid, storing measurements or category codes in each cell of the grid. Digital photos are raster data you are already familiar with. If you zoom in far enough on a digital photo, you’ll see that photo is made up of pixels, which appear as colored squares. Pixels are cells in a regular grid and each contains the digital code that corresponds to the color that should be displayed there. Satellite images (like you see in Google Maps) are a very similar situation. 16.4 Data Structures Applied to Geospatial Data In the lecture on Data Structures (week 6), you learned that data can be structured in a number of ways, including tabular, tree (XML and JSON), relational databases, and non-hierarchical data. All of these structures can include spatial information. Data Structure Example File Type How it’s implemented Tabular CSV One or more columns hold spatial data (like Latitude &amp; Longitude) Tree geoJSON Tags in the structure indicate spatial information like geometry type and vertex locations Relational Database PostGIS or Spatialite One column holds the “geometry” information (vertexes &amp; CRS) Non-Hierarchical Relational Data Spatial Graph Databases Nodes have locations associated with them, edges represent flow (think: transportation networks or stream networks) For visualization purposes, geospatial software typically show all of these data structures as a map where each entity is linked with a table of the attribute data - one row of data in the table relates to one entity on the map. So regardless of the underlying data structure, you can think of these as interactive maps like you find on Google Maps. 16.5 Cleaning Geospatial Data What Can Go Wrong? Location data isn’t usable Location data is incorrect Attribute data is incorrect Coordinate Reference System (CRS) is improperly defined 16.5.1 Example Data The dataset we’ll be working with as an example contains locations and attributes about lake monsters. Lake monsters are fictional creatures like sea monsters, but they live in lakes and not the ocean. The most famous lake monster is probably Nessie, who lives in Loch Ness. The dataset we’re working with today is the early stages of a now much cleaner dataset. This data came from a Wikipedia page and the locations were geocoded (a process that matches text locations with real-world locations). We’ll walk through some common processes and challenges with point data stored in a csv file. 16.5.2 Making Location Data Usable Someone sends you a CSV file. At first glance, nothing looks amiss. There is a column for latitude and another for longitude, but how is it formatted? It’s degrees-minutes-seconds (DMS)! DMS looks like this: 34° 36’ 31.774” - 34 degrees, 36 minutes, 31.447 seconds - and sometimes people put in the symbols for degree (°), minutes (’), and seconds (“), and sometimes not. The computer can’t read this format, especially the symbols. It has to be converted to decimal degrees (DD), which looks like this: 34.60882611 To convert it, we need to know that there are 60 minutes in a degree and 60 seconds in a minute. Decimal Degrees = Degrees + (Minutes/60) + (Seconds/3600) 34.60882611 = 34 + (36/60) + (31.447/3600) First, we need to load the libraries we’ll need and then load the data. # Load Libraries ---------------------------------------------------------- library(&quot;sf&quot;) ## Linking to GEOS 3.8.1, GDAL 3.2.1, PROJ 7.2.1; sf_use_s2() is TRUE library(&quot;mapview&quot;) library(&quot;gdtools&quot;) #makes the display... dependency of mapview but it&#39;s not loading library(&quot;leafem&quot;) #makes the labels work... dependency of mapview but it&#39;s not loading library(&quot;leaflet&quot;) # Read Data --------------------------------------------------------------- monsters.raw&lt;-read.csv(&quot;data/lake_monsters.csv&quot;, stringsAsFactors = FALSE, encoding = &quot;utf-8&quot;) #Explore the data head(monsters.raw) ## fid field_1 Lake Area Country Continent ## 1 1 1 Arenal Lagoon Alajuela Costa Rica North America ## 2 2 2 Bangweulu Swamp Zambia Africa ## 3 3 3 Bassenthwaite Lake England United Kingdom Europe ## 4 4 4 Bear Lake Idaho, Utah USA North America ## 5 5 5 Brosno Lake Tver Oblast Russia Europe ## 6 6 6 Bueng Khong Long Bueng Kan Thailand Asia ## Name lat lon lat_dms ## 1 unnamed 10.49143 -84.851696 10°29?29.1304? ## 2 Nsanga -11.14741 29.784582 -11°8?50.6760? ## 3 Eachy 54.65279 -3.213612 54°39?10.0359? ## 4 Bear Lake Monster, Isabella 42.21721 -111.319881 42°13?1.9643? ## 5 Brosno Dragon 56.82407 31.914652 56°49?26.6520? ## 6 Phaya Naga 18.02363 104.014360 18°1?25.0676? ## lon_dms coords_3395 lon_3395 ## 1 -84°51?6.1069? Point (-9445647.63285386 1166706.48735204) -9445647.6 ## 2 29°47?4.4935? Point (3315604.44829715 -1240572.14607131) 3315604.4 ## 3 -3°12?49.0016? Point (-357737.60213262 7259890.14217639) -357737.6 ## 4 -111°19?11.5709? Point (-12392072.44582391 5164853.16566773) -12392072.4 ## 5 31°54?52.7472? Point (3552722.80948453 7688451.06249625) 3552722.8 ## 6 104°0?51.6974? Point (11578825.63491604 2027100.69217741) 11578825.6 ## lat_3395 ## 1 1166706 ## 2 -1240572 ## 3 7259890 ## 4 5164853 ## 5 7688451 ## 6 2027101 Next, we need to write some functions to deal with our specific DMS data and how its formatted. # Functions --------------------------------------------------------------- #This function splits up the DMS column into three columns - D, M, &amp; S split.dms&lt;-function(dms.column){ #separate the pieces of the DMS column variable&lt;-do.call(rbind, args=c(strsplit(dms.column, &#39;[°?]+&#39;))) #makes a matrix of characters mode(variable)=&quot;numeric&quot; #assigning the data type to numeric instead of character dms.split&lt;-as.data.frame(variable) split.string&lt;-strsplit(dms.column, &#39;[°?]+&#39;) # naming the columns names(dms.split)&lt;-c(&quot;D&quot;, &quot;M&quot;, &quot;S&quot;) return(dms.split) } # this function coverts a 3 column dataframe of DMS to DD, like the data created by split.dms() decimaldegrees&lt;-function(dms.df){ dd&lt;-data.frame() for (i in 1:dim(dms.df)[1]){ if (dms.df[i, 1]&gt;0){ #Decimal Degrees = Degrees + (Minutes/60) + (Seconds/3600) dd.row&lt;-dms.df[i,1]+(dms.df[i,2]/60)+(dms.df[i,3]/3600) dd&lt;-rbind(dd, dd.row) } else{ #-Decimal Degrees = Degrees - (Minutes/60) - (Seconds/3600) dd.row&lt;-dms.df[i,1]-(dms.df[i,2]/60)-(dms.df[i,3]/3600) dd&lt;-rbind(dd, dd.row) } } return(dd) } Finally, we can process our DMS data to convert it to Decimal Degreess (DD). # Process Latitude dms.split&lt;-split.dms(monsters.raw$lat_dms) dd&lt;-decimaldegrees(dms.split) monsters.df&lt;-cbind(monsters.raw, dd) names(monsters.df)[15]&lt;-&quot;lat_dd&quot; # Process Longitude dms.split&lt;-split.dms(monsters.raw$lon_dms) dd&lt;-decimaldegrees(dms.split) monsters.df&lt;-cbind(monsters.df, dd) names(monsters.df)[16]&lt;-&quot;lon_dd&quot; # Look at the data head(monsters.df) ## fid field_1 Lake Area Country Continent ## 1 1 1 Arenal Lagoon Alajuela Costa Rica North America ## 2 2 2 Bangweulu Swamp Zambia Africa ## 3 3 3 Bassenthwaite Lake England United Kingdom Europe ## 4 4 4 Bear Lake Idaho, Utah USA North America ## 5 5 5 Brosno Lake Tver Oblast Russia Europe ## 6 6 6 Bueng Khong Long Bueng Kan Thailand Asia ## Name lat lon lat_dms ## 1 unnamed 10.49143 -84.851696 10°29?29.1304? ## 2 Nsanga -11.14741 29.784582 -11°8?50.6760? ## 3 Eachy 54.65279 -3.213612 54°39?10.0359? ## 4 Bear Lake Monster, Isabella 42.21721 -111.319881 42°13?1.9643? ## 5 Brosno Dragon 56.82407 31.914652 56°49?26.6520? ## 6 Phaya Naga 18.02363 104.014360 18°1?25.0676? ## lon_dms coords_3395 lon_3395 ## 1 -84°51?6.1069? Point (-9445647.63285386 1166706.48735204) -9445647.6 ## 2 29°47?4.4935? Point (3315604.44829715 -1240572.14607131) 3315604.4 ## 3 -3°12?49.0016? Point (-357737.60213262 7259890.14217639) -357737.6 ## 4 -111°19?11.5709? Point (-12392072.44582391 5164853.16566773) -12392072.4 ## 5 31°54?52.7472? Point (3552722.80948453 7688451.06249625) 3552722.8 ## 6 104°0?51.6974? Point (11578825.63491604 2027100.69217741) 11578825.6 ## lat_3395 lat_dd lon_dd ## 1 1166706 10.49143 -84.851696 ## 2 -1240572 -11.14741 29.784582 ## 3 7259890 54.65279 -3.213612 ## 4 5164853 42.21721 -111.319881 ## 5 7688451 56.82407 31.914652 ## 6 2027101 18.02363 104.014360 Another common issue with point data is that the latitude and longitude are not in any form of degrees, but instead are in a projected coordinate system with linear units (usually feet or meters). If the data doesn’t come with metadata, you may be left guessing which coordinate system it is in. With experience, you’ll get better at guessing, but sometimes the data is not usable. Our monsters dataset has latitude and longitude in the World Mercator (EPSG: 3395) projection as well. Let’s briefly look at that here, but we’ll play with that more later in this document. monsters.df[1:10,13:16] ## lon_3395 lat_3395 lat_dd lon_dd ## 1 -9445647.6 1166706 10.49143 -84.851696 ## 2 3315604.4 -1240572 -11.14741 29.784582 ## 3 -357737.6 7259890 54.65279 -3.213612 ## 4 -12392072.4 5164853 42.21721 -111.319881 ## 5 3552722.8 7688451 56.82407 31.914652 ## 6 11578825.6 2027101 18.02363 104.014360 ## 7 -7845463.9 5433619 43.98618 -70.477001 ## 8 -12704546.6 6054836 47.87777 -114.126884 ## 9 -9467608.2 5128572 41.97447 -85.048971 ## 10 -12525669.2 5011829 41.18707 -112.520000 Note that data preparation and cleaning is the vast majority of the work for all data, not just spatial data. All of the code we just looked at was just to get the data in a usable format. We’ll convert it to a spatial data type and map it in the next section. 16.5.3 Cleaning Location Data Sometimes, the locations in your dataset are incorrect. This can happen for a number of reasons. For example, it’s fairly common for data to get truncated or rounded if you open a CSV in Excel. Removing decimal places from coordinate data loses precision. People often swap their latitude and longitude columns as well, which make data show up in the wrong cartesian coordinate, for example, (-119, 34) is a verry different location than (34, -119)… -119 is actually out of the range of latitude data and will often break your code. Another common source of error is in the way the data was made. If data is produced by geocoding, turning an address or place name into a coordinate, the location may have been matched badly. If the data was made by an analysis process, an unexpected aspect of the data could cause problems, like a one-to-many join when you thought you had a one-to-one join in a database. Regardless of how the errors came about, how do we find incorrect locations? Start by mapping the data and see where it lands. Is it where you expect the data to be? Sometimes you can’t tell it’s wrong because the data looks normal. # Convert the monsters dataframe into an SF (spatial) object # Note: x is the dataframe, not longitude. # Coordinate Reference System (CRS) - we&#39;re using lat/long here so we need WGS84 which is EPSG code 4326 - we just need to tell R what the CRS is, we don&#39;t change it this way. If we want to change it, we need to use st_transform(). monsters.sf&lt;-st_as_sf(x=monsters.df, coords=c(&quot;lon_dd&quot;, &quot;lat_dd&quot;), crs = 4326) # Notice we added a geometry column! names(monsters.sf) ## [1] &quot;fid&quot; &quot;field_1&quot; &quot;Lake&quot; &quot;Area&quot; &quot;Country&quot; ## [6] &quot;Continent&quot; &quot;Name&quot; &quot;lat&quot; &quot;lon&quot; &quot;lat_dms&quot; ## [11] &quot;lon_dms&quot; &quot;coords_3395&quot; &quot;lon_3395&quot; &quot;lat_3395&quot; &quot;geometry&quot; # Plot a map mapview(monsters.sf) This is a screen capture of the output for the mapview function. Running this code in a regular R session (i.e. not in knitr or bookdown like we need to for this reader) will make an interactive map. In the interactive version of this map, you can pan and zoom to different areas to see more detail. Clicking on a point will open a popup with attribute information. First impressions: This map looks good! The points are all on land masses, none in the ocean. Let’s see if they are on the correct continent… mapview(monsters.sf, zcol=&quot;Continent&quot;, legend= TRUE) Map of monster locations by continent It’s hard to see, but there’s a point in Michigan that’s the wrong color for North America! Map of monster locations by continent zoomed in to the Great Lakes Whoops! Lakes of Killarney isn’t in Michigan! That point should be in Ireland! If we zoom in, we can see why the geocoder got confused. The lake names are very similar. 16.5.4 Cleaning Attribute Data Attribute data can be proofed in much the same way tabular data can be proofed. You can look at the statistical properties of numeric data or the unique entities in a list of categorical variables to see if any values are odd or out of place. With spatial data, we can also map the data and visualize it by attribute values to see if anything is out of place spatially. Labels are another helpful tool. Sometimes cleaning attributes uncovers issues with the locations. Let’s make sure the lake names match the lakes the points are in. We’ll make a map and if you zoom in enough, the lake names will appear in the background map data. my.label.options&lt;-labelOptions(clickable=TRUE) #makes a popup with attribute information map.lakename&lt;-mapview(monsters.sf, zcol=&quot;Lake&quot;, legend= FALSE) labels.lakename&lt;-addStaticLabels(map.lakename, label=monsters.sf$Lake, labelOption=my.label.options) labels.lakename Map of monster locations by lake name zoomed in to the Great Lakes Map of monster locations by lake name zoomed in to the Great Lakes And for fun, let’s look at the monster names: map.monstername&lt;-mapview(monsters.sf, zcol=&quot;Name&quot;, legend= FALSE) labels.monstername&lt;-addStaticLabels(map.monstername, label=monsters.sf$Name, labelOption=my.label.options) labels.monstername Map of monster locations by monster name zoomed in to the Great Lakes Map of monster locations by monster name zoomed in to the Great Lakes Yikes! That needs some clean-up too! The name column is missing some names and some records have extra information in them. 16.5.5 Checking Coordinate Reference Systems “Why is my California data showing up in Arizona?” is a common question UC Davis researchers ask on the Geospatial email list. Why does this happen? It’s usually because the CRS for their data is impropperly defined. Someone changed the defnition but didn’t reproject the data (the mathematical process of switching CRSs). Using the wrong CRS will often shift data just enough to look really funny on a map, but sometimes it won’t show up at all. “Why don’t my datasets line up in my map?” Again, it’s your CRS. In this case, they could be correct for all of the datasets you’re using, but each dataset has a different CRS. You can think of CRSs as different dimensions in your favorite SciFi story. Sometimes you can see the other person in the other dimension (CRS), but usually they are too different and you’re nowhere near each other. Datasets have to have the same CRS to make a map or do any analysis. Our data came with lat/long data in another coordinate reference system - EPSG 3395 “World Mercator”, a world projection centered on Europe. Notice how the coordinates look very different from the lat/long coordinates in EPSG 4326 “WGS 84” monsters.df[1:10,13:16] ## lon_3395 lat_3395 lat_dd lon_dd ## 1 -9445647.6 1166706 10.49143 -84.851696 ## 2 3315604.4 -1240572 -11.14741 29.784582 ## 3 -357737.6 7259890 54.65279 -3.213612 ## 4 -12392072.4 5164853 42.21721 -111.319881 ## 5 3552722.8 7688451 56.82407 31.914652 ## 6 11578825.6 2027101 18.02363 104.014360 ## 7 -7845463.9 5433619 43.98618 -70.477001 ## 8 -12704546.6 6054836 47.87777 -114.126884 ## 9 -9467608.2 5128572 41.97447 -85.048971 ## 10 -12525669.2 5011829 41.18707 -112.520000 # Let&#39;s make our World Mercator data spatial so we can explore its CRS monsters.sf.3395&lt;-st_as_sf(x=monsters.df, coords=c(&quot;lon_3395&quot;, &quot;lat_3395&quot;), crs = 3395) # st_crs tells us what the CRS is in well known text (WKT) and EPSG (if it&#39;s avaialble) st_crs(monsters.sf) ## Coordinate Reference System: ## User input: EPSG:4326 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;geodetic latitude (Lat)&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;geodetic longitude (Lon)&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## USAGE[ ## SCOPE[&quot;Horizontal component of 3D system.&quot;], ## AREA[&quot;World.&quot;], ## BBOX[-90,-180,90,180]], ## ID[&quot;EPSG&quot;,4326]] st_crs(monsters.sf.3395) ## Coordinate Reference System: ## User input: EPSG:3395 ## wkt: ## PROJCRS[&quot;WGS 84 / World Mercator&quot;, ## BASEGEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4326]], ## CONVERSION[&quot;World Mercator&quot;, ## METHOD[&quot;Mercator (variant A)&quot;, ## ID[&quot;EPSG&quot;,9804]], ## PARAMETER[&quot;Latitude of natural origin&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8801]], ## PARAMETER[&quot;Longitude of natural origin&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8802]], ## PARAMETER[&quot;Scale factor at natural origin&quot;,1, ## SCALEUNIT[&quot;unity&quot;,1], ## ID[&quot;EPSG&quot;,8805]], ## PARAMETER[&quot;False easting&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8806]], ## PARAMETER[&quot;False northing&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8807]]], ## CS[Cartesian,2], ## AXIS[&quot;(E)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;metre&quot;,1]], ## AXIS[&quot;(N)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;metre&quot;,1]], ## USAGE[ ## SCOPE[&quot;Very small scale conformal mapping.&quot;], ## AREA[&quot;World between 80°S and 84°N.&quot;], ## BBOX[-80,-180,84,180]], ## ID[&quot;EPSG&quot;,3395]] # Check to see if they are identical, returning a logical vector identical(st_crs(monsters.sf), st_crs(monsters.sf.3395)) ## [1] FALSE 16.6 Conclusions We’ve learned some of the basics of geospatial data. We learned that the main components of geospatial data are locations, attributes, and a coordinate reference system. We saw how geospatial data can be represented with different data models, but we focused on point vector data. We learned that the data structures we were already familiar with can be modified to contain spatial data. And finally, we looked at some common processes for cleaning our geospatial data. This was a lot to cover, but we just scratched the surface of all your can do with geospatial data science! If you want to learn more, UC Davis has some fantastic introductory classes for GIS (Geographic Information Systems/Science) and Remote Sensing (working satelite data and air photos). 16.7 Optional Further Reading Bolstad, P. 2019. GIS Fundamentals: A first text on geographic information systems. Sixth Edition. XanEdu. Ann Arbor, MI. 764 pp. Sutton, T., O. Dassau, &amp; M. Sutton. 2021. A Gentle Introduction to GIS. https://docs.qgis.org/3.16/en/docs/gentle_gis_introduction/preamble.html (accessed on 2021-02-11) "],["references.html", "17 References", " 17 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
