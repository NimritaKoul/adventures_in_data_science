[["index.html", "Adventures in Data Science Overview", " Adventures in Data Science Dr. Carl Stahmer Dr. Pamela L. Reynolds Dr. Nick Ulle Dr. Tyler Shoemaker Dr. Michele Tobias Dr. Wesley Brooks Arthur Koehl Carrie Alexander Jared Joseph 2022-02-03 Overview This is the course reader for IST008, Adventures in Data Science: Social Science Edition. The course is designed to provide students with a basic understanding of computing and network architecture, basic programming skills, and an introduction to common methods in Data Science and Digital Humanities. This course reader provides background information that will help you to better understand the concepts that we will discuss in class and to better participate in the hands-on portion of the course. "],["working-with-the-command-line.html", "1 Working with the Command Line 1.1 Interacting with the Command Line 1.2 Common Command Line Commands 1.3 Command Line Text Editors 1.4 Basic Vim Commands", " 1 Working with the Command Line Most users interact with their computer through a Graphical User Interface (GUI) that allows them to use a mouse, keyboard, and graphical elements on screen (such as file menus, pictures of folders and files, etc.) to perform their work. Users tend to conflate their Operating System and their GUI because computer hardware and software manufacturers tightly pack these two concerns as a convenience to users. But the Windows 10 or Mac Big Sur operating system that makes your computer work and the Windows 10 or Mac Big Sur GUI that you interact with are, in fact completely different and separable software packages and it is possible to use different methods/software to interact with your computer than the stock, tightly coupled GUI that launches automatically when you turn on your computer. Because computer manufacturers like Windows and Mac devote so many resources to the development of their system GUIs, there are few viable (at present, none, commercially available) competing GUIs for these platforms. This is not the case in the Linux world, however, where users have several system GUI packages from which to choose and can seamlessly switch between them as desired. Despite the lack of competition/choice on the GUI front when it comes to interacting with your computer, there are other, non-graphical ways of communicating directly with your operating system that exist for all operating systems. We call these “Command Line” interfaces. The Command Line offers a text-only, non graphical means of interacting with your computer. In the early days of computing, all user interaction with the computer happened at the command line. In the current days of graphical user interfaces, using the Command Line requires you to launch a special program that provides Command Line access. Mac users will use an application called “Terminal” which ships by default with the Mac operating system. To launch the Terminal application, go to: Applications -&gt; Utilities -&gt; Terminal When you launch the application, you will see something like this: Windows users will use an application called Git Bash, which was installed on your system when you installed Git. To launch Git Bash, go to: Click on the Windows Start Menu and search for “Git Bash” Alternatively, Click on the Windows Start Menu, select Programs, and browse to Git Bash When you launch the application, you will see something like this: 1.1 Interacting with the Command Line While it can look intimidating to those raised on the GUI, working with the Command Line is actually quite simple. Instead of pointing and clicking on things to make them happen, you type written commands. The figure below shows a new, empty Command Line Interface in the Mac Terminal application The Command Line prompt contains a lot of valuable information. The beginning of the line, “(base) MacPro-F5KWP01GF694” tells us exactly which computer we are communication with. This may seem redundant, but it is actually possible to interact with computers other than the one you are typing on by connecting to them via the Command Line over the network. The bit of information after the colon, in this example the “~” character tells us where in the computer’s filesystem we are. We’ll learn more about this later, for now you need to undersant that the “~” character means that you are in your home directory. The next piece of information we are given is the username under which we are logged into the computer, in this case, my local username, “cstahmer”. After the username, we see the “$” character. This is known as the Command Prompt. It is an indicator that the Command Line application is waiting for you to enter something. The Command Prompt character is used througout these materials when giving command examples. When working through materials, DO NOT ENTER the Command Prompt. It will already be there telling you that the computer is ready to receive your command. Depending on your system and/or Command Line interface, you may or may not also see a solid or flashing box that appears after the Command Prompt. This is a Cursor Position Indicator, which tells you where the current cursor is in the terminal. This is useful if you need to go gack and correct an error. Generally speaking, you can’t click a mouse in a terminal app to edit text. You need to use your computer’s right and left arrows to move the cursor to the correct location and then make your edit. As noted earlier, we interact with the Command Line by typing commands. The figure below shows an example of a simple command, “echo” being entered into the Command Line. The “echo” command prints back to screen any text that you supply to the command It literally echoes your text. To execute, this or any command, you simply hit the “return” or “enter” key on your keyboard. You’ll see that when you execute a Command Line command the sytem performs the indicated operation, prints any output from the operation to screen and then delivers a new Command Line prompt. Note that depending on your particular system and/or Command Line interface, things might look slightly different on your computer. However, the basic presentation and function as described above will be the same. 1.2 Common Command Line Commands During our hands-on, in-class session we will practice using the following Command Line commands. Be prepared to have this page ready as a reference during class to make things easier. Table 1.1: Command Name Function ls List Lists all files in the current directory. ls -l List with Long flag Lists additional information about each file. ls -a List with All flag Lists all files, including hidden files. pwd Print Working Directory Prints the current working directory. mkdir Make Directory Creates a new file directory. cd Change Directory Navigates to another directory on the file system. mv Move Moves files. cp Copy Copies files. rm Remove/delete Deletes files. For a more complete list of Unix Commands, see the Unix Cheat Sheet. 1.3 Command Line Text Editors The Command Line also features a variety of different text editors, similar in nature to Microsoft Word or Mac Pages but much more stripped down. These editors are only accessible from the Command Line; we won’t spend very much time with them, but it is important to know how to use them so that you can open, read, and write directly in the Command Line window. Macs and Git Bash both ship with a text editor called Vim (other common editors include Emacs and Nano). To open a file with vim, type vi in a Command Line window, followed by the filename. If you want to create a new file, simply type the filename you’d like to use for that file after vi. Vim works a bit differently than other text editors and word processors. It has a number of ‘modes,’ which provide different forms of interaction with a file’s data. We will focus on two modes, Normal mode and Insert. When you open a file with Vim, the program starts in Normal mode. This mode is command-based and, somewhat strangely, it doesn’t let you insert text directly in the document (the reasons for this have to do with Vim’s underlying design philosophy: we edit text more than we write it on the Command Line). To insert text in your document, switch to Insert mode by pressing i. You can check whether you’re in Insert mode by looking at the bottom left hand portion of the window, which should read -- INSERT --. Once you are done inserting text, pressing ESC (the Escape key) will bring you back to Normal mode. From here, you can save and quit your file, though these actions differ from other text editors and word processors: saving and quitting with Vim works through a sequence of key commands (or chords), which you enter from Normal mode. To save a file in Vim, make sure you are in Normal mode and then enter :w. Note the colon, which must be included. After you’ve entered this key sequence, in the bottom left hand corner of your window you should see “[filename] XL, XC written” (L stands for “lines” and C stands for “characters”). To quit Vim, enter :q. This should take you back to your Command Line and, if you have created a new file, you will now see that file in your window. If you don’t want to save the changes you’ve made in a file, you can toss them out by typing :q! in place of :w and then :q. Also, in Vim key sequences for save, quit, and hundreds of other commands can be chained together. For example, instead of separately inputting :w and :q to save and quite a file, you can use :wq, which will produce the same effect. There are dozens of base commands like this in Vim, and the program can be customized far beyond what we need for our class. More information about this text editor can be found here. 1.4 Basic Vim Commands Table 1.2: Command Function esc Enter Normal mode. i Enter Insert mdoe. :w Save. :q Quit. :q! Quit without saving. For a more complete list of Vim commands, see this Cheat Sheet. "],["introduction-to-version-control.html", "2 Introduction to Version Control 2.1 What is Version Control? 2.2 Software Assisted Version Control 2.3 Local vs Server Based Version Control 2.4 Central Version Control Systems 2.5 Distributed Version Control Systems 2.6 The Best of Both Worlds 2.7 VCS and the Computer File System 2.8 How Computers Store and Access Information 2.9 How VCS Manage Your Files 2.10 Graph-Based Data Management 2.11 Additional Resources", " 2 Introduction to Version Control This section covers the basics of using Version Control Software (VCS) to track and record changes to files on your local computer. It provides background information that will help you to better understand what VCS is, why we use it, and how it does its work. 2.1 What is Version Control? Version control describes a process of storing and organizing multiple versions (or copies) of documents that you create. Approaches to version control range from simple to complex and can involve the use of various human workflows and/or software applications to accomplish the overall goal of storing and managing multiple versions of the same document(s). Most people have a folder/directory somewhere on their computer that looks something like this: Or perhaps, this: This is a rudimentary form of version control that relies completely on the human workflow of saving multiple versions of a file. This system works minimally well, in that it does provide you with a history of file versions theoretically organized by their time sequence. But this filesystem method provides no information about how the file has changed from version to version, why you might have saved a particular version, or specifically how the various versions are related. This human-managed filesystem approach is more subject to error than software-assisted version control systems. It is not uncommon for users to make mistakes when naming file versions, or to go back and eit files out of sequence. Software-assisted version control systems (VCS) such as Git were designed to solve this problem. 2.2 Software Assisted Version Control Version control software has its roots in the software development community, where it is common for many coders to work on the same file, sometimes synchronously, amplifying the need to track and understand revisions. But nearly all types of computer files, not just code, can be tracked using modern version control systems. IBM’s OS/360 IEBUPDTE software update tool is widely regarded as the earliest and most widely adopted precursor to modern, version control systems. Its release in 1972 of the Source Code Control System (SCCS) package marked the first, fully fledged system designed specifically for software version control. Today’s marketplace offers many options when it comes to choosing a version control software system. They include systems such as Git, Visual Source Safe, Subversion, Mercurial, CVS, and Plastic SCM, to name a few. Each of these systems offers its twist on version control, differing sometimes in the area of user functionality, sometimes in how it handles things on the back-end, and sometimes both. This tutorial focuses on the Git VCS, but in the sections that follow we offer some general information about classes of version control systems to help you better understand how Git does what it does and help you make more informed decisions about how to deploy it for you own work. 2.3 Local vs Server Based Version Control There are two general types of version control systems: Local and Server (sometimes called Cloud) based systems. When working with a Local version control system, all files, metadata, and everything associated with the version control system live on your local drive in a universe unto itself. Working locally is a perfectly reasonable option for those who work independently (not as part of a team), have no need to regularly share their files or file versions, and who have robust back-up practices for their local storage drive(s). Working locally is also sometimes the only option for projects involving protected data and/or proprietary code that cannot be shared. Server based VCS utilize software running on your local computer that communicates with a remote server (or servers) that store your files and data. Depending on the system being deployed, files and data may reside exclusively on the server and are downloaded to temporary local storage only when a file is being actively edited. Or, the system may maintain continuous local and remote versions of your files. Server based systems facilitate team science because they allow multiple users to have access to the same files, and all their respective versions, via the server. They can also provide an important, non-local back-up of your files, protecting you from loss of data should your local storage fail. Git is a free Server based version control system that can store files both locally and on a remote server. While the sections that follow offer a broader description of Server based version control, in this workshop we will focus only on using Git locally and will not configure the software to communicate with, store files on, or otherwise interact with a remote server. DataLab’s companion “Git for Teams” workshop focuses on using Git with the GitHub cloud service to capitalize on Git’s distributed version control capabilities. Server based version control systems can generally be segmented into two distinct categories: 1) Centralized Version Control Systems (Centralized VCS) and 2) Distributed Version Control Systems (Distributed VCS). 2.4 Central Version Control Systems Centralized VCS is the oldest and, surprisingly to many, still the dominant form of version control architecture worldwide. Centralized VCS implement a “spoke and wheel” architecture to provided server based version control. With the spoke and wheel architecture, the server maintains a centralized collection of file versions. Users utilize version control clients to “check-out” a file of interest to their local file storage, where they are free to make changes to the file. Centralized VCS typically restrict other users from checking out editable versions of a file if another user currently has the file checked out. Once the user who has checked out the file has finished making changes, they “check-in” their new version, which is then stored on the server from where it can be retrieved and “checked-out” by another user. As can be seen, Centralized VCS provide a very controlled and ordered universe that ensures file integrity and tracking of changes. However, this regulation comes at a cost. Namely, it reduces the ease with which multiple users can work simultaneously on the same file. 2.5 Distributed Version Control Systems Distributed VCS are not dependent on a central repository as a means of sharing files or tracking versions. Distributed VCS implement a network architecture (as opposed to the spoke and wheel of the Centralized VCS as pictured above) to allow each user to communicate directly with every other user. In Distributed VCS, each user maintains their own version history of the files being tracked, and the VCS software communicates between users to keep the various local file systems in sync with each other. With this type of system, the local versions of two different users will diverge from each other if both users make changes to the file. This divergence will remain in place until the local repositories are synced, at which time the VCS stitches (or merges) the two different versions of the file into a single version that reflects the changes made by each individual, and then saves the stitched version of the file onto both systems as the current version. Various mechanisms can then be used to resolve the conflicts that may arise during this merge process. Distributed VCS offer greater flexibility and facilitate collaborative work, but a lack of understanding of the sync/merge workflow can cause problems. It is not uncommon for a user to forget to synch their local repository with the repositories of other team members and, as a result, work for extended periods of time on outdated files that don’t reflect their teammates and result in work inefficiencies and merge challenges. 2.6 The Best of Both Worlds An important feature of Distributed VCS is that many users and organizations choose to include a central server as a node in the distributed network. This creates an hybrid universe in which some users will sync directly to each other while other users will sync through a central server. Syncing with a cloud-based server provides an extra level of backup for your files and also facilitates communication between users. But treating the server as just another node on the network (as opposed to a centralized point of control) puts the control and flexibility back in the hands of the individual developer. For example, in a true Centralized CVS, if the server goes down then nobody can check files in and out of the server, which means that nobody can work. But in a Distributed CVS this is not an issue. Users can continue to work on local versions and the system will sync any changes when the server becomes available. Git, which is the focus of this tutorial, is a Distributed VCS. You can use Git to share and sync repositories directly with other users or through a central Git server such as, for example, GitHub or GitLab. 2.7 VCS and the Computer File System When we think about Version Control, we typically think about managing changes to individual files. From the user perspective, the File is typically the minimum accessible unit of information. Whether working with images, tabular data, or written text, we typically use software to open a File that contains the information we want to view or edit. As such, it comes as a surprise to most users that the concept of Files, and their organizing containers (Folders or Directories), are not intrinsic to how computers themselves store and interact with data. In this section of the tutorial we will learn about how computers store and access information and how VCS interact with this process to track and manage files. 2.8 How Computers Store and Access Information For all of their computing power and seeming intelligence, computers still only know two things: 0 and 1. In computer speak, we call this a binary system, and the unit of memory on a hard-disk, flash drive, or computer chip that stores each 1 or 0 is called a bit. You can think of your computer’s storage device (regardless of what kind it is) as a presenting a large grid, where each box is a bit: In the above example, as with most computer storage, the bits in our storage grid are addressable, meaning that we can designate a particular bit using a row and column number such as, for example, A7, or E12. Also, remember, that each bit can only contain one of two values: 0 or 1. So, in practice, our storage grid would actually look something like this: All of the complex information that we store in the computer is translated to this binary language prior to storage using a system called Unicode. You can think of Unicode as a codebook that assigns a unique combination of 8, 16, 32, 64, etc. (depending on how old your computer is) ones and zeros to each letter, numeral, or symbol. For example, the 8-bit Unicode for the upper case letter “A” is “01000001”, and the 8-bit Unicode character for the digit “3” is “00110011”. The above grid actually spells out the phrase, “Call me Ishmael”, the opening line of Herman Melville’s novel Moby Dick. An important aspect of how computers story information in binary form is that, unlike most human readable forms of data storage, there is no right to left, up or down, or any other regularized organization of bits on a storage medium. When you save a file on your computer, the computer simply looks for any open bits and starts recording information. The net result is that the contents of single file are frequently randomly interleaved with data from other files. This mode of storage is used because it maximizes the use of open bits on the storage device. But it presents the singular problem of not making data readable in a regularized, linear fashion. To solve this problem, all computers reserve a particular part of their internal memory for a “Directory” which stores a sector map of all chunks of data. For example, if you create a file called README.txt with the word “hello” in it, the computer would randomly store the Unicode for the five characters in the word “hello” on the storage device and make a directory entry something like the following: Understanding the Directory concept and how computers store information is crucial to understanding how VCS mange your Files. 2.9 How VCS Manage Your Files Most users think about version control as a process of managing files. For example, if I might have a directory called “My Project” that holds several files related to this project as follows: One approach to managing changes to the above project files would be to store multiple versions of each file as in the figure below for the file analysis.r: In fact, many VCS do exactly this. They treat each file as the minimum unit of data and simply save various versions of each file along with some additional information about the version. This approach can work reasonably well. However, it has limitations. First, this approach can unnecessarily consume space on the local storage device, especially if you are saving many versions of a very large file. It also has difficulty dealing with changes in filenames, typically treating the same file with a new name as a completely new file, thereby breaking the chain of version history. To combat these issues, good VCS don’t actually manage files at all. They manage Directories. Distributed VCS like Git take this alternate approach to data storage that is Directory, rather than file, based. 2.10 Graph-Based Data Management Git (and many other Distributed VCS) manage your files as collections of data rather than collections of files. Git’s primary unit of management is the “Repository,” or “Repo” for short, which is aligned with your computer’s Directory/Folder structure. Consider, for example, the following file structure: Here we see a user, Tom’s, home directory, which contains three sub directories (Data, Thesis, and Tools) and one file (Notes.txt). Both the Data and Tools directories contain sub files and/or directories. If Tom wanted to track changes to the two files in the Data directory, he would first create a Git repository by placing the Data directory “under version control.” When a repository is created, the Git system writes a collection of hidden files into the Data Directory that it uses to store information about all of the data that lives under that directory. This includes information about the addition, renaming, and deletion of both files and folders as well as information about changes to the data contained in the files themselves. Additions, deletions and versions of files are tracked and stored not as copies of files, but rather as a set of instructions that describes changes made to the underling data and the directory structure that describes them. 2.11 Additional Resources The Git Book is the defintive Git resource and provides an excellent reference for everythign that we will cover in the Interactive session. There is no need to read the book prior to the session, but it’s a good reference resource to have avaialable as you begin to work with Git after the workshop. "],["git-version-control-basics.html", "3 Git Version Control Basics 3.1 Save, Stage, Commit 3.2 Creating Your First Repo 3.3 Checking the Status of a Repo 3.4 Version of a File 3.5 View a History of Your Commits 3.6 Comparing Commits 3.7 Comparing Files 3.8 To View an Earlier Commit 3.9 Undoing Things 3.10 When Things go Wrong! 3.11 Git Branching", " 3 Git Version Control Basics 3.1 Save, Stage, Commit Git does not automatically preserve versions of every “saved” file. When working with Git, you save files as you always do, but this has no impact on the versions that are preserved in the repository. To create a “versions”, you must first add saved files to a Staging area and then “Commit” your staged files to the repository. The Commits that you make constituted the versions of files that are preserved in the repository. 3.2 Creating Your First Repo Move to your Home directory $ cd ~ note: The $ character represents your command prompt. DO NOT type it into your terminal Create a new directory for this course module $ cd ~ $ mkdir dsadventures Change to the new directory $ cd dsadventures Put the new directory under version control $ git init 3.3 Checking the Status of a Repo To check the status of a repository use the followign command $ git status 3.4 Version of a File In Gitspeak, we ‘commit’ if version of a file to the repository to save a copy of the current working version of a file as a version. This is a multi-step process in which we first ‘stage’ the file to be committed and then ‘commit’ the file. STEP 1: Place the file you want to version into the Staging Area $ git add &lt;filename&gt; Replace in the command above with the actual name of the file you want to version. STEP 2: Commit Staged Files $ git commit -m &#39;A detailed comment explaining the nature of the versio being committed. Do not include any apostrophe&#39;s in your comment.&#39; 3.5 View a History of Your Commits To get a history of commits $ git log To see commit history with patch data (insertions and deletions) for a specified number of commits $ git log -p -2 To see abbreviated stats for the commit history $ git log --stat You can save a copy of your Git log to a text file with the following command: $ git --no-pager log &gt; log.txt 3.6 Comparing Commits $ git diff &lt;commit&gt; &lt;commit&gt; 3.7 Comparing Files $ git diff &lt;commit&gt; &lt;file&gt; or $ git diff &lt;commit&gt;:&lt;file&gt; &lt;commit&gt;:&lt;file&gt; 3.8 To View an Earlier Commit $ git checkout &lt;commit&gt; To solve Detached Head problem either RESET HEAD as described below or just chekout another branch git checkout &lt;branch&gt; To save this older version as a parallel branch execute $ git checkout -b &lt;new_branch_name This will save the older commit as a new branch running parallel to master. 3.9 Undoing Things One of the common undos takes place when you commit too early and possibly forget to add some files, or you mess up your commit message. If you want to redo that commit, make the additional changes you forgot, stage them, and commit again using the –amend option $ git commit --amend To unstage a file for commit use $ git reset HEAD &lt;file&gt; Throwing away changes you’ve made to a file $ git checkout -- &lt;file&gt; Rolling everything back to the last commit $ git reset --hard HEAD Rolling everything back to the next to last commit (The commit before the HEAD commit) $ git reset --hard HEAD^ Rolling everything back tp two commits before the head $ git reset --hard HEAD^2 Rolling everything back to an identified commit using HASH/ID from log $ git reset --hard &lt;commit&gt; 3.10 When Things go Wrong! To reset everything back to an earlier commit and make sure that the HEAD pointer is pointing to the newly reset HEAD, do the following $ git reset --hard &lt;commit&gt; $ git reset --soft HEAD@{1} 3.11 Git Branching Branching provides a simple way to maintain multiple, side-by-side versions of the files in a repository. Conceptually, branching a repository creates a copy of the codebase in its current state that you can work on without affecting the primary version from which it was copied. This alows you to work down multiple paths without affecting the main (or other) codebase. To see a list of branches in your repository $ git branch To create a new branch $ git checkout -b hotfix New branches are created of the current working branch. To change branches use $ git checkout &lt;branch name&gt; 3.11.1 Merging Branches When you merge a branch, git folds any changes that you made to files in an identified branch into the current working branch. It also adds any new files. When you perform a merge, a new commit will be automatically created to track the merge. To merge branches, commit any changes to the branch you want to merge (in this example, the ‘hotfix’ branch) then checkout the branch into which you want to merge (for example, master), and then execute a merge command. $ git commit -m &#39;commiting staged files in hotfix branch&#39; $ git checkout master $ git merge hotfix 3.11.2 Branching Workflows There are as many different branching workflows as there are development teams and projects. However, over the years something approximating an “industry standard” has evolved as follows: The “master” or “primary” branch is typically reserved for the current, live and in production version of the codebase. The “development” or “dev” branch holds the current, combined, working version of the code. “topic” branches are created on-the-fly by individuals and are focused on particular coding efforts, one each for each development task. For example, let’s consider a case where there is a team maintaining and developing a company website. In this case, the “master” branch would contain the version of the code that is currently deployed on the live webserver. The “dev” branch would contain a testable version of the code that reflects completed changes to the site made by all team members that have yet to be deployed. Finally, the repository would also contain many topic branches, each of which holds code related to a particular change that was or is being worked on. For exmaple, a team developing a new widget for visualizing data some area of the site would create a suitably named topic branch (somehting like “viz_widget”) for this topic and do all their initial coding in this branch. Once they have completed and tested their code in this branch, they would merge it into the “dev” branch. The new code can then be vidwed and tested by others as part of the “dev” branch. Once all topics branches for planned features for the next release of the website have been merged to “dev” and “dev” has been thoroughly testes (and fixed as necessary), “dev” is then merged into “master” and the “master” branch is then deployed to the live webserver. "],["working-with-remote-repositories.html", "4 Working with Remote Repositories 4.1 GitHub Basics 4.2 Basic GitHub Account Setup 4.3 GitHub Desktop, or the Command Line? 4.4 Sync with GitHub 4.5 Cloning a Repository", " 4 Working with Remote Repositories One of the advantages of working with a version control system like Git is the ability to maintain and sync repositories across multiple computers and users. While there a variety of available, internet accesible remote repository hosting options, in this course, we will work with the Github platform. 4.1 GitHub Basics At its simplest, GitHub is a hosting service for Git repositories. Much like Dropbox or Google Drive, it gives you a space to remotely store your code and related files. This can be useful when working on projects that require, for example, some kind of server, whether for the purposes of running large, potentially time-consuming data analyses or for serving up public-facing content (like a website). For such projects, GitHub acts as a reference point with which you can add, or push, changes on one computer and bring them down, or pull them, onto another. The process would look something like the following, where pushing and pulling from a remote branch entails keeping a reference point for a project that you’re developing locally: Image source. With this diagram in mind, it’s not much of a conceptual leap to imagine how two or more people could work from the same remote repository. Each would pull that repository onto their respective local computers, make a branch, implement their changes, and push those changes back to the remote source. That way, multiple parts of a project could be under development simultaneously, and any such changes made to that project would be trackable according to the logic of version control. Simultaneously pushing and pulling on multiple computers would look something like the following: Image source. 4.1.1 Communicating Through GitHub What makes GitHub special is the fact that, more than being simply a place to store files, the service is above all a communication channel. Where GitHub extends the functionality of version control is not just where it offers various forms of cloud hosting; it is also where GitHub provides tools that let people talk about the code they’re working on. It’s a place where team members can propose and explain the changes they make, look at changes others have made, track and discuss any bugs that might come up, get feedback from others, and plan for any future changes the team intends to make. Learning how to use GitHub, then, is as much about learning how to communicate effectively through the different facets of the service as it is about acquainting yourself with new technical skills (i.e., using your computer to track code remotely). A short summary of the different facets of communication GitHub provides includes: Documentation, often through README files Issue tracking for bug reporting and assigning tasks Pull requests for proposing and discussing changes Wikis, which may feature additional documentation, tutorials, etc. Project boards for long-term planning Various graph visualizations for project overview Additionally, GitHub users can monitor and modify other projects’ code using “Watch”, “Star”, and “Fork” functionalities. The service also provides teams with the ability to specify licensing information for their projects. 4.1.2 What Should I Push to GitHub? A quick word about what should and shouldn’t be pushed to a remote repository, especially with an eye toward what we’ve said about communication. You can, of course, host large data files on GitHub, but there are a few caveats. For one, the site does have a storage limit, and it can also become quite inefficient to have team members constantly push/pull large files to/from GitHub. Further, hosting data files might not be particularly relevant to what a team might need to discuss. Data may change often over the course of a project, but tracking individual observations might not be necessary—more meaningful would be a conversation about how code has made, or might make, such changes. The latter is likely to be something that GitHub is better suited to facilitate. It’s best, then, to host your data files separately from GitHub, either by way of a remote database or some kind of cloud service like Google Drive. Exceptions may come up, however, so the decision about what to track should ultimately be one made by the team. Examples of what should be tracked with GitHub: Code Documentation Make files Some supporting media (small images, for example) Finally, note that even though you can set a repository to either “Public” or “Private” (which controls who can see your project), it’s recommended that you refrain from uploading various access credentials (API keys, database passwords, etc.) to GitHub. 4.2 Basic GitHub Account Setup To use GitHub, you need to make a (free) account. You can do so by going to github.com. Once you’re there, click “Sign Up” in the top-right corner of the page. This should take you to a form, which asks you to enter a username, email address, and password. After you’ve entered in this information (and completed a quick CAPTCHA), GitHub will make you an account. Then, the site will prompt you to complete an optional survey. Fill it out, or scroll to the bottom to skip it. Either way, you’ll need to then verify your email address. Go to your inbox and look for an email from GitHub. Click the “Verify email address” button. Doing so will take you to your homepage, where, if you’d like, you can add a few details about yourself. You now have a GitHub account! 4.2.1 Locally Setting Up Your Git Credentials Regardless of how you make your commits, you will need to use the command line to provide Git with some information about who will be making commits. You may have already done this, however (and sometimes your computer does it automatically). To check, enter the following two commands in either Terminal (Mac) or Git Bash (Windows): git config --global user.name git config --global user.email If you see your name (or some kind of username) and your email after entering the above commands, you’re set. If nothing happens when you type them, you’ll need to provide this information with the following: git config --global user.name &quot;&lt;your name&gt;&quot; git config --global user.email &quot;&lt;your email&gt;&quot; You can check whether this was successful by simply calling either, or both, of the first two commands. They should echo back the information you’ve just entered. 4.2.2 SSH Keys and GitHub When you work with remote repositories on GitHub, you’ll often need to enter your username/password to identify yourself. This is for two reasons: 1) it allows GitHub to track who has made changes to what files; 2) it adds a layer of security to projects, letting teams control who can make changes to their files. Repositories can be either public or private, and this layer of security helps teams control who has access to files in the first place. It can be a pain, though, to have to enter and re-enter your credentials when making changes. More, passwords can be lost or worse, stolen. To avoid these problems, we can set up an SSH key. SSH keys (short for “Secure Shell”) are special, machine-readable credentials that allow users to safely connect and authenticate with remote servers over unsecure networks. An SSH key has two parts: A public key, which encrypts messages intended for a particular recipient. This can be stored on remote servers, or even shared with others, to facilitate secure data transfers A private key, which deciphers messages encrypted by the public key. Your private key is the only thing capable of unlocking what is sent with your public key. It stays on your computer and should never be shared with anyone Beyond what security measures an SSH key brings, it also acts as your digital signature. GitHub uses this internally to verify that you are, in fact, who you say you are when you commit code to a repository. 4.2.3 Connecting to GitHub with SSH GitHub offers thorough, straightforward documentation for setting up an SSH key with its services, which we won’t repeat here. Instead, please visit the link below and follow the step-by-step instructions there to get yourself set up with a key. Connecting to GitHub with SSH The following steps at the link above are required: Checking for existing SSH keys Generating a new SSH key and adding it to the ssh-agent Adding a new SSH key to your GitHub account Testing your SSH connection Once you have completed these steps, be sure you can successfully run the following command: ssh -T git@github.com If your connection is successful, you will see this message (a warning may first appear—see the documentation on GitHub for more information): Hi &lt;your username&gt;! You&#39;ve successfully authenticated, but GitHub does not provide shell access. 4.3 GitHub Desktop, or the Command Line? Remember that Git is separate from GitHub. The latter is a service that’s been built around the former. One part of the services that GitHub offers is an application called GitHub Desktop, which allows users to manage their local repositories with a point-and-click graphical user interface (or GUI). Ultimately, it’s a matter of preference whether you use the GUI or stick with the command line for your own projects, but it is generally a good idea to become proficent at interacting with GitHub via the command line. One of the primary reasons for this has to do with the fact that not every computer you use will have GitHub’s GUI installed—or even have a screen! Many remote servers offer command line-only access, and if you ever want to sync your files with these machines, you’ll need to do so without GitHub Desktop. Luckily, GitHub seamlessly extends Git commands, so using the service without the GUI is, as we’ll see, quite straightforward. 4.4 Sync with GitHub Now that you’re all set up with GitHub, it’s time to sync the website with a local repository on your computer. We’ll start by creating a test repository on your local Git intance. First, use the command line to make a new directory in your Home folder: mkdir ~/my_first_remote_directory Put this directory under version control with Git: cd ~/my_first_remote_directory git init With Vim, make a README markdown file: vim README.md Write and save “Hello world!” in the file. You should see something like the following: Exit Vim. Then, add README.md to Git and commit your changes. Don’t forget to write a short note in the commit message. git add README.md git commit -m &#39;Add a README file&#39; You should see the following: 4.4.1 Preparing to Sync Your Repository So far so good! All we’ve done is repeat the normal workflow for putting files under version control. But now we need to step away from the command line for a moment and prepare a space for receiving this repository on GitHub. To do so, go to github.com and, on your homepage, click the “Create repository” button. You’ll be taken to this page: There are a few things of note here: Repository name: your repository’s name, which should be the same as what’s on your computer Description: a short (1-2 sentence) explanation of what’s in this repository Public/private setting: repositories may be either “public” (viewable by anyone) or “private” (only viewable by you and those to whom you grant access) Initialize with details, including: A README file: a form of documentation; provides information about the files in the repository A .gitignore file: instructs Git to ignore specific files or filetypes A license: governs the use or redistribution of your files Because we’re initializing this repository from an existing directory, we won’t bother with most of the extra details. But we do need a title, which should be the same as what’s on your local computer (“my_first_remote_directory”). A description is helpful but not necessary for our purposes; the same goes for a license. Finally, we will choose to make this a public repository (meaning anyone can see it). 4.4.2 Pushing a Local Repository Once you’ve entered the above information, click “Create repository.” GitHub will take you to a new screen, which gives you a number of options for making or uploading new files to the repository. Since we already have a repository made, we need to use the “Push an existing repository from the command line.” Pushing our repository is as easy as sequentially entering into the command line the three commands GitHub provides. git remote add origin git@github.com:&lt;your user account&gt;/my_first_remote_directory.git git branch -M main git push -u origin On the command line, that looks like this: To summarize the above, we’ve done the following: Associated GitHub’s remote repository with our local repository (git remote etc.) Made a new branch in our local repository called “main” (git branch -M main) Pushed the contents of main (from origin) to a new, corresponding remote branch on GitHub From here on out, when you want to update the remote repository with further changes, you can simply use the shorthand git push after the usual save, add, commit steps. Importantly, Git will only update the branch you’re on when you enter git push, so before making any pushes, it’s a good idea to run a quick git status command to make sure you’re on the branch you want to be on. When you make your changes, the GitHub site won’t immediately refresh itself, but if you click on the “&lt; &gt; Code” tab or on the name of the directory, you’ll see that the repository has been synced and your README.md file is now online. Note that GitHub automatically looks for a README file in your repository. If it finds one that contains renderable markdown code, it will render the file on your repository’s main page. (More information about writing effective README files is available through the DataLab’s data documentation workshop.) 4.4.3 Tracking Files Remotely With this repository made, GitHub can start tracking changes you make to your files, much as Git does locally. The process works exactly like the one you do for Git, though it requires one more step. First, we’ll alter our README.md. Reopen the file with vim, skip a line down from the line you’ve already written, and add “My name is .” Save and quit. Then, add the file and commit your changes. If you want to push these changes to your remote repository, simply enter git push. You’ll see a similar message appear about enumerating, counting, and writing objects to GitHub. Afterwards, if you refresh your file on GitHub, you should see your changes: Note that your commit message appears here as well: If you click the commit tag: You’ll be taken to another page, which shows you the differences between your old version and the new one: 4.4.4 Pulling Changes from a Remote Repository Before moving on, it’s also worth noting that we can pull changes directly from GitHub. If a file has been altered on the remote version of a project, GitHub offers functionality for syncing that file with your local copy (or creating a new file altogether, if need be). For example, if you return to the main page of “my_first_remote_directory”, you can alter the README directly on GitHub. Click the pencil in the right-hand corner of the rendered file. This will open up a text editor interface. Using it, add “What’s yours?” on the fifth line of the document. The complete document should look like this: Hello world! My name is &lt;your name&gt; What&#39;s yours? Scroll to the bottom and click the green “Commit changes” button. This is the equivalent of doing git add &lt;file&gt; and git commit -m &lt;message&gt; on the command line. You’ll see something like the following: Back on the command line, if you type git status, you’ll see that your local repository is now out of sync. If you haven’t made any changes to your directory, syncing it with the remote version can be achieved with a straighforward pull command: git pull Once you enter this command, your command line should look something like this: Your files are now synced. A later portion of this reader will discuss how to handle this process when you have made changes to your directory between the time the remote was altered and the time you go to make a pull. 4.5 Cloning a Repository While tracking your own files remotely with GitHub is great for managing and storing your files, this doesn’t quite tap into the full use of the service. Remember, GitHub is above all a communication channel, in which people can share and discuss the code/files they’re working on. We haven’t yet taken advantage of much of what makes GitHub useful: getting files for a project, modifying them, discussing the changes with team members, and implementing those changes. 4.5.1 How to Clone a Repository To start using GitHub collaboratively, we need to retrieve, or clone, a repository. This will create a local copy of project files. First, go back to your Home directory. You’ll be putting a repository here (in command line speak, the repository will be a “child” of Home). cd ~ Then, go to the following link: https://github.com/ucdavis-datalab-training/workshop_git_for_teams_sandbox Once there, click on the green “Code” button, which should show the following: Since you have SSH keys, select the “SSH” option. Copy the text GitHub provides to your clipboard. Then, in the command line, type git clone, add a space, and paste in the line of text GitHub generated for you. The full command should look like this: git clone git@github.com:ucdavis-datalab-training/workshop_git_for_teams_sandbox.git Hit “Enter”. If you’d like, you can use ls to see the newly made directory. You should see something like the following: If you cd into the directory and then type ls -a, you’ll see a README.md file and a .git file, which contains all the logging info for the repository. "],["introduction-to-r.html", "5 Introduction to R 5.1 Learning objectives 5.2 Before We Start 5.3 Mathematical Operations 5.4 Variables 5.5 Calling Functions 5.6 HELP! 5.7 Vectors 5.8 Data Frames 5.9 Data Types &amp; Classes", " 5 Introduction to R 5.1 Learning objectives After this lecture, you should be able to: define reproducible research and the role of programming languages explain what R and RStudio are, how they relate to eachother, and identify the purpose of the different RStudio panes create and save a script file for later use; use comments to annotate solve simple mathematical operations in R create variables and dataframes inspect the contents of vectors in R and manipulate their content identify the data type and class of an object and vector subset and extract values from vectors use the help function 5.2 Before We Start What is R and RStudio? “R” is both a free and open source programming language designed for statistical computing and graphics, and the software for interpreting the code written in the R language. RStudio is an integrative development environment (IDE) within which you can write and execute code, and interact with the R software. It’s an interface for working with the R software that allows you to see your code, plots, variables, etc. all on one screen. This functionality can help you work with R, connect it with other tools, and manage your workspace and projects. You don’t need RStudio to use R, but many people find that using RStudio makes writing, editing, searching and running their code easier. You cannot run RStudio without having R installed. While RStudio is a commercial product, the free version is sufficient for most researchers. You can download R for free here. You can download RStudio Desktop Open-Source Edition for free here. Why learn R? There are many advantages to working with R. Scientific integrity. Working with a scripting language like R facilitates reproducible research. Having the commands for an analysis captured in code promotes transparency and reproducibility. Someone using your code and data should be able to exactly reproduce your analyses. An increasing number of research journals not only encourage, but are beginning to require, submission of code along with a manuscript. Many data types and sizes. R was designed for statistical computing and thus incorporates many data structures and types to facilitate analyses. It can also connect to local and cloud databases. Graphics. R has buit-in plotting functionalities that allow you to adjust any aspect of your graph to effectively tell the story of your data. Open and cross-platform. Because R is free, open-source software that works across many different operating systems, anyone can inspect the source code, and report and fix bugs. It is supported by a large community of users and developers. Interdisciplinary and extensible. Because anyone can write and share R packages, it provides a framework for integrating approaches across domains, encouraging innovation. Navigating the interface The first time you open RStudio, you’ll see a window divided into several panes, like this: The exact presentation of the panes might be slightly different depending on your operating system, versions of R and RStudio, and any set preferences. Generally, the panes include: Source is your script. You can write your code here and save this as a .R file and re-run to reproduce your results. Console is where you run the code. You can type directly here, but anything entered here won’t be saved when you exit RStudio. Environment/history lists all the objects you have created (including your data) and the commands you have run. Files/plots/packages/help/viewer pane is useful for locating files on your machine to read into R, inspecting any graphics you create, seeing a list of available packages, and getting help. To interact with R, compose your code in the source pane and use the execute (or run) command to send them to the console. (Shortcuts: You can use the shortcut Ctrl + Enter, or Cmd + Return, to run a line of code.) Create a script file for today’s lecture and save it to your lecture_3 folder under ist008_2022 in your home directory. (It’s good practice to keep your projects organized., Some suggested sub-folders for a research project might be: data, documents, scripts, and, depending on your needs, other relevant outputs or products such as figures. 5.3 Mathematical Operations R works by the process of “REPL”: Read-Evaluate-Print-Loop: R waits for you to type an expression (a single piece of code) and press Enter. R then reads in your expressions and parses them. It reads whether the expression is syntactically correct. If so, it will then evaluate the code to compute a result. R then prints the result in the console and loops back around to wait for your next expression. You can use R like a calculator to see how it processes expressions. 7 + 2 R always puts the result on a separate line (or lines) from your code. In this case, the result begins with the tag [1], which is a hint from R that the result is a vector and that this line starts with the element at position 1. We’ll learn more about vectors later in this lesson, and eventually learn about other data types that are displayed differently. If you enter an incomplete expression, R will get stuck at the evaluate step and change the prompt to +, then wait for you to type the rest of the expression and press the Enter key. If this happens, you can finish entering the expression on the new line, or you can cancel it by pressing the Esc key (or Ctrl-c if you’re using R without RStudio). R can only tell an expression is incomplete if it’s missing something that it is expecting, like the second operand here: 7 - ## Error: &lt;text&gt;:2:0: unexpected end of input ## 1: 7 - ## ^ Let’s do more math! Other arithmetic operators are: - for subtraction * for multiplication / for division %% for remainder division (modulo) ^ or ** for exponentiation 7 - 2 244/12 2 * 12 Arithmetic in R follows an order of operations (aka PEMDAS): parenthesis, exponents, multiplication and division, addition and subtraction. You can combine these and use parentheses to make more complicated expressions, just as you would when writing a mathematical expression. For example, to estimate the area of a circle with radius 3, you can write: 3.14 * 3^2 ## [1] 28.26 To see the complete order of operations, use the help command: ?Syntax You can also perform other operations in R: 3 &gt; 1 3 &lt; 1 Tip: You can write R expressions with any number of spaces (including none) around the operators and R will still compute the result. Nevertheless, putting spaces in your code makes it easier for you and others to read, so it’s good to make it a habit. Put spaces around most operators, after commas, and after keywords. 5.4 Variables Since R is designed for mathematics and statistics, you might expect that it provides a better appoximation for \\(\\pi\\) than 3.14. R and most other programming languages allow you to create and store values called variables. Variables allow you to reuse the result of a computation, write general expressions (such as a*x + b), and break up your code into smaller steps so it’s easier to test and understand. R has a built in variable called ‘pi’ for the value of \\(\\pi\\). You can display a variable’s value by entering its name in the console: pi You can also use variables in mathematical expressions. Here’s a more precise calculation of the area of a circle with radius 3: pi *3^2 You can define your own variables with the assignment operator ‘=’ or ‘&lt;-’. Variable names can contain letters, numbers, dots ., and underscores _, but they cannot begin with a number. Spaces and other symbols are not allowed in variable names. In general, variable names should be descriptive but concise, and should not use the same name as common (base R) functions like mean, T, median, sum, etc.. Let’s make some more variables: x &lt;- 10 y &lt;- 24 fantastic.variable2 = x x &lt;- y / 2 In R, variables are copy-on-write. When we change a variable (a “write”), R automatically copies the original value so dependent variables are unchanged until they are re-run. x &lt;- 13 y &lt;- x x &lt;- 16 y Why do you think copy-on-write is helpful? Where do you think it could trip you up? 5.5 Calling Functions R has many functions (reusable commands) built-in that allow you to compute mathematical operations, statistics, and perform other computing tasks on your variables. You can think of a function as a machine that takes some inputs and uses them to produce some output. Code that uses a function is said to call that function. When you call a function, the values that you assign as input are called arguments. The output is called the return value. We call a function by writing its name followed by a parentheses containing the arguments. log(10) sqrt(9) Many functions have multiple parameters and can accept multiple arguments. For example, the sum function accepts any number of arguments and adds them all together. When you call a function with multiple arguments, separate the arguments with commas. sum(5, 4, 1) When you call a function, R assigns each argument to a parameter. Parameters are special variables that represent the inputs to a function and only exist while that function runs. For example, the log function, which computes a logarithm, has parameters x and base for the operand and base of the logaritm, respectively. By default, R assigns arguments to parameters based on their order. The first argument is assigned to the function’s first parameter, the second to the second, and so on. If you know the order that a function expects to receive the parameters then you can list them separated by commas. Here the argument 64 is assigned to the parameter x, and the argument 2 is assigned to the parameter base. log(64, 2) You can also assign arguments to parameters by name with = (but not with &lt;-), overriding their positions. log(64, base = 2) log(base = 2, x= 64) Tip: Both of these expressions are equivalent, so which one should you use? When you write code, choose whatever seems the clearest to you. Leaving parameter names out of calls saves typing, but including some or all of them can make the code easier to understand. Not sure what parameters a specific function needs? Read on for how to get… 5.6 HELP! This is just the beginning, and there are lots of resources to help you learn more. R has built-in help files that can be accessed with the ? and help commands. You can also search within the help documentation using the ?? commands. There’s also a vibrant online help community. Here are some examples of how you can use all this help to help yourself: ? The help pages for all of R’s built-in functions usually have the same name as the function itself. Function help pages usually include a brief description, a list of parameters, a description of the return value, and some examples. To open the help page for the log function: ?log There are also help pages for other topics, such as built-in mathematical constants (such as ?pi), data sets (such as ?cars), and operators. To look up the help page for an operator, put the operator’s name in single or double quotes. For example, this code opens the help page for the arithmetic operators: ?&quot;+&quot; Tip: It’s always okay to put single or double quotes around the name of the page when you use ?, but they’re only required if it contains arithmetic commands or non-alphabetic characters. So ?sqrt, ?'sqrt', and ?\"sqrt\" all open the documentation for sqrt, the square root function. Why does this work? R treats anything inside single or double quotes as literal text rather than as an expression to evaluate. In programming jargon, a piece of literal text is called a string. You can use whichever kind of quotes you prefer, but the quote at the beginning of the string must match the quote at the end. We’ll learn more about strings in later lessons when we cover working with unstructured data. ?? Sometimes you might not know the name of the help page you want to look up. You can do a general search of R’s help pages with ?? followed by a string of search terms. For example, to get a list of all help pages related to linear models: ??&quot;linear model&quot; This search function doesn’t always work well, and it’s often more efficient to use an online search engine. When you search for help with R online, include “R” as a search term. Alternatively, you can use RSeek, which restricts the search to a selection of R-related websites. In later lessons we’ll learn about packages, which are sharable bundles of code. You’ll often need to look up the documentation to get help figuring out how to work with a new package. You can view a package’s help documentation using packageDescription(\"Name\"). 5.6.1 When Something Goes Wrong (and it will) Sooner or later you’ll run some code and get an error message or result you didn’t expect. Don’t panic! Even experienced programmers make mistakes regularly, so learning how to diagnose and fix problems is vital. We call this troubleshooting or debugging. Stay calm and try going through these steps: If R returned a warning or error message, read it! If you’re not sure what the message means, try searching for it online. Check your code for typ0s. Did you capitalize something that should be lower case? Are you missing or have an extra comma, quote, parenthesis? Test your code one line at a time, starting from the beginning. After each line that assigns a variable, check that the value of the variable is what you expect. Try to determine the exact line where the problem originates (which may differ from the line that emits an error!). Sometimes the “shut it down and restart” trick really works - you might have created a variable and forgot about it, and you need a fresh start for the code to work as intended. If all else fails, just Google it. Stack Overflow is a popular question and answer website and you can often find solutions to your problems there, or pick up tips to help you tackle your problem in a new way. On CRAN, check out the Intro to R Manual and R FAQ. Many regions also have grassroots R-Users Groups that you can join and ask for help. Just remember to pay it forward and use your newfound R prowess to help others in the community on their learning journies! Tip: When asking for help, clearly state the problem and provide a reproducible example. R also has a posting guide to help you write questions that are more likely to get a helpful reply. It’s also a good idea to save your sessionInfo() so you can show others how your machine and session was configured. Doing this before coming to office hours for a programming class is also highly recommended! 5.7 Vectors A vector is an ordered collection of values. The values in a vector are called elements. Vectors can have any number of elements, including 0 or 1 element. For example, a single value, like 3, is a vector with 1 element. So every value that you’ve worked with in R so far was a vector. The elements of a vector must all be the same type of data (we say the elements are homogeneous). A vector can contain integers, decimal numbers, strings (text), or several other types of data, but not a mix these all at once. You can combine or concatenate vectors to create a longer vector with the c function: # numbers time.min &lt;- c(5, 4, 4, 12, 10, 2, 3, 4, 4, 5, 19) # strings pets &lt;- c(&quot;woof&quot;, &quot;woof&quot;, &quot;cat&quot;, &quot;woof&quot;, &quot;woof&quot;, &quot;cat&quot;, &quot;woof&quot;, &quot;woof&quot;, &quot;woof&quot;, &quot;woof&quot;, &quot;woof&quot;) place &lt;- c(&quot;Temple&quot;, &quot;Yakitori&quot;, &quot;Panera&quot;, &quot;Yakitori&quot;, &quot;Guads&quot;, &quot;Home&quot;, &quot;Tea List&quot;, &quot;Raising Canes&quot;, &quot;Pachamama&quot;, &quot;Lazi Cow&quot;, &quot;Wok of Flame&quot;) You can check the length of a vector (and other objects) with the length function: length(3) ## [1] 1 length(&quot;hello&quot;) ## [1] 1 length(time.min) ## [1] 11 length(pets) ## [1] 11 5.7.1 Indexing Vectors Vectors are ordered, which just means the elements have specific positions. For example, in the place vector, the value of the 1st element is \"Temple\", the 2nd is \"Yakitori\", the 5th is \"Guads\", and so on. You can access individual elements of a vector with the indexing operator [ (also called the square bracket operator). The way to write it, or syntax is: VECTOR[INDEXES] Here INDEXES is a vector of positions of elements you want to get or set. For example, let’s get the 2nd element of the place vector: place[2] ## [1] &quot;Yakitori&quot; Now let’s get the 3rd and 1st element: place[c(3, 1)] ## [1] &quot;Panera&quot; &quot;Temple&quot; Indexing is among the most frequently used operations in R, so take some time to try it out with few different vectors and indexes. We’ll revisit indexing in a later lesson to learn a lot more about it. 5.7.2 Vectorization Let’s look at what happens if we call a mathematical function, like sqrt, on a vector: x &lt;- c(2, 16, 4, 7) sqrt(x) ## [1] 1.414214 4.000000 2.000000 2.645751 This gives us the same result as if we had called the function separately on each element. That is, the result is the same as: c(sqrt(2), sqrt(16), sqrt(4), sqrt(7)) ## [1] 1.414214 4.000000 2.000000 2.645751 Of course, the first version is much easier to type. Functions that take a vector argument and get applied element-by-element are called vectorized functions. Most functions in R are vectorized, especially math functions. Some examples include sin, cos, tan, log, exp, and sqrt. A function can be vectorized across multiple arguments. This is easiest to understand in terms of the arithmetic operators. Let’s see what happens if we add two vectors together: x = c(1, 2, 3, 4) y = c(-1, 7, 10, -10) x + y ## [1] 0 9 13 -6 The elements are paired up and added according to their positions. The other arithmetic operators are also vectorized: x - y ## [1] 2 -5 -7 14 x * y ## [1] -1 14 30 -40 x / y ## [1] -1.0000000 0.2857143 0.3000000 -0.4000000 Functions that are not vectorized tend to be ones that combine or aggregate values in some way. For instance, the sum, length, and class functions are not vectorized. 5.8 Data Frames We frequently work with 2-dimensional tables of data (also called tabular data). Typically each row corresponds to a single subject and is called an observation. Each column corresponds to a measurement of the subject. In data science, the columns of a table are called features or covariates. Sometimes people also refer to them as “variables”, but that can be confusing as “variable” means something else in R, so here we’ll try to avoid that term. R’s structure for tabular data is the data frame. The columns are vectors, so the elements within a column must all be the same type of data. In a data frame, every column must have the same number of elements (so the table is rectangular). There are no restrictions on the data types in each row. You can use the data.frame function to create a data frame from column vectors: # current data (vectors) time.min place pets # create new data (vectors) distance.mi &lt;- c(0.9, 0.6, 0.8, 0.6, 2, 100, 0.6, 0.7, 0.8, 1, 3.7) major &lt;- c(&quot;chicanix studies&quot;, &quot;human development&quot;, &quot;economics&quot;, &quot;undeclared&quot;, &quot;psychology&quot;, &quot;MMM&quot;, &quot;psychology&quot;, &quot;undeclared&quot;, &quot;human development&quot;, &quot;undeclared&quot;, &quot;GG&quot;) # combine vectors into dataframe my.data &lt;- data.frame(place, distance.mi, time.min, major, pets) 5.8.1 Selecting Columns You can select an individual column from a data frame by name with $, the dollar sign operator. The syntax is: VARIABLE$COLUMN_NAME For instance, to select the place column: my.data$place ## [1] &quot;Temple&quot; &quot;Yakitori&quot; &quot;Panera&quot; &quot;Yakitori&quot; ## [5] &quot;Guads&quot; &quot;Home&quot; &quot;Tea List&quot; &quot;Raising Canes&quot; ## [9] &quot;Pachamama&quot; &quot;Lazi Cow&quot; &quot;Wok of Flame&quot; The selected column is just a vector, so you can assign it to a variable and use it in functions. For example, to compute the sum of the distance.mi column: sum(my.data$distance.mi) ## [1] 111.7 Preview of future lesson content: What if you want to extract a row from your data frame? For example, to pull out all responses from only the 11th row, you would subset it: my.data[11, ] 5.8.2 Inspecting Data You can print a small dataset, but it can be slow and hard to read especially if there are a lot of coumns. R has many built in functions to inspect objects: head(my.data) # this prints only the beginning of the dataset tail(my.data) # this prints only the end of the dataset nrow(my.data) # number of rows ncol(my.data) # number of columns ls(my.data) # lists the names of the columns alphabetically names(my.data) # lists the names of the columns as they appear in the data frame rownames(my.data) # names of the rows A highly informative function for inspecting the structure of a data frame or other object is str: str(my.data) The table function is another useful function for inspecting data. The table function computes the frequency of each unique value in a vector. For instance, you can use table to compute how many entries in the pets column are woof: table(my.data$pets) ## ## cat woof ## 2 9 Additional reading: Check out this DataLab workshop on “Keeping Your Data Tidy” that covers best practices for structuring, naming, and organizing your data frames (and spreadsheets). 5.9 Data Types &amp; Classes Data can be categorized into different types based on sets of shared characteristics. For instance, statisticians tend to think about whether data are numeric or categorical: numeric continuous (real or complex numbers) discrete (integers) categorical nominal (categories with no ordering) ordinal (categories with some ordering) Of course, other types of data, like graphs (networks) and natural language (books, speech, and so on), are also possible. Categorizing data this way is useful for reasoning about which methods to apply to which data. In R, data objects are categorized in two different ways: The class of an R object describes what the object does, or the role that it plays. Sometimes objects can do more than one thing, so objects can have more than one class. The class function returns the classes of its argument. The type of an R object describes what the object is. Technically, the type corresponds to how the object is stored in your computer’s memory. Each object has exactly one type. The typeof function returns the type of its argument. Of the two, classes tend to be more important than types. If you aren’t sure what an object is, checking its classes should be the first thing you do. The built-in classes you’ll use all the time correspond to vectors and lists (which we’ll learn more about in Section 5.9.1): Class Example Description logical TRUE, FALSE Logical (or Boolean) values integer -1L, 1L, 2L Integer numbers numeric -2.1, 7, 34.2 Real numbers complex 3-2i, -8+0i Complex numbers character \"hi\", \"YAY\" Text strings list list(TRUE, 1, \"hi\") Ordered collection of heterogeneous elements The class of a vector is the same as the class of its elements: class(&quot;hi&quot;) ## [1] &quot;character&quot; class(c(&quot;hello&quot;, &quot;hi&quot;)) ## [1] &quot;character&quot; In addition, for ordinary vectors, the class and the type are the same: x &lt;- c(TRUE, FALSE) class(x) ## [1] &quot;logical&quot; typeof(x) ## [1] &quot;logical&quot; The exception to this rule is numeric vectors, which have type double for historical reasons: class(pi) ## [1] &quot;numeric&quot; typeof(pi) ## [1] &quot;double&quot; typeof(3) ## [1] &quot;double&quot; The word “double” here stands for double-precision floating point number, a standard way to represent real numbers on computers. By default, R assumes any numbers you enter in code are numeric, even if they’re integer-valued. The class integer also represents integer numbers, but it’s not used as often as numeric. A few functions, such as the sequence operator : and the length function, return integers. The difference between numeric and integer is generally not important. class(3) ## [1] &quot;numeric&quot; class(length(pets)) ## [1] &quot;integer&quot; class(1:3) ## [1] &quot;integer&quot; Besides the classes for vectors and lists, there are several built-in classes that represent more sophisticated data structures: Class Description function Functions factor Categorical values matrix Two-dimensional ordered collection of homogeneous elements array Multi-dimensional ordered collection of homogeneous elements data.frame Data frames For these, the class is usually different from the type. We’ll learn more about most of these later on. 5.9.1 Lists A list is an ordered data structure where the elements can have different types (they are heterogeneous). This differs from a vector, where the elements all have to have the same type, as we saw in Section 5.7. The tradeoff is that most vectorized functions do not work with lists. You can make an ordinary list with the list function: x &lt;- list(1, c(&quot;hi&quot;, &quot;bye&quot;)) class(x) ## [1] &quot;list&quot; typeof(x) ## [1] &quot;list&quot; For ordinary lists, the type and the class are both list. In a later lesson, we’ll learn how to get and set list elements, and more about when and why to use lists. You’ve already seen one list, the my.data data frame: class(my.data) ## [1] &quot;data.frame&quot; typeof(my.data) ## [1] &quot;list&quot; Under the hood, data frames are lists, and each column is a list element. Because the class is data.frame rather than list, R treats data frames differently from ordinary lists. For example, R prints data frames differently from ordinary lists. 5.9.2 Implicit Coercion R’s types fall into a natural hierarchy of expressiveness: Each type on the right is more expressive than the ones to its left. For example, with the convention that FALSE is 0 and TRUE is 1, we can represent any logical value as an integer. In turn, we can represent any integer as a double, and any double as a complex number. By writing the number out, we can also represent any complex number as a string. The point is that no information is lost as we follow the arrows from left to right along the types in the hierarchy. In fact, R will automatically and silently convert from types on the left to types on the right as needed. This is called implicit coercion. As an example, consider what happens if we add a logical value to a number: TRUE + 2 ## [1] 3 R automatically converts the TRUE to the numeric value 1, and then carries out the arithmetic as usual. We’ve already seen implicit coercion at work once before, when we learned the c function. Since the elements of a vector all have to have the same type, if you pass several different types to c, then R tries to use implicit coercion to make them the same: x &lt;- c(TRUE, &quot;hi&quot;, 1, 1+3i) class(x) ## [1] &quot;character&quot; x ## [1] &quot;TRUE&quot; &quot;hi&quot; &quot;1&quot; &quot;1+3i&quot; Implicit coercion is strictly one-way; it never occurs in the other direction. If you want to coerce a type on the right to one on the left, you can do it explicitly with one of the as.TYPE functions. For instance, the as.numeric (or as.double) function coerces to numeric: as.numeric(&quot;3.1&quot;) ## [1] 3.1 There are a few types that fall outside of the hierarchy entirely, like functions. Implicit coercion doesn’t apply to these. If you try to use these types where it doesn’t make sense to, R generally returns an error: sin + 3 ## Error in sin + 3: non-numeric argument to binary operator If you try to use these types as elements of a vector, you get back a list instead: x &lt;- c(1, 2, sum) class(x) ## [1] &quot;list&quot; Understanding how implicit coercion works will help you avoid bugs, and can also be a time-saver. "],["control-structures.html", "6 Control Structures 6.1 If Statement 6.2 Relationship Operators 6.3 If Else Statement 6.4 ifelse Statement 6.5 The switch Statement 6.6 The which Statement", " 6 Control Structures Control Structures are functions in computer programming the evaluate conditions (like, for example, the value of a variable) and change the way code behaves based upon evaluated values. For example, you might to perform one function if the value stored in the variable x is greater than 5 and a different function if it is less than less than 5. The Wikiversit Control Structures page contains a good, general description of control structures that is not programming language specific. The information that follows provides examples of the most frequetly used R control structures and how to implement them. For more complete documentation on control strcutures in R run the following help command: ?Control 6.1 If Statement The “If Statement” is the most basic of the R control structures. It tests whether a particular condition is true. For example, the below statement tests whether the value of the variable x is greater than 5. If it is, the code prints the phrase “Yay!” to screen. If it is not, the code does nothing: x &lt;- 7 if (x &gt; 5) { print(&quot;Yay!&quot;) } Note, the general syntax in the example is: control_statement (condition) { #code to execute condition is true } While you will occasionally see variations in how control structures are present, this is a fairly universal syntax across computer programming languages. The specific control structure being invoked is followed by the condition to be tested. Any actions to be performed if the condition evaluates to TRUE are place between curly brackets {} following the condition. 6.2 Relationship Operators The most common conditions evaluate whether one value is equal to ( x == y), equal to or greater than (x =&gt; y), equal to or lesser than (x &lt;= y), greater than (x &gt; y), or lesser than (x &lt; y) another value. Another common task is to test whether a BOOLEAN value is TRUE or FALSE. The syntax for this evaluation is: if (*x*) { #do something} Control structures in R also have a negation symbol which allows you to specify a negative condition. For example, the conditional statement in the following code evaluates to TRUE (meaning any code placed between the curly brackets will be executed) if the x IS NOT EQUAL to 5: if (x !=5) { #do something} 6.3 If Else Statement The “If Else” statement is similar to the “If Statement,” but it allows you specify one code path to execute if the conditional evaluates to TRUE and another to execute if the conditional evaluates to FALSE: x &lt;- 7 if (x &gt; 5) { print(&quot;Yay!&quot;) } else { print(&quot;Boo!&quot;) } 6.4 ifelse Statement R also offers a combined if/else syntax for quick execution of small code chunks: x &lt;- 12 ifelse(x &lt;= 10, &quot;x less than 10&quot;, &quot;x greater than 10&quot;) 6.5 The switch Statement The switch statement provides a mechanism for selecting between multiple possible conditions. For example, the following code returns one of several possible values from a list based upon the value of a variable: x &lt;- 3 switch(x,&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;) Note: if you pass switch a value that exceeds the number of elements in the list R will not compute a reply. 6.6 The which Statement The which statement is not a true conditional statement, but it provides a very useful way to test the values of a dataset and tell you which elements match a particular condition. In the example below, we load the R IRIS dataset and find out which rows have a Petal.Length greater than 1.4: data(&quot;iris&quot;) rows &lt;- which(iris$Petal.Length &gt; 1.4) note: you can see all of the R. build in datasets with the data() command. "],["iterating-loops.html", "7 Iterating (Loops) 7.1 For i in x Loops 7.2 While Loops 7.3 Repeat Loops 7.4 Break and Next 7.5 Iterating Data.Frame Rows in R 7.6 lapply()", " 7 Iterating (Loops) In computer programming iteration is a specific type of control structure that repeatedly runs a specified operation either for a set numbe of iterations or untul some condition is met. For example, you might want your code to peform the same math operation on all of the numbers stored in a vector of values; or perhaps you want the computer to look through a list until it finds the first entry with a value greater than 10; or, maybe you just want the computer to sound an alarm exactly 5 times. Each of these is a type of iteration or “Loop” as they are also commonly called. 7.1 For i in x Loops The most common type of loop is the “For i in x” loop which interates through each value (i) in a list (x) and does something with each value. For example, assume that x is a vector containing the following four names names: Sue, John, Heather, George, and that we want to print each of these names to screen. We can do so with the followig code: x &lt;- c(&quot;Sue&quot;, &quot;John&quot;, &quot;Heather&quot;, &quot;George&quot;) for (i in x) { print(i) } In the first line of code, we create our vecctor of names (x). Next we begin our “For i in x loop”, which has the following general syntax, which is similar to that of the conditional statements you’ve already mastered: for (condition) {} Beginning with the first element of the vector x, which in our case is “Sue”, for each iteration of the for loop the value of the corresponding element in x is assiged to the variable i and then i can be acted upon in the code icnluded between the curly brackets of the function call. In our case we simply tell the conputer to print the value of i to the sreen. Witgh each iteration, the next value in our vector is assigned to i and is subsequently printed to screen, resulting in the following output: [1] &quot;Sue&quot; [1] &quot;John&quot; [1] &quot;Heather&quot; [1] &quot;George&quot; In addition to acting on vectors or lists, For loops can also be coded to simply execute a chunk of code a designated number of times. For example, the following code will print “Hello World!” to screen exactly 10 times: for (i in 1:10) { print(&quot;Hello World!&quot; } 7.2 While Loops Unlike For loops, which iterate a defined number of times based on the length of a list of range of values provided in the method declaration, While loops continue to iterate infinitely as long as (while) a defined condition is met. For example, assume you have a boolean variable x the value of which is TRUE. You might want to write code that performs some function repeatly until the value of x is switched to FALSE. A good example of this is a case where your program asks the user to enter data, which can then be evaluated for correctness before the you allow the program to move on in its execution. In the example below, we ask the user to tell us the secret of the universe. If the user answeres with the correct answer (42), the code moves on. But if the user provides and incorrect answer, the code iterates back to the beginning of the loop and asks for input again. response &lt;- 0 while (response!=42) { response &lt;- as.integer(readline(prompt=&quot;What is the answer to the Ultimate Question of Life, the Universe, and Everything? &quot;)); } 7.3 Repeat Loops Like While loops, Repeat loops continue to iterate until a specified condition is met; but with Repeat loops that condition is defined not as an argument to the function but is a specific call to “break” that appears in the functions executable code. In the example below we assign the value 1 to a variable i and then loop through code that prints and then iterates the value of i until it reaches 10, at which time we forceably exit the loop: i &lt;- 1 repeat { print(i) i = i+1 if (i &gt; 10){ break } } 7.4 Break and Next In the previous section we saw the use of the break statement to force an exit from a repeat loop based on a conditional evaluation in an if statement. Break can actually be used inside any conditional (for, while, repeat) in order to force the end of iteration. This can be useful in a variety of contexts where you want to test for multiple conditions as a means of stopping iteration. The next command is similar to break in that it can be used inside any iteration structure to force R to skip execution of the iteration code for particular cases only. For example, we use next below to iterate through the nunbers 1 to 10 and print all values to screen EXCEPT the value 5: for (i in 1:10) { if (i == 5){ next } print(i) } 7.5 Iterating Data.Frame Rows in R In the section on for loops above, we learned that you can easily iterate across all values of a list using a “for i in x” loop. Working with R data.frames adds a bit of complexity to this process. Because R was developed as a language for statistial analysis, which always involves the comparison of multiple observations of the same variable (for example, all of the weights recroded across all patients), the default behavior of the “for i in x” loop when applied to data.frames is to iterate across columns (variables) rather than rows (observations). Consider the following example: for (i in iris) { print(i) } If you run the above code, in the first iteration R will assign the vector of values contained in the firt column (Sepal.Length) to i, in the second iteration it will assign vectore of values contained in the second column (Sepal.Width) to i, etc. Iterating through the data columns of a data.frame is useful for many (if not most) operations. However, there are time when we want to iterate through data one observation at a time. To accomplish this, we nee do specifically direct R to move through the data.frame by row, as follows: for (i in 1:nrow(iris)) { thisrow &lt;- iris[i,] print(thisrow) } 7.6 lapply() R has a built-in class of functions known as the apply family that provide a shorthand for iterating through collections of data. These behave like a for loop, but require much less actual code to accomplish. The lapply function iterates across lists, such as vectors. When you invoke lapply it applies a defined operation to each item in the subitted list and returns a list of equal length that contains the results of this calculation. In the code below, we assign the values 1 through 10 to a vector and then use lapply to subtract 1 from each item in the vector and finally print the results to screen: v &lt;- c(1:10) results &lt;- lapply(v, function(x) (x-1)) print(results) We could accomplish the exact same thing with the following for loop v &lt;- c(1:10) for (i in v) { x &lt;- i - 1 print(x) } The basic syntax of lapply is: lapply(list, function) where “list” is some list object supplied and “function” is pre-defined chunk of code that will be exectuted. You’ll learn more about functions in a future lesson. "],["functions.html", "8 Functions 8.1 Learning objectives 8.2 What is a function? 8.3 What is the basic syntax of a function in R? 8.4 Building and calling functions 8.5 Saving functions and sourcing them from another file 8.6 Saving functions and calling them from another file", " 8 Functions 8.1 Learning objectives After this lecture, you should be able to: explain what a function is read and understand the basic syntax of a function in R use this syntax to call a function use this syntax to build your own function test your function install packages in R load libraries in R 8.2 What is a function? Why build code several or a hundred times when you can build it once and then call and run it as many times as you want? The answer is, don’t! A function allows you to perform an action multiple times in R by calling it and applying it in similar contexts. For instance, if you build a function that checks the class of all vectors in a dataframe, you can name this function and then apply it to do the same operation with any other dataframe. Or, if you build a function that graphs the correlation between two numeric vectors and exports this graph to a .png file, you can call this same function and apply it to two other vectors, again and again as needed. Functions can greatly increase the efficiency of your programming, and allow you to create flexible and customized solutions. 8.3 What is the basic syntax of a function in R? The basic syntax of a function in R, or the way it should be written so that R recognizes it and applies it do perform actions, is usually stated as follows: function_name &lt;- function(argument_1, argument_2, ...) { Function body } What this does not demonstrate is that there are actually two steps to a function: building it, and applying it. We will look at both steps in the following code from DataCamp: 8.4 Building and calling functions 8.4.1 Step 1: Building the function The code chunk builds the function, setting “myFirstFun” as the name, or variable, to which they have assigned the function. The function itself runs from the word “function” down through the closing curly brace. myFirstFun&lt;-function(n) { # Compute the square of integer `n` n*n } What is an argument? In the above example, “(n)” is the argument. R looks for this argument (in this case, “n”) in the body of the function, which in this case is n*n. When we run the above script, the function is saved as an object into the global environment so that it can be called elsewhere, as demonstrated in the code chunks below. The function has no effect unless you apply it. Until that happens, the function will do nothing but wait to be called. 8.4.2 Step 2: Calling the function The code chunk below calls “myFirstFun(n)” and tells R to assign the results of the operation the function performs (n*n) to the variable “u”. But if we run this code as it is (with “n” in the parentheses), we will get an error (unless we have previously assigned “n” as a variable with a value that will accept the operation to be performed — so “n” needs to be a number in this case so that it can be multiplied). We do not actually want to perform the function on the letter “n” but rather, on a number that we will insert in the place of “n.” We can apply this function by setting “n” as a number, such as 2, in the example below. # Call the function with argument `n` u &lt;- myFirstFun(2) # Call `u` u Once we have changed “n” to a number, R then performs this operation and saves the result to a new variable “u”. We can then ask R to tell us what “u” is, and R returns or prints the results of the function, which in this case, is the number 4 (2*2). The image below shows the results we get if we attempt to run the function without changing the argument “n” to a number (giving us an error), and the results when we change “n” to the number “2” which assigns the result of the function (4) to “u”, or the number “3” which assigns the result of the function (now 9) to “u”. It is important to understand that “n” is an argument of the function “myFirstFun.” R does not consider “n” a variable, but it acts like a variable because it can change as you call the function into different contexts. To R, “u” and “myFirstFun” are variables because they are names to which values and other content are assigned. 8.4.3 Example function with one argument Step 1: Build the function In the code below, we will build a function that groups age, as a number, into a category based on conditions we set using an if…else statement. #build function with one argument (variable) categorize_age &lt;- function(age) { if(age &lt; 26){ category &lt;- &quot;25 and under&quot; } else if(age &gt; 25 &amp; age &lt; 56) { category &lt;- &quot;26 to 55&quot; } else { category &lt;- &quot;56 and over&quot; } return(category) } Step 2: Call the function in one or more contexts. In the code below, we will call the function we built above and apply it to two different values for age. Just as we saw in the example above where we inserted the numbers 2 or 3 in place of “n”, we will insert the values of age we want to use in place of the word “age” to call the new function we have built. #run check_class function on two different values for age categorize_age(15) ## [1] &quot;25 and under&quot; categorize_age(70) ## [1] &quot;56 and over&quot; 8.4.4 Example function with more than one argument A function works similarly when it has two or more arguments. Let’s use the class data we collected at the start of term, my.data, generated below. pets &lt;- c(&quot;woof&quot;, &quot;woof&quot;, &quot;cat&quot;, &quot;woof&quot;, &quot;woof&quot;, &quot;cat&quot;, &quot;woof&quot;, &quot;woof&quot;, &quot;woof&quot;, &quot;woof&quot;, &quot;woof&quot;) place &lt;- c(&quot;Temple&quot;, &quot;Yakitori&quot;, &quot;Panera&quot;, &quot;Yakitori&quot;, &quot;Guads&quot;, &quot;Home&quot;, &quot;Tea List&quot;, &quot;Raising Canes&quot;, &quot;Pachamama&quot;, &quot;Lazi Cow&quot;, &quot;Wok of Flame&quot;) time.min &lt;- c(5, 4, 4, 12, 10, 2, 3, 4, 4, 5, 19) distance.mi &lt;- c(0.9, 0.6, 0.8, 0.6, 2, 100, 0.6, 0.7, 0.8, 1, 3.7) major &lt;- c(&quot;chicanx studies&quot;, &quot;human development&quot;, &quot;economics&quot;, &quot;undeclared&quot;, &quot;psychology&quot;, &quot;MMM&quot;, &quot;psychology&quot;, &quot;undeclared&quot;, &quot;human development&quot;, &quot;undeclared&quot;, &quot;GG&quot;) my.data &lt;- data.frame(place, distance.mi, time.min, major, pets) Now let’s say we only want to look at the first vector or column in the dataframe my.data. We would write a line of code that looks like this: my.data[1] ## place ## 1 Temple ## 2 Yakitori ## 3 Panera ## 4 Yakitori ## 5 Guads ## 6 Home ## 7 Tea List ## 8 Raising Canes ## 9 Pachamama ## 10 Lazi Cow ## 11 Wok of Flame But if we wanted to create a function that looks at any column/vector in any dataframe, we could write a function that looks like this: #build function with two arguments (variable) one_column &lt;- function(data, x) { data[x] } Note: if we want to tell a user what kind of input we want to include, we could instead do something like function(dataset, column_position) or function(dataset, column_name). Once we have run the above function (telling R to save it to the global environment), we would then call this new function, which we have named one_column, and apply it to various dataframes, and telling R which column or vector in each dataframe we want to view. #run one_column function on two different dataframes one_column(my.data, 1) ## place ## 1 Temple ## 2 Yakitori ## 3 Panera ## 4 Yakitori ## 5 Guads ## 6 Home ## 7 Tea List ## 8 Raising Canes ## 9 Pachamama ## 10 Lazi Cow ## 11 Wok of Flame one_column(my.data, 2) ## distance.mi ## 1 0.9 ## 2 0.6 ## 3 0.8 ## 4 0.6 ## 5 2.0 ## 6 100.0 ## 7 0.6 ## 8 0.7 ## 9 0.8 ## 10 1.0 ## 11 3.7 8.5 Saving functions and sourcing them from another file You can save the functions you build to a separate file, and then load these in to another script in the future. For example, I can paste the function I wrote above into a new script. #build function with two arguments (variable) one_column &lt;- function(data, x) { data[x] } Then I could save this script with any file name, typically something like “functions.r”. This “functions.r” script can then be loaded into any other script with the source() function. To load your pre-saved functions into a new script, you can call the “function.R” file at the top of the new script: # Open a new script, then paste and run the following line source(&quot;functions.r&quot;) The above code will allow you to call functions that are saved in these libraries and in the functions.r file. 8.6 Saving functions and calling them from another file You can save the functions you build to a separate file, and then load these as a source. For example, I might save my functions to an R script, called “functions.r”. I can then load these sources along with my packages into my R environment. Note: Although we loaded libraries as we went through this lesson, the best practice is to run your packages and source files at the very beginning of your new R script, as shown in the example that follows. library(dplyr) library(wakefield) library(rlang) library(ggforce) source(&quot;functions.r&quot;) The above code will allow you to call functions that are saved in these libraries and in the functions.r file. "],["working-with-files-and-packages.html", "9 Working with Files and Packages 9.1 Learning Objectives 9.2 Setup 9.3 Exploring Files 9.4 Reading and Writing Files in R 9.5 Packages", " 9 Working with Files and Packages This lesson will focus on working with data files in R. It will reinforce understanding of the command line, as well as RStudio. And, it will demonstrate finding and loading packages in R. 9.1 Learning Objectives After this lecture you should be able to: Identify file extensions Identify some common data science file formats Read csv files into R Save data to disk as RDS Download packages from CRAN Read excel files into R 9.2 Setup To follow along, download this zip file from the following url: https://datalab.ucdavis.edu/adventures-in-datascience/best_in_show.zip Navigate to where you want to save your work: cd ~/Documents/ Next, make a directory: mkdir files_in_r cd files_in_r Copy the downloaded zip file into that directory: cp ~/Downloads/best_in_show.zip . Unzip the file: unzip best_in_show.zip Navigate to the newly created directory cd best_in_show 9.3 Exploring Files When working with files, its important to gather lots of information, and constantly test assumptions that you may have. This process is a key part of programming and of working in the command line. Lets start by seeing what we have, which we do with the ls command: ls Remember that ls can be modified with flags, for example, to see all the files including hidden ones, use the -a flag: ls -a You can see more information about the files with the -l flag: ls -l Modifiers can be combined for ls: ls -la You can use du -h to see the disk usage (file size) of a given file: du -h dogs.csv -h refers to human readable as by default du displays the size in block units. Being aware of the size of a file early on can help debug issues with running out of disk space, as well as issues down the line in the analysis process. For example, reading in too many too large files into R can create issues for you by overloading your system’s RAM (your computer’s working memory). You can view the disk usage for all the files in the directory with the wildcard symbol *: du -h * 9.3.1 File Extensions Most of the time, you can guess the format of a file by looking at its extension, the characters (usually three) after the last dot . in the filename. For example, the extension .jpg or .jpeg indicates a JPEG image file. Some operating systems hide extensions by default, but you can find instructions to change this setting online by searching for “show file extensions” and your operating system’s name. The extension is just part of the file’s name, so it should be taken as a hint about the file’s format rather than a guarantee. 9.3.2 Text Files A text file is one that contains human-readable lines of text. You can check this by opening the file with a text editor such as Microsoft Notepad or macOS TextEdit. Many file formats use text in order to make the format easier to work with. On the command line, you can get information about the type of a file by using the file command followed by the name of a file: file dogs.csv Note that file uses a series of tests (learn more by reading man file), to determine the file type and may not always perfectly report the type of the file. The output of file is the filename followed by a colon and then a description of the file type. In this case, the output tells us that dogs.csv is a CSV text file. A comma-separated values (CSV) file records tabular data using one line per row, with commas separating columns. From the command line we can read text files with vim: vim dogs.csv To see the type of all the files in the directory you can use the wildcard * operator: file * 9.3.3 Binary Files A binary file is one that’s not human-readable. You can’t just read off the data if you open a binary file in a text editor, but they have a number of other advantages. Compared to text files, binary files are often faster to read and take up less storage space (bytes). For demonstrations sake, see what happens when you try to use vim to ‘read’ a binary data file: vim dogs.rds Notice that the editor displays data but it isn’t human readable, it looks like a bunch of random symbols with potentially the occasional recognizable word. 9.3.4 Common Data File Types Name Extension Tabular? Text? Comma-separated Values .csv Yes Yes Tab-separated Values .tsv Yes Yes Fixed-width File .fwf Yes Yes Microsoft Excel .xlsx Yes No Microsoft Excel 1993-2007 .xls Yes No Apache Arrow .feather Yes No R Data .rds Sometimes No R Data .rda Sometimes No Plaintext .txt Sometimes Yes Extensible Markup Language .xml No Yes JavaScript Object Notation .json No Yes 9.4 Reading and Writing Files in R R has many functions for working with file systems, reading and writing files. 9.4.1 The Working Directory The working directory is the starting point R uses for relative paths. Think of the working directory as the directory R is currently “at” or watching. The function getwd returns the absolute path for the current working directory, as a string. It doesn’t require any arguments: getwd() On your computer, the output from getwd will likely be different. This is a very useful function for getting your bearings when you write relative paths. If you write a relative path and it doesn’t work as expected, the first thing to do is check the working directory. The related setwd function changes the working directory. It takes one argument: a path to the new working directory. Here’s an example: setwd(&quot;..&quot;) # Now check the working directory. getwd() Generally, you should avoid using calls to setwd in your R scripts and R Markdown files. Calling setwd makes your code more difficult to understand, and can always be avoided by using appropriate relative paths. If you call setwd with an absolute path, it also makes your code less portable to other computers. It’s fine to use setwd interactively (in the R console), but avoid making your saved code dependent on it. When working in RStudio, you can set the working directory at the start of your session in session -&gt; Set Working Directory -&gt; To Source File Location Another function that’s useful for dealing with the working directory and file system is list.files. The list.files function returns the names of all of the files and directories inside of a directory. It accepts a path to a directory as an argument, or assumes the working directory if you don’t pass a path. For instance: # List files and directories in ~/. list.files(&quot;~/&quot;) # List files and directories in the working directory. list.files() If you call list.files with an invalid path or an empty directory, the output is character(0): list.files(&quot;/this/path/is/fake/&quot;) Later on, we’ll learn about what character(0) means more generally. 9.4.2 Reading a CSV File Let’s go ahead and read the dogs.csv file we extracted from the zip file at the start. R provides a very easy built-in function for reading CSV files, and a variety of other formats for text files containing tabular data. To read a csv file into R, use read.csv: dogs = read.csv(&#39;dogs.csv&#39;) 9.4.2.1 Inspecting the Data Note that when we used read.csv, we assumed that the file extension was appropriate. We can feel confident about this because we examined the file earlier. We can also feel confident that it will fit into memory as we saw it was relatively small. Whenever you import data into R, it is crucial to check that things went as expected. To check things went according to our expections, look at the output of the read.csv function, which we saved into dogs. Let’s see what the output is. We can check what the object is with the class function. class(dogs) We can see that the read.csv function returned a dataframe. This makes sense because dataframes represent tabular data, and csv files contain tabular data. We can get more information with the str function. str concisely gives information about the content of an R object: str(dogs) Let’s check the dimensions of our dataset: dim(dogs) Recall we can access the number of rows with: nrow(dogs) And the number of columns: ncol(dogs) To display the first rows from the dataset, use head: head(dogs) And to display the last rows from the dataset, use tail: tail(dogs) 9.4.3 Reader Functions for Tabular Data The read.csv function is a shortcut for read.table, a general purpose function for reading tabular data from plain-text files into R. The read.csv function acts just like read.table, but with the arguments sep = \",\" and header = TRUE. Table of R’s read functions (https://rstudio-education.github.io/hopr/dataio.html) Function Defaults Use read.table sep = ” “, header = FALSE General-purpose read function read.csv sep = “,”, header = TRUE Comma-separated-variable (CSV) files read.delim sep = “, header = TRUE Tab-delimited files read.csv2 sep = “;”, header = TRUE, dec = “,” CSV files with European decimal format read.delim2 sep = “, header = TRUE, dec =”,” Tab-delimited files with European decimal format 9.4.4 RDS You can save any R object, such as a data frame, as an RDS file. RDS files are a great option for storing data that is intended to be loaded into R. Data saved as RDS can be quickly and accurately loaded out of and back into R without losing any information. This isn’t always the case when saving data in plain text formats such as CSV. Any R-related metadata associated with the object you are saving will be maintainted in the RDS format. This is useful in the case of data frames if your data contains factors, or dates, or other specific class attributes that won’t be represented in a csv. You would need to reproduce the process for parsing the data into R. Additionally, RDS files often times take significantly less disk space to save, as they are a compressed format. RDS files in general are faster to read. However, its important to keep in mind that RDS files are meant to be used only in R. If you save data as an RDS, you are assuming that however is using that data will have access to and an understanding of R. As a result, its common to use the RDS format for saving intermediary data in a project. While when exporting results to a collaborator, or the internet you would most likely want to use a commonly used plain-text format such as CSV. Use saveRDS to save our data as an rds file with the rds file extension. saveRDS(dogs, &quot;./outputs/dogs.rds&quot;) It’s easy to load RDS files in R with readRDS: dogs = readRDS(&quot;./outputs/dogs.rds&quot;) 9.4.5 Writing a csv We just saved and read the dogs data as an RDS file, and we can practice saving data in other forms, such as a comma separated values (csv) file. Because we will be re-using the class survey data from the first week, let’s go ahead and save this data frame as a csv in your working directory. First, you will want to create a folder called “data” in your working directory. You can do this in your console with the dir.create() function (this is like the mkdir command used in the command line). (Hint: make sure you are in your class working directory). You can run the following in your console: dir.create(&quot;data&quot;) You can also use a point-and-click method by finding the New Folder button in the bottom right pane of RStudio, under the Files tab. Next, let’s manually create the my.data data frame once more, by copying and pasting the code below. pets &lt;- c(&quot;woof&quot;, &quot;woof&quot;, &quot;cat&quot;, &quot;woof&quot;, &quot;woof&quot;, &quot;cat&quot;, &quot;woof&quot;, &quot;woof&quot;, &quot;woof&quot;, &quot;woof&quot;, &quot;woof&quot;) place &lt;- c(&quot;Temple&quot;, &quot;Yakitori&quot;, &quot;Panera&quot;, &quot;Yakitori&quot;, &quot;Guads&quot;, &quot;Home&quot;, &quot;Tea List&quot;, &quot;Raising Canes&quot;, &quot;Pachamama&quot;, &quot;Lazi Cow&quot;, &quot;Wok of Flame&quot;) time.min &lt;- c(5, 4, 4, 12, 10, 2, 3, 4, 4, 5, 19) distance.mi &lt;- c(0.9, 0.6, 0.8, 0.6, 2, 100, 0.6, 0.7, 0.8, 1, 3.7) major &lt;- c(&quot;chicanx studies&quot;, &quot;human development&quot;, &quot;economics&quot;, &quot;undeclared&quot;, &quot;psychology&quot;, &quot;MMM&quot;, &quot;psychology&quot;, &quot;undeclared&quot;, &quot;human development&quot;, &quot;undeclared&quot;, &quot;GG&quot;) my.data &lt;- data.frame(place, distance.mi, time.min, major, pets) Now that we have a data frame called my.data, we can use the write.csv() function to save this data frame as a csv in our data folder. Let’s call it class_survey.csv. write.csv(my.data, &quot;data/class_survey.csv&quot;, row.names = F) Now this data will be available to us for future use without having to copy and paste anymore. 9.4.6 Excel Files in R Excel is very popular in the data analysis world. Millions of people use Excel to input, clean, analyze, and store data. R doesn’t provide a built-in function to load Excel files. Fortunately, members of the R community share code for a variety of tasks, including loading Excel files. 9.5 Packages Lots of the most useful parts of R do not come preloaded when you install R. Packages bundle together code, documentation and data. It’s easy to share, and easy to include in your own code. Users have contributed thousands of R packages which can be found online. You can think of a package as one or more functions that are related to a specific task, that you can include in your code. Packages need to be installed on your system and then loaded into your R session. 9.5.1 CRAN Comprehensive R Archive Network (CRAN) is the main website that makes R packages accessible. 9.5.1.1 readxl readxl is a package written to provide functions for working with Excel files in R. 9.5.2 Using Packages To use an R package, it first needs to be installed on your system, and then loaded into the R session. 9.5.2.1 Installing Packages You can install packages from CRAN onto your system using install.packages. It will search for the package on CRAN, and download the code onto your computer in a place that R can access. To install the readxl package, we pass the name to install.packages: install.packages(&quot;readxl&quot;) 9.5.2.2 Loading Packages Even if the package is on your system, it is not automatically loaded into R. Every time you restart R you will need to reload each package that your script uses. Do so with library at the top of your script for each package that you will use. This signals to you and anyone else that uses your script which packages are required to run the code, and will stop the execution of the script if any of the packages are not found. To load in the readxl package we installed in the previous step, use library: library(&quot;readxl&quot;) This will load in all the functions, data, and documentation from the readxl library, so we can now access them in our R session. To see all the packages installed you can run library without any arguments: library() This displays all the installed libraries as well the path R is searching to find them. 9.5.3 Load Excel Data With readxl, we can list all the sheets in an excel spreadsheet: sheets = excel_sheets(&quot;./data/dogs.xlsx&quot;) We can then load the data with read_excel: data = read_xlsx(&quot;./data/dogs.xlsx&quot;) "],["factors-special-values-and-indexing.html", "10 Factors, Special Values, and Indexing 10.1 Learning Objectives 10.2 Factors 10.3 Special Values 10.4 Indexing", " 10 Factors, Special Values, and Indexing This lesson covers what factors are, R’s special values, and R’s various indexing operators. 10.1 Learning Objectives After this lesson, you should be able to: Recognize categorical data Explain what factors are and when to use them Explain the purpose of R’s special values, especially missing values Explain the four different types of indexes and how to use them Explain why the [[ operator is necessary and how to use it Explain the syntax to subset a data frame 10.2 Factors A feature in a data set is categorical if it measures a qualitative category. Some examples of categories are: Music genres rock, blues, alternative, folk, pop Colors red, green, blue, yellow Answers yes, no Months January, February, and so on In some cases, a feature can be interpreted as categorical or quantitative. For instance, months can be interpreted as categories, but they can also be interpreted as numbers. There’s not necessarily a “correct” interpretation; each can be useful for different kinds of analyses. R uses the class factor to represent categorical data. For instance, in the dogs data set, the group column is a factor: class(dogs$group) Visualizations and statistical models sometimes treat factors differently than other data types, so it’s important to think about whether you want R to interpret data as categorical. When you load a data set, R usually can’t tell which features are categorical. That means identifying and converting the categorical features is up to you. For beginners, it can be difficult to understand whether a feature is categorical or not. The key is to think about whether you want to use the feature to divide the data into groups. For example, if we want to know how many songs are in the rock genre, we first need to divide the songs by genre, and then count the number of songs in each group (or at least the rock group). As a second example, months recorded as numbers can be categorical or not, depending on how you want to use them. You might want to treat them as categorical (for example, to compute max rainfall in each month) or you might want to treat them as numbers (for example, to compute the number of months time between two events). The bottom line is that you have to think about what you’ll be doing in the analysis. In some cases, you might treat a feature as categorical only for part of the analysis. You can use the factor function to convert a vector into a factor: colors = c(&quot;red&quot;, &quot;green&quot;, &quot;red&quot;, &quot;blue&quot;) colors = factor(colors) colors ## [1] red green red blue ## Levels: blue green red Notice that factors are printed differently than strings. The categories of a factor are called levels. You can list the levels with the levels function: levels(colors) ## [1] &quot;blue&quot; &quot;green&quot; &quot;red&quot; Factors remember all possible levels even if you take a subset: colors[1:3] ## [1] red green red ## Levels: blue green red This is another way factors are different from strings. Factors “remember” all possible levels even if they aren’t present. This ensures that if you plot a factor, the missing levels will still be represented on the plot. You can make a factor forget levels that aren’t present with the droplevels function: droplevels(colors[1:3]) ## [1] red green red ## Levels: green red 10.3 Special Values R has four special values to represent missing or invalid data. 10.3.1 Missing Values The value NA is called the missing value. Most of the time, missing values originate from how the data were collected (as opposed to computer errors). As an example, imagine the data came from a survey, and respondents chose not to answer some questions. In the data set, their answers for those questions might be recorded as NA. Of course, there are sometimes exceptions where missing values are the result of a computation. When you see missing values in a data set, you should think carefully about what the cause might be. Sometimes documentation or other parts of the data set provide clues. The missing value is a chameleon: it can be a logical, integer, numeric, complex, or character value. By default, the missing value is logical, and the other types occur through coercion (5.9.2): class(NA) ## [1] &quot;logical&quot; class(c(1, NA)) ## [1] &quot;numeric&quot; class(c(&quot;hi&quot;, NA, NA)) ## [1] &quot;character&quot; The missing value is also contagious: it represents an unknown quantity, so using it as an argument to a function usually produces another missing value. The idea is that if the inputs to a computation are unknown, generally so is the output: NA - 3 ## [1] NA mean(c(1, 2, NA)) ## [1] NA As a consequence, testing whether an object is equal to the missing value with == doesn’t return a meaningful result: 5 == NA ## [1] NA NA == NA ## [1] NA You can use the is.na function instead: is.na(5) ## [1] FALSE is.na(NA) ## [1] TRUE is.na(c(1, NA, 3)) ## [1] FALSE TRUE FALSE Missing values are a feature that sets R apart from most other programming languages. 10.3.2 Not a Number The value NaN, called not a number, represents a quantity that’s undefined mathematically. For instance, dividing 0 by 0 is undefined: 0 / 0 ## [1] NaN class(NaN) ## [1] &quot;numeric&quot; NaN can be numeric or complex. You can use the is.nan function to test whether a value is NaN: is.nan(c(10.1, log(-1), 3)) ## Warning in log(-1): NaNs produced ## [1] FALSE TRUE FALSE 10.3.3 Null The value NULL represents a quantity that’s undefined in R. Most of the time, NULL indicates the absence of a result. For instance, vectors don’t have dimensions, so the dim function returns NULL for vectors: dim(c(1, 2)) ## NULL class(NULL) ## [1] &quot;NULL&quot; typeof(NULL) ## [1] &quot;NULL&quot; Unlike the other special values, NULL has its own unique type and class. You can use the is.null function to test whether a value is NULL: is.null(&quot;null&quot;) ## [1] FALSE is.null(NULL) ## [1] TRUE 10.3.4 Infinity The value Inf represents infinity, and can be numeric or complex. You’re most likely to encounter it as the result of certain computations: 13 / 0 ## [1] Inf class(Inf) ## [1] &quot;numeric&quot; You can use the is.infinite function to test whether a value is infinite: is.infinite(3) ## [1] FALSE is.infinite(c(-Inf, 0, Inf)) ## [1] TRUE FALSE TRUE 10.4 Indexing The way to get and set elements of a data structure is by indexing. Sometimes this is also called subsetting or (element) extraction. Indexing is a fundamental operation in R, key to reasoning about how to solve problems with the language. We first saw indexing in Section 5.7.1, where we used [, the indexing or square bracket operator, to get and set elements of vectors. We saw indexing again in Section 5.8, where we used $, the dollar sign operator, to get and set data frame columns. The indexing operator [ is R’s primary operator for indexing. It works in four different ways, depending on the type of the index you use: An empty index selects all elements A numeric index selects elements by position A character index selects elements by name A logical index selects elements for which the index is TRUE Let’s explore each in more detail. We’ll use this vector as an example, to keep things concise: x = c(a = 10, b = 20, c = 30, d = 40, e = 50) x ## a b c d e ## 10 20 30 40 50 Even though we’re using a vector here, the indexing operator works with almost all data structures, including factors, lists, matrices, and data frames. We’ll look at unique behavior for some of these later on. 10.4.1 All Elements The first way to use [ to select elements is to leave the index blank. This selects all elements: x[] ## a b c d e ## 10 20 30 40 50 This way of indexing is rarely used for getting elements, since it’s the same as entering the variable name without the indexing operator. Instead, its main use is for setting elements. Suppose we want to set all the elements of x to 5. You might try writing this: x = 5 x ## [1] 5 Rather than setting each element to 5, this sets x to the scalar 5, which is not what we want. Let’s reset the vector and try again, this time using the indexing operator: x = c(a = 10, b = 20, c = 30, d = 40, e = 50) x[] = 5 x ## a b c d e ## 5 5 5 5 5 As you can see, now all the elements are 5. So the indexing operator is necessary to specify that we want to set the elements rather than the whole variable. Let’s reset x one more time, so that we can use it again in the next example: x = c(a = 10, b = 20, c = 30, d = 40, e = 50) 10.4.2 By Position The second way to use [ is to select elements by position. This happens when you use an integer or numeric index. We already saw the basics of this in Section 5.7.1. The positions of the elements in a vector (or other data structure) correspond to numbers starting from 1 for the first element. This way of indexing is frequently used together with the sequence operator : to get ranges of values. For instance, let’s get the 2nd through 4th elements of x: x[2:4] ## b c d ## 20 30 40 You can also use this way of indexing to set specific elements or ranges of elements. For example, let’s set the 3rd and 5th elements of x to 9 and 7, respectively: x[c(3, 5)] = c(9, 7) x ## a b c d e ## 10 20 9 40 7 When getting elements, you can repeat numbers in the index to get the same element more than once. You can also use the order of the numbers to control the order of the elements: x[c(2, 1, 2, 2)] ## b a b b ## 20 10 20 20 Finally, if the index contains only negative numbers, the elements at those positions are excluded rather than selected. For instance, let’s get all elements except the 1st and 5th: x[-c(1, 5)] ## b c d ## 20 9 40 When you index by position, the index should always be all positive or all negative. Using a mix of positive and negative numbers causes R to emit error rather than returning elements, since it’s unclear what the result should be: x[c(-1, 2)] ## Error in x[c(-1, 2)]: only 0&#39;s may be mixed with negative subscripts 10.4.3 By Name The third way to use [ is to select elements by name. This happens when you use a character vector as the index, and only works with named data structures. Like indexing by position, you can use indexing by name to get or set elements. You can also use it to repeat elements or change the order. Let’s get elements a, c, d, and a again from the vector x: y = x[c(&quot;a&quot;, &quot;c&quot;, &quot;d&quot;, &quot;a&quot;)] y ## a c d a ## 10 9 40 10 Element names are generally unique, but if they’re not, indexing by name gets or sets the first element whose name matches the index: y[&quot;a&quot;] ## a ## 10 Let’s reset x again to prepare for learning about the final way to index: x = c(a = 10, b = 20, c = 30, d = 40, e = 50) 10.4.4 By Condition The fourth and final way to use [ is to select elements based on a condition. This happens when you use a logical vector as the index. The logical vector should have the same length as what you’re indexing, and will be recycled (that is, repeated) if it doesn’t. Congruent Vectors To understand indexing by condition, we first need to learn about congruent vectors. Two vectors are congruent if they have the same length and they correspond element-by-element. For example, suppose you do a survey that records each respondent’s favorite animal and age. These are two different vectors of information, but each person will have a response for both. So you’ll have two vectors that are the same length: animal = c(&quot;dog&quot;, &quot;cat&quot;, &quot;iguana&quot;) age = c(31, 24, 72) The 1st element of each vector corresponds to the 1st person, the 2nd to the 2nd person, and so on. These vectors are congruent. Notice that columns in a data frame are always congruent! Back to Indexing When you index by condition, the index should generally be congruent to the object you’re indexing. Elements where the index is TRUE are kept and elements where the index is FALSE are dropped. If you create the index from a condition on the object, it’s automatically congruent. For instance, let’s make a condition based on the vector x: is_small = x &lt; 25 is_small ## a b c d e ## TRUE TRUE FALSE FALSE FALSE The 1st element in the logical vector is_small corresponds to the 1st element of x, the 2nd to the 2nd, and so on. The vectors x and is_small are congruent. It makes sense to use is_small as an index for x, and it gives us all the elements less than 25: x[is_small] ## a b ## 10 20 Of course, you can also avoid using an intermediate variable for the condition: x[x &gt; 10] ## b c d e ## 20 30 40 50 If you create index some other way (not using the object), make sure that it’s still congruent to the object. Otherwise, the subset returned from indexing might not be meaningful. You can also use indexing by condition to set elements, just as the other ways of indexing can be used to set elements. For instance, let’s set all the elements of x that are greater than 10 to the missing value NA: x[x &gt; 10] = NA x ## a b c d e ## 10 NA NA NA NA 10.4.5 Logic All of the conditions we’ve seen so far have been written in terms of a single test. If you want to use more sophisticated conditions, R provides operators to negate and combine logical vectors. These operators are useful for working with logical vectors even outside the context of indexing. Negation The NOT operator ! converts TRUE to FALSE and FALSE to TRUE: x = c(TRUE, FALSE, TRUE, TRUE, NA) x ## [1] TRUE FALSE TRUE TRUE NA !x ## [1] FALSE TRUE FALSE FALSE NA You can use ! with a condition: y = c(&quot;hi&quot;, &quot;hello&quot;) !(y == &quot;hi&quot;) ## [1] FALSE TRUE The NOT operator is vectorized. Combinations R also has operators for combining logical values. The AND operator &amp; returns TRUE only when both arguments are TRUE. Here are some examples: FALSE &amp; FALSE ## [1] FALSE TRUE &amp; FALSE ## [1] FALSE FALSE &amp; TRUE ## [1] FALSE TRUE &amp; TRUE ## [1] TRUE c(TRUE, FALSE, TRUE) &amp; c(TRUE, TRUE, FALSE) ## [1] TRUE FALSE FALSE The OR operator | returns TRUE when at least one argument is TRUE. Let’s see some examples: FALSE | FALSE ## [1] FALSE TRUE | FALSE ## [1] TRUE FALSE | TRUE ## [1] TRUE TRUE | TRUE ## [1] TRUE c(TRUE, FALSE) | c(TRUE, TRUE) ## [1] TRUE TRUE Be careful: everyday English is less precise than logic. You might say: I want all subjects with age over 50 and all subjects that like cats. But in logic this means: (subject age over 50) OR (subject likes cats) So think carefully about whether you need both conditions to be true (AND) or at least one (OR). Rarely, you might want exactly one condition to be true. The XOR (eXclusive OR) function xor() returns TRUE when exactly one argument is TRUE. For example: xor(FALSE, FALSE) ## [1] FALSE xor(TRUE, FALSE) ## [1] TRUE xor(TRUE, TRUE) ## [1] FALSE The AND, OR, and XOR operators are vectorized. Short-circuiting The second argument is irrelevant in some conditions: FALSE &amp; is always FALSE TRUE | is always TRUE Now imagine you have FALSE &amp; long_computation(). You can save time by skipping long_computation(). A short-circuit operator does exactly that. R has two short-circuit operators: &amp;&amp; is a short-circuited &amp; || is a short-circuited | These operators only evaluate the second argument if it is necessary to determine the result. Here are some of these: TRUE &amp;&amp; FALSE ## [1] FALSE TRUE &amp;&amp; TRUE ## [1] TRUE TRUE || TRUE ## [1] TRUE c(TRUE, FALSE) &amp;&amp; c(TRUE, TRUE) ## [1] TRUE For the final expression, notice R only combines the first element of each vector. The others are ignored. In other words, the short-circuit operators are not vectorized! Because of this, generally you _should not use*_ the short-circuit operators for indexing. Their main use is in writing conditions for control structures (6) and loops (7). 10.4.6 Indexing Lists Lists are a container for other types of R objects. When you select an element from a list, you can either keep the container (the list) or discard it. The indexing operator [ almost always keeps containers. As an example, let’s get some elements from a small list: x = list(first = c(1, 2, 3), second = sin, third = c(&quot;hi&quot;, &quot;hello&quot;)) y = x[c(1, 3)] y ## $first ## [1] 1 2 3 ## ## $third ## [1] &quot;hi&quot; &quot;hello&quot; class(y) ## [1] &quot;list&quot; The result is still a list. Even if we get just one element, the result of indexing a list with [ is a list: class(x[1]) ## [1] &quot;list&quot; Sometimes this will be exactly what we want. But what if we want to get the first element of x so that we can use it in a vectorized function? Or in a function that only accepts numeric arguments? We need to somehow get the element and discard the container. The solution to this problem is the extraction operator [[, which is also called the double square bracket operator. The extraction operator is the primary way to get and set elements of lists and other containers. Unlike the indexing operator [, the extraction operator always discards the container: x[[1]] ## [1] 1 2 3 class(x[[1]]) ## [1] &quot;numeric&quot; The tradeoff is that the extraction operator can only get or set one element at a time. Note that the element can be a vector, as above. Because it can only get or set one element at a time, the extraction operator can only index by position or name. Blank and logical indexes are not allowed. The final difference between the index operator [ and the extraction operator [[ has to do with how they handle invalid indexes. The index operator [ returns NA for invalid vector elements, and NULL for invalid list elements: c(1, 2)[10] ## [1] NA x[10] ## $&lt;NA&gt; ## NULL On the other hand, the extraction operator [[ raises an error for invalid elements: x[[10]] ## Error in x[[10]]: subscript out of bounds The indexing operator [ and the extraction operator [[ both work with any data structure that has elements. However, you’ll generally use the indexing operator [ to index vectors, and the extraction operator [[ to index containers (such as lists). 10.4.7 Indexing Data Frames For two-dimensional objects, like matrices and data frames, you can pass the indexing operator [ or the extraction operator [[ a separate index for each dimension. The rows come first: DATA[ROWS, COLUMNS] For instance, let’s get the first 3 rows and all columns of the dogs data: dogs[1:3, ] ## breed group datadog popularity_all popularity lifetime_cost ## 1 Border Collie herding 3.64 45 39 20143 ## 2 Border Terrier terrier 3.61 80 61 22638 ## 3 Brittany sporting 3.54 30 30 22589 ## intelligence_rank longevity ailments price food_cost grooming kids ## 1 1 12.52 2 623 324 weekly low ## 2 30 14.00 0 833 324 weekly high ## 3 19 12.92 0 618 466 weekly medium ## megarank_kids megarank size weight height ## 1 1 29 medium NA 20 ## 2 2 1 small 13.5 NA ## 3 3 11 medium 35.0 19 As we saw in Section 10.4.1, leaving an index blank means all elements. As another example, let’s get the 3rd and 5th row, and the 2nd and 4th column: dogs[c(3, 5), c(2, 4)] ## group popularity_all ## 3 sporting 30 ## 5 sporting 130 Mixing several different ways of indexing is allowed. So for example, we can get the same above, but use column names instead of positions: dogs[c(3, 5), c(&quot;breed&quot;, &quot;longevity&quot;)] ## breed longevity ## 3 Brittany 12.92 ## 5 Welsh Springer Spaniel 12.49 For data frames, it’s especially common to index the rows by condition and the columns by name. For instance, let’s get the breed, popularity, and weight columns for all rows with toy dogs: result = dogs[dogs$group == &quot;toy&quot;, c(&quot;breed&quot;, &quot;popularity&quot;, &quot;weight&quot;)] head(result) ## breed popularity weight ## 8 Papillon 33 NA ## 13 Affenpinscher 84 NA ## 16 Chihuahua 14 5.5 ## 28 Maltese 23 5.0 ## 29 Pomeranian 17 5.0 ## 30 Shih Tzu 11 12.5 The Drop Parameter If you use two-dimensional indexing with [ to select exactly one column, you get a vector: result = dogs[1:3, 2] class(result) ## [1] &quot;factor&quot; The container is dropped, even though the indexing operator [ usually keeps containers. This also occurs for matrices. You can control this behavior with the drop parameter: result = dogs[1:3, 2, drop = FALSE] class(result) ## [1] &quot;data.frame&quot; The default is drop = TRUE. "],["data-forensics.html", "11 Data Forensics 11.1 Learning Objectives 11.2 Structural Summaries &amp; Cleaning 11.3 Statistical Forensics 11.4 Apply Functions", " 11 Data Forensics This lesson covers a variety of ways to investigate and summarize tabular data in order to understand the data better and identify potential problems. The lesson also describes how to fix some of the most common data problems. 11.1 Learning Objectives After this lesson, you should be able to: Cleaning and Forensics: Explain what it means for a data set to be “tidy” Convert columns to appropriate data types Locate and count missing values in a data set Explain what it means for a value to be an “outlier” Locate and count outliers in a data set Statistical Summaries: Explain the location statistics mean, median, and mode Explain the scale statistics range and standard deviation Apply Functions Use apply functions to compute summaries of multiple columns Use the split-apply pattern to compute grouped summaries 11.2 Structural Summaries &amp; Cleaning Whenever you load a data set into R, your next step should be to investigate the data’s structure. This step is important because it can help you identify whether: The data was loaded correctly There are structural problems with the data that will make it difficult to use if they aren’t fixed Section 5.8.2 and Section 9.4.2.1 demonstrated several functions for getting structural summaries of data. Some of these are: str to get a detailed structural summary head, tail to preview the data nrow, ncol, dim, length to get dimension information names, colnames, rownames to get element names class, typeof to get classes and types Let’s look at some examples using a data set collected from the classified advertisements website Craigslist. The data set contains information from ads for rentals in the Sacramento area. First we need to load the data set: cl = read.csv(&quot;data/cl_rentals.csv&quot;) Now we can use the str function to check the classes of the columns: str(cl) ## &#39;data.frame&#39;: 2987 obs. of 20 variables: ## $ title : chr &quot;$1,125 / 1br - 550ft2 - 1Bedroom Prime Location -2520 S Limited Access/Gated $1125 Avail Now (2520 S St)&quot; &quot;$1,449 / 1br - 680ft2 - 1x1 with washer &amp; dryer in unit! Move in ready! (The Phoenix/Sacramento/Folsom/SF)&quot; &quot;$1,449 / 1br - 680ft2 - 1x1 with washer &amp; dryer in unit! Move in ready! (The Phoenix/Sacramento/Folsom/SF)&quot; &quot;$1,479 / 1br - 680ft2 - 1x1 with washer &amp; dryer in unit! Move in ready! (The Phoenix/Sacramento/Folsom/SF)&quot; ... ## $ text : chr &quot;QR Code Link to This Post\\n \\n \\n* SEE MY OTHER MIDTOWN 1 bedroom apts-text for web site\\n*An&quot;| __truncated__ &quot;QR Code Link to This Post\\n \\n \\n Lease our 1x1 Apartment with Che starting at $1449+ Pric&quot;| __truncated__ &quot;QR Code Link to This Post\\n \\n \\n Lease our 1x1 Apartment with Che starting at $1449+ Pric&quot;| __truncated__ &quot;QR Code Link to This Post\\n \\n \\n Lease our 1x1 Apartment with Che starting at $1479+ Pric&quot;| __truncated__ ... ## $ latitude : num 38.6 38.6 38.6 38.6 38.6 ... ## $ longitude : num -121 -121 -121 -121 -121 ... ## $ city : chr &quot;2520 S St&quot; &quot;The Phoenix/Sacramento/Folsom/SF&quot; &quot;The Phoenix/Sacramento/Folsom/SF&quot; &quot;The Phoenix/Sacramento/Folsom/SF&quot; ... ## $ date_posted : chr &quot;2021-02-04 15:03:12&quot; &quot;2021-03-02 12:41:17&quot; &quot;2021-03-02 13:26:17&quot; &quot;2021-03-03 10:02:05&quot; ... ## $ date_updated: chr &quot;2021-03-03 08:41:39&quot; NA NA NA ... ## $ price : int 1125 1449 1449 1479 1414 1441 1615 1660 1877 1611 ... ## $ deleted : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ sqft : int 550 680 680 680 680 680 816 816 916 916 ... ## $ bedrooms : int 1 1 1 1 1 1 2 2 2 2 ... ## $ bathrooms : num 1 1 1 1 1 1 1 1 2 2 ... ## $ pets : chr NA &quot;both&quot; &quot;both&quot; &quot;both&quot; ... ## $ laundry : chr &quot;shared&quot; &quot;in-unit&quot; &quot;in-unit&quot; &quot;in-unit&quot; ... ## $ parking : chr &quot;off-street&quot; &quot;covered&quot; &quot;covered&quot; &quot;covered&quot; ... ## $ craigslist : chr &quot;sacramento&quot; &quot;sacramento&quot; &quot;sacramento&quot; &quot;sacramento&quot; ... ## $ shp_place : chr &quot;Sacramento&quot; &quot;Sacramento&quot; &quot;Sacramento&quot; &quot;Sacramento&quot; ... ## $ shp_city : chr &quot;Sacramento&quot; &quot;Sacramento&quot; &quot;Sacramento&quot; &quot;Sacramento&quot; ... ## $ shp_state : chr &quot;CA&quot; &quot;CA&quot; &quot;CA&quot; &quot;CA&quot; ... ## $ shp_county : chr &quot;Sacramento&quot; &quot;Sacramento&quot; &quot;Sacramento&quot; &quot;Sacramento&quot; ... Often when you load a new data set, some of the columns won’t have the correct data type (or class) for what you want to do. For instance, in the Craigslist data, the pets, laundry, and parking columns all contain categorical data, so they should be factors. You can convert these columns to factors with the factor function from Section 10.2: cl$pets = factor(cl$pets) cl$laundry = factor(cl$laundry) cl$parking = factor(cl$parking) There’s another way we could’ve done this that uses only two lines of code, no matter how many columns there are: cols = c(&quot;pets&quot;, &quot;laundry&quot;, &quot;parking&quot;) cl[cols] = lapply(cl[cols], factor) We’ll learn more about the lapply function in Section 11.4. You can use whichever approach is more convenient and makes more sense to you. If there were other columns to convert, we’d go through the same steps with the appropriate conversion function. R provides as. functions to convert to the most common data types. For instance, as.character converts an object to a string: x = 3.1 class(x) ## [1] &quot;numeric&quot; y = as.character(x) y ## [1] &quot;3.1&quot; class(y) ## [1] &quot;character&quot; The read.csv function does a good job at identifying columns of numbers, so it’s rarely necessary to convert columns of numbers manually. However, you may have to do this for data you got some other way (rather than loading a file). For instance, it’s common to make these conversions when scraping data from the web. It’s also a good idea to convert categorical columns into factors with the factor function, and to convert columns of dates into dates (more about this in the next section). 11.2.1 Dates The as.Date function converts times and dates to R’s Date class. This data type allows us to do computations on dates, such as sorting by date or finding the number of days between two dates. How does as.Date work? We can use it to convert just about any date. The syntax is: as.Date(x, format) The parameter x is a string to convert to a date. The parameter format is a string that explains how the date is formatted. In the format string, a percent sign % followed by a character is called a specification and has special meaning. The most useful are: Specification Description January 29, 2015 %Y 4-digit year 2015 %y 2-digit year 15 %m 2-digit month 01 %B full month name January %b short month name Jan %d day of month 29 %% literal % % You can find a complete list in ?strptime. Let’s look at some examples: as.Date(&quot;January 29, 2015&quot;, &quot;%B %d, %Y&quot;) ## [1] &quot;2015-01-29&quot; as.Date(&quot;01292015&quot;, &quot;%m%d%Y&quot;) ## [1] &quot;2015-01-29&quot; x = c(&quot;Dec 13, 98&quot;, &quot;Dec 12, 99&quot;, &quot;Jan 1, 16&quot;) class(x) ## [1] &quot;character&quot; y = as.Date(x, &quot;%b %d, %y&quot;) class(y) ## [1] &quot;Date&quot; y ## [1] &quot;1998-12-13&quot; &quot;1999-12-12&quot; &quot;2016-01-01&quot; # You can do arithmetic on dates. y[2] - y[1] ## Time difference of 364 days Now let’s convert the date_posted column in the Craigslist data. It’s always a good idea to test your format string before saving the results back into the data frame: dates = as.Date(cl$date_posted, &quot;%Y-%m-%d %H:%M:%S&quot;) head(dates) ## [1] &quot;2021-02-04&quot; &quot;2021-03-02&quot; &quot;2021-03-02&quot; &quot;2021-03-03&quot; &quot;2021-03-04&quot; ## [6] &quot;2021-03-04&quot; The as.Date function returns NA if conversion failed, so in this case it looks like the dates were converted correctly. Now we can save the dates back into the data frame. We can also do the same thing for the other column of dates, date_updated: cl$date_posted = dates dates = as.Date(cl$date_updated, &quot;%Y-%m-%d %H:%M:%S&quot;) head(dates) # some NAs here because the column already contained NAs ## [1] &quot;2021-03-03&quot; NA NA NA NA ## [6] NA cl$date_updated = dates 11.2.2 Tidy Data Many functions require data frames that are in tidy form. Before we see the requirements for a data set to be tidy, we need to define or review some terminology from statistics. A feature (also called a covariate) is measurement of something, usually across multiple subjects. For example, we might decide to measure the heights of everyone in the class. Each person in the class is a subject, and the height measurement is a feature. Features don’t have to be quantitative. If we also asked each person their favorite color, then favorite color would be another feature in our data set. Features are usually, but not always, the columns in a tabular data set. An observation is a set of features measured for a single subject or at a single time. So in the preceding example, the combined height and favorite color measurement for one student is one observation. Observations are usually, but not always, the rows in a tabular data set. Now we can define what it means to be tidy. A tabular data set is tidy if and only if: Each observation has its own row. Each feature has its own column. Each value has its own cell. These rules ensure that all of the values are visually organized and are easy to access with R’s built-in indexing operations. For instance, the $ operator gets a column, and in a tidy data set, columns are features. The rules also reflect the way statisticians traditionally arrange tabular data sets. When you first look at a data set, think about what the observations are and what the features are. If the data set comes with documentation, it may help you figure this out. Since this data set is a tidy data set, we already know each row is an observation and each column is a feature. Chapter 20 gives examples of tidy and untidy data, as well as explanations of how to make untidy data tidy. 11.3 Statistical Forensics After investigating the data’s structure, it’s a good idea to check some basic statistical properties. This step is important because it can help you identify limitations of and patterns in the data. Which statistics are appropriate for a given feature often depends on the type of the feature. Recall from Section 5.9 that the types statisticians typically think about are: Categorical Nominal - data separated into specific categories, with no order. For example, hair color (red, brown, blonde, …) is categorical. Ordinal - data separated into specific categories, with an order. For example, school level (elementary, middle, high, college) is ordinal. Numerical Discrete - integers, or a finite set of decimal numbers with no values in between. Sometimes discrete values can also be treated as ordinal. For example, month as a number (1, 2, …, 12) is discrete. Continuous - decimal numbers. There are no specific categories, but there is an order. For example, height in inches is numerical. The table function, which was introduced in Section 5.8.2, is great for summarizing categorical (and sometimes discrete) data. For example: table(cl$pets) ## ## both cats dogs none ## 2511 46 31 385 What about numerical data? Two important questions to ask about data are: Where is it? This is the location of the data. How spread out is it? This is the scale of the data. Let’s use the data x = c(-2, -1, -1, -1, 0, 2, 6) as an example. Location is generally summarized with a number near the middle or center of the data. A few options are: Mode - the value that appears most frequently. The mode can be calculated for any kind of data, but doesn’t work well for continuous data. For our example, the mode of x is -1. You can compute the mode with table: table(x) Median - sort the data, then find the value in the middle. The median can be calculated for ordinal or numerical data. For our example, the median is -1. Compute this with median: median(x) Mean - the balancing point of the data, if a waiter was trying to balance the data on a tray. The mean can only be calculated for numerical data. For our example the mean is 0.4285. Compute this with mean: mean(x) Adding large values to the data affects the mean more than the median: y = c(x, 100) mean(y) median(y) Because of this, we say that the median is robust. The mean is good for getting a general idea of where the center of the data is, while comparing it with the median reveals whether there are any unusually large or small values. Scale is generally summarized by a number that says how far the data is from the center (mean, median, etc…). Two options are: Standard Deviation - square root of the average squared distance to the mean. You can think of this as approximately the average distance from a data point to the mean. As a rule of thumb, most of the data will be within 3 standard deviations of the mean. You can compute the standard deviation with sd: sd(x) Interquartile Range (IQR) - difference between the 75th and 25th percentile. The median is the 50th percentile of the data; it’s at the middle of the sorted data. We can also consider other percentiles. For instance, the 25th percentile is the value one-quarter of the way through the sorted data. Quantile is another word for percentile. Quartile specifically refers to the 25th, 50th, and 75th percentiles. You can compute quantiles with quantile, or compute the IQR directly with IQR: quantile(x) # IQR IQR(x) The IQR is more robust than the standard deviation. Many of the functions for computing statistical summaries have a parameter na.rm to ignore missing values. Setting na.rm = TRUE is often useful when you’re just trying to do an initial investigation of the data. However, in a more complete analysis, you should think carefully about what the missing values mean, whether they follow any patterns, and whether there are enough non-missing values for statistical summaries to be good representatives of the data. Finally, the summary function computes a detailed statistical summary of an R object. For data frames, the function computes a summary of each column, guessing an appropriate statistic based on the column’s data type. 11.3.1 Missing Values If your data contains missing values, it’s important to think about why the values are missing. Statisticians use two different terms to describe why data is missing: missing at random (MAR) missing not at random (MNAR) - causes bias! When values are missing at random, the cause for missingness is not related to any of the other features. This is rare in practice. For example, if people in a food survey accidentally overlook some questions. When values are missing not at random, the cause for missingness depends on other features. These features may or may not be in the data set. Think of this as a form of censorship. For example, if people in a food survey refuse to report how much sugar they ate on days where they ate junk food, data is missing not at random. Values MNAR can bias an analysis. The default way to handle missing values in R is to ignore them. This is just a default, not necessarily the best or even an appropriate way to deal with them. You can remove missing values from a data set by indexing: cl_no_sqft_na = cl[!is.na(cl$sqft), ] head(cl_no_sqft_na) ## title ## 1 $1,125 / 1br - 550ft2 - 1Bedroom Prime Location -2520 S Limited Access/Gated $1125 Avail Now (2520 S St) ## 2 $1,449 / 1br - 680ft2 - 1x1 with washer &amp; dryer in unit! Move in ready! (The Phoenix/Sacramento/Folsom/SF) ## 3 $1,449 / 1br - 680ft2 - 1x1 with washer &amp; dryer in unit! Move in ready! (The Phoenix/Sacramento/Folsom/SF) ## 4 $1,479 / 1br - 680ft2 - 1x1 with washer &amp; dryer in unit! Move in ready! (The Phoenix/Sacramento/Folsom/SF) ## 5 $1,414 / 1br - 680ft2 - 1x1 with washer &amp; dryer in unit! Move in ready! (The Phoenix/Sacramento/Folsom/SF) ## 6 $1,441 / 1br - 680ft2 - 1x1 with washer &amp; dryer in unit! Move in ready! (The Phoenix/Sacramento/Folsom/SF) ## text ## 1 QR Code Link to This Post\\n \\n \\n* SEE MY OTHER MIDTOWN 1 bedroom apts-text for web site\\n*An upstairs apt @ 2520 S is coming available 3/18/21\\n*I have 4 apts coming avail in midtown\\n*New flooring in lower apt and redone hardwood flooring in upper unit\\n*1 Bedroom lower unit in 20 unit complex (2-10 unit buildings-courtyard in middle) with manager on site\\n*Gated front and back\\n*9 parking spots in back\\n*Laundry on site with new washers and dryers (coin op)\\n*Owner pays water/sewer/garbage\\n*Wall heat and window air\\n*New paint and new Pergo-type wood flooring \\n*Updated lighting\\n*Nicely maintained building and grounds\\n*$500 deposit\\n*Non-Smoking/vaping Complex\\n*Long time Mgr on Site\\n*No dogs\\n*Pictures of a like unit\\n*Text/call showing Wes show contact info\\n to get copy of video\\n*You need to make 3X rent, have good rental history and credit score of 600 or greater to qualify-no dogs. ## 2 QR Code Link to This Post\\n \\n \\n Lease our 1x1 Apartment with Che starting at $1449+ Prices Subject to change daily! \\n Apartment Features: \\nPrivate Balconies &amp; Patios \\nLuxury Vinyl Wood Flooring \\nDesigner Interior Paint \\nUpgraded cabinets and countertops \\nModern white doors and trim \\nModern white base boards \\nEnergy efficient appliances \\nSpacious bathroom \\nTONS of closet and storage space \\nSac State Shuttle-including Sac State Shuttle App! PET FRIENDLY - MEOW! WOOF! \\n\\nUpgraded Fitness Centers \\nTanning Booth \\nHydro Massage Table \\nResident Business Center \\nResort-Style Pool Furniture \\nSparkling Waterfall Wall \\nFire pit locations \\nBeach Area for lounging \\nOutdoor Recreation Area \\nSand Volleyball Court \\nGrilling Locations Call today to schedule your tour ... show contact info\\n -or- \\nStop in to visit Mon - Sat 10:00a - 5:00p for more info! :D \\n\\n\\nCommunity Amenities: \\nComplete fitness center \\nFour sparkling pools \\nLighted tennis courts \\nFull basketball court \\nWhirlpool Jacuzzi \\n24 hour Emergency Maintenance Hornet Shuttle Access to Sacramento State University \\n Easy Access to Public Transportation (lightrail and bus) \\n Easy Access to the American River Trail And so much more! \\n\\n Check us out online @ www.thephoenixsacramento.com TODAY! \\n Connect with us: \\nFacebook = The Phoenix Apartment Living \\nTwitter = @thephoenixlife Prices and availability are subject to change. Offered prices are starting prices for base rent only. Other charges, conditions, fees, and terms may apply. Equal housing opportunity provider ## 3 QR Code Link to This Post\\n \\n \\n Lease our 1x1 Apartment with Che starting at $1449+ Prices Subject to change daily! \\n Apartment Features: \\nPrivate Balconies &amp; Patios \\nLuxury Vinyl Wood Flooring \\nDesigner Interior Paint \\nUpgraded cabinets and countertops \\nModern white doors and trim \\nModern white base boards \\nEnergy efficient appliances \\nSpacious bathroom \\nTONS of closet and storage space \\nSac State Shuttle-including Sac State Shuttle App! PET FRIENDLY - MEOW! WOOF! \\n\\nUpgraded Fitness Centers \\nTanning Booth \\nHydro Massage Table \\nResident Business Center \\nResort-Style Pool Furniture \\nSparkling Waterfall Wall \\nFire pit locations \\nBeach Area for lounging \\nOutdoor Recreation Area \\nSand Volleyball Court \\nGrilling Locations Call today to schedule your tour ... show contact info\\n -or- \\nStop in to visit Mon - Sat 10:00a - 5:00p for more info! :D \\n\\n\\nCommunity Amenities: \\nComplete fitness center \\nFour sparkling pools \\nLighted tennis courts \\nFull basketball court \\nWhirlpool Jacuzzi \\n24 hour Emergency Maintenance Hornet Shuttle Access to Sacramento State University \\n Easy Access to Public Transportation (lightrail and bus) \\n Easy Access to the American River Trail And so much more! \\n\\n Check us out online @ www.thephoenixsacramento.com TODAY! \\n Connect with us: \\nFacebook = The Phoenix Apartment Living \\nTwitter = @thephoenixlife Prices and availability are subject to change. Offered prices are starting prices for base rent only. Other charges, conditions, fees, and terms may apply. Equal housing opportunity provider ## 4 QR Code Link to This Post\\n \\n \\n Lease our 1x1 Apartment with Che starting at $1479+ Prices Subject to change daily! \\n Apartment Features: \\nPrivate Balconies &amp; Patios \\nLuxury Vinyl Wood Flooring \\nDesigner Interior Paint \\nUpgraded cabinets and countertops \\nModern white doors and trim \\nModern white base boards \\nEnergy efficient appliances \\nSpacious bathroom \\nTONS of closet and storage space \\nSac State Shuttle-including Sac State Shuttle App! PET FRIENDLY - MEOW! WOOF! \\n\\nUpgraded Fitness Centers \\nTanning Booth \\nHydro Massage Table \\nResident Business Center \\nResort-Style Pool Furniture \\nSparkling Waterfall Wall \\nFire pit locations \\nBeach Area for lounging \\nOutdoor Recreation Area \\nSand Volleyball Court \\nGrilling Locations Call today to schedule your tour ... show contact info\\n -or- \\nStop in to visit Mon - Sat 10:00a - 5:00p for more info! :D \\n\\n\\nCommunity Amenities: \\nComplete fitness center \\nFour sparkling pools \\nLighted tennis courts \\nFull basketball court \\nWhirlpool Jacuzzi \\n24 hour Emergency Maintenance Hornet Shuttle Access to Sacramento State University \\n Easy Access to Public Transportation (lightrail and bus) \\n Easy Access to the American River Trail And so much more! \\n\\n Check us out online @ www.thephoenixsacramento.com TODAY! \\n Connect with us: \\nFacebook = The Phoenix Apartment Living \\nTwitter = @thephoenixlife Prices and availability are subject to change. Offered prices are starting prices for base rent only. Other charges, conditions, fees, and terms may apply. Equal housing opportunity provider ## 5 QR Code Link to This Post\\n \\n \\n Lease our 1x1 Apartment with Juliet starting at $1414+ Prices Subject to change daily! \\n Apartment Features: \\nPrivate Balconies &amp; Patios \\nLuxury Vinyl Wood Flooring \\nDesigner Interior Paint \\nUpgraded cabinets and countertops \\nModern white doors and trim \\nModern white base boards \\nEnergy efficient appliances \\nSpacious bathroom \\nTONS of closet and storage space \\nSac State Shuttle-including Sac State Shuttle App! PET FRIENDLY - MEOW! WOOF! \\n\\nUpgraded Fitness Centers \\nTanning Booth \\nHydro Massage Table \\nResident Business Center \\nResort-Style Pool Furniture \\nSparkling Waterfall Wall \\nFire pit locations \\nBeach Area for lounging \\nOutdoor Recreation Area \\nSand Volleyball Court \\nGrilling Locations Call today to schedule your tour ... show contact info\\n -or- \\nStop in to visit Mon - Sat 10:00a - 5:00p for more info! :D \\n\\n\\nCommunity Amenities: \\nComplete fitness center \\nFour sparkling pools \\nLighted tennis courts \\nFull basketball court \\nWhirlpool Jacuzzi \\n24 hour Emergency Maintenance Hornet Shuttle Access to Sacramento State University \\n Easy Access to Public Transportation (lightrail and bus) \\n Easy Access to the American River Trail And so much more! \\n\\n Check us out online @ www.thephoenixsacramento.com TODAY! \\n Connect with us: \\nFacebook = The Phoenix Apartment Living \\nTwitter = @thephoenixlife Prices and availability are subject to change. Offered prices are starting prices for base rent only. Other charges, conditions, fees, and terms may apply. Equal housing opportunity provider ## 6 QR Code Link to This Post\\n \\n \\n Lease our 1x1 Apartment with Che starting at $1441+ Prices Subject to change daily! \\n Apartment Features: \\nPrivate Balconies &amp; Patios \\nLuxury Vinyl Wood Flooring \\nDesigner Interior Paint \\nUpgraded cabinets and countertops \\nModern white doors and trim \\nModern white base boards \\nEnergy efficient appliances \\nSpacious bathroom \\nTONS of closet and storage space \\nSac State Shuttle-including Sac State Shuttle App! PET FRIENDLY - MEOW! WOOF! \\n\\nUpgraded Fitness Centers \\nTanning Booth \\nHydro Massage Table \\nResident Business Center \\nResort-Style Pool Furniture \\nSparkling Waterfall Wall \\nFire pit locations \\nBeach Area for lounging \\nOutdoor Recreation Area \\nSand Volleyball Court \\nGrilling Locations Call today to schedule your tour ... show contact info\\n -or- \\nStop in to visit Mon - Sat 10:00a - 5:00p for more info! :D \\n\\n\\nCommunity Amenities: \\nComplete fitness center \\nFour sparkling pools \\nLighted tennis courts \\nFull basketball court \\nWhirlpool Jacuzzi \\n24 hour Emergency Maintenance Hornet Shuttle Access to Sacramento State University \\n Easy Access to Public Transportation (lightrail and bus) \\n Easy Access to the American River Trail And so much more! \\n\\n Check us out online @ www.thephoenixsacramento.com TODAY! \\n Connect with us: \\nFacebook = The Phoenix Apartment Living \\nTwitter = @thephoenixlife Prices and availability are subject to change. Offered prices are starting prices for base rent only. Other charges, conditions, fees, and terms may apply. Equal housing opportunity provider ## latitude longitude city date_posted date_updated ## 1 38.5728 -121.4675 2520 S St 2021-02-04 2021-03-03 ## 2 38.5511 -121.4068 The Phoenix/Sacramento/Folsom/SF 2021-03-02 &lt;NA&gt; ## 3 38.5511 -121.4068 The Phoenix/Sacramento/Folsom/SF 2021-03-02 &lt;NA&gt; ## 4 38.5511 -121.4068 The Phoenix/Sacramento/Folsom/SF 2021-03-03 &lt;NA&gt; ## 5 38.5511 -121.4068 The Phoenix/Sacramento/Folsom/SF 2021-03-04 &lt;NA&gt; ## 6 38.5511 -121.4068 The Phoenix/Sacramento/Folsom/SF 2021-03-04 &lt;NA&gt; ## price deleted sqft bedrooms bathrooms pets laundry parking craigslist ## 1 1125 FALSE 550 1 1 &lt;NA&gt; shared off-street sacramento ## 2 1449 FALSE 680 1 1 both in-unit covered sacramento ## 3 1449 FALSE 680 1 1 both in-unit covered sacramento ## 4 1479 FALSE 680 1 1 both in-unit covered sacramento ## 5 1414 FALSE 680 1 1 both in-unit covered sacramento ## 6 1441 FALSE 680 1 1 both in-unit covered sacramento ## shp_place shp_city shp_state shp_county ## 1 Sacramento Sacramento CA Sacramento ## 2 Sacramento Sacramento CA Sacramento ## 3 Sacramento Sacramento CA Sacramento ## 4 Sacramento Sacramento CA Sacramento ## 5 Sacramento Sacramento CA Sacramento ## 6 Sacramento Sacramento CA Sacramento The na.omit function is less precise than indexing, because it removes rows that have a missing value in any column. This means lots of information gets lost. Another way to handle missing values is to impute, or fill in, the values with estimates based on other data in the data set. We won’t get into the details of how to impute missing values here, since it is a fairly deep subject. Generally it is safe to impute MAR values, but not MNAR values. 11.3.2 Outliers An outlier is an anomalous or extreme value in a data set. We can picture this as a value that’s far away from most of the other values. Sometimes outliers are a natural part of the data set. In other situations, outliers can indicate errors in how the data were measured, recorded, or cleaned. There’s no specific definition for “extreme” or “far away”. A good starting point for detecting outliers is to make a plot that shows how the values are distributed. Box plots and density plots work especially well for this (you’ll learn about how to make plots in a later lesson): library(ggplot2) ## Registered S3 methods overwritten by &#39;tibble&#39;: ## method from ## format.tbl pillar ## print.tbl pillar ggplot(cl, aes(x = sqft)) + geom_boxplot() ## Warning: Removed 347 rows containing non-finite values (stat_boxplot). Statisticians tend to use the rule of thumb that any value more than 3 standard deviations away from the mean is an outlier. You can use the scale function to compute how many standard deviations the elements in a column are from their mean: z = scale(cl$sqft) head(z) ## [,1] ## [1,] -0.1910838 ## [2,] -0.1161393 ## [3,] -0.1161393 ## [4,] -0.1161393 ## [5,] -0.1161393 ## [6,] -0.1161393 which(z &lt;= -3 | 3 &lt;= z) ## [1] 1261 2461 Be careful to think about what your specific data set measures, as this definition isn’t appropriate in every situation. How can you handle outliers? First, try inspecting other features from the row to determine whether the outlier is a valid measurement or an error. When an outlier is valid, keep it. If the outlier interferes with a plot you want to make, you can adjust the x and y limits on plots as needed to “ignore” the outlier. Make sure to mention this in the plot’s title or caption. When an outlier is not valid, first try to correct it. For example: Correct with a different covariate from the same observation. Estimate with a mean or median of similar observations. This is another example of imputing values. For example, in the Craigslist data, we can use the text column to try to correct outliers: cat(cl$text[1261]) ## QR Code Link to This Post ## ## ## Villages of the Galleria ## 701 Gibson Drive, Roseville, CA, 95678 ## Want more information? Follow this link: ## http://rcmi.aptdetails.com/49u13n ## Call Now: show contact info ## ## Roseville&#39;s Premier Luxury Condominium Rentals ## This is a 1 Bedroom, 1 Bath, approximately 819 Sq. Ft. ## Signature Collection ## This collection of fully renovated homes is limited to a select few. These unique homes are renting quickly. ## The beautifully remodeled floor plan offers an entertainment style kitchen, gracious living area, formal dining room with access to your outdoor balcony, designer two tone paint with crown molding, spacious bathroom with relaxing oval bath tub, linen closet and large vanity. The bedroom offers a sliding glass door giving you additional access to the private balcony over looking the picturesque courtyard. ## Brand New Featured Interiors: ## Entertainment Style Kitchen ## • Beautiful warm espresso custom built cabinets with brush nickel hardware ## • Opulent Granite Countertops with backsplash ## • Satin finish under mount sink with disposal and upgraded Moen faucet and fixtures ## • Stainless steel appliances, spacious built in microwave, multi-cycle dishwasher and self-cleaning oven ## • Plant/décor cabinet ledge ## • Spacious pantry and personalized custom shelving in all cabinets ## • Attractive bright recessed lighting ## • Private in home personal laundry room with full size washer &amp; dryer ## Living and Dining ## • Hand laid tile resembling hard wood flooring ## • Designer two-tone paint with white accent crown molding and baseboards ## • Upgraded wooden style blinds ## • Dual pane windows featuring custom framed molding ## • Spacious coat closet ## Bath ## • Oval Roman soaking tub with surround Opulent granite walls ## • Warm espresso custom built cabinets with brush nickel hardware ## • Spacious linen closet and personal cabinet storage ## • Hand selected Opulent Granite countertops ## • Unique hand crafted above counter sink ## • Contemporary waterfall faucet ## • Custom wood-look framed mirror ## • White glass contemporary light fixture with brush nickel base ## • Upgraded brush nickel accents (towel bars and holders) ## • Hand laid tile resembling hard wood flooring ## Bedroom and Closet ## • Rich plush carpeting ## • Spacious walk in closets with personalized built in custom organizers and compartments ## Other Features and Amenities ## • Brilliant bright recessed lighting ## • Central heat and air ## • Private balcony or terrace ## • Pre-wired for high-speed internet, multi-line phone and cable ## • Brushed nickel hardware accents (door knobs, latches, deadbolts, locks, door knocker and light fixtures) ## • Covered parking ## • Additional patio storage ## Select Homes Offer ## • Private detached garage ## • Additional linen or storage space ## • Cozy Gas Fireplace with carved stone-look mantel and molding ## Style, sophistication, beautiful landscaping and stunning architecture accent the Villages of the ## Galleria apartment homes, located in dynamic Roseville, California. Villages of the Galleria is just minutes from the Galleria Mall and Fountains at Roseville and offers easy freeway access to downtown, Sacramento International Airport, Arco Arena and major employers such as NEC, Oracle and HP. Select from a variety of one, two or three bedroom floor plans. All apartment homes offer gracious living areas with designer two-tone paint, crown molding, large walk-in closets and in home full size washer and dryer. Enjoy the many fine conveniences offered, such as an expansive fitness center, executive business center and refreshing pool. Villages of the Galleria is the perfect place to call home. ## Features ## - Contemporary Recessed Lighting ## - Built-In Linen Closet in Bathroom * ## - Entertainment Style Kitchens ## - Private Balconies and Patios ## - Crown Molding Accents ## - Six-Panel Interior Doors ## - Custom Maple-Front Cabinetry ## - Nine-Foot Ceilings ## - Microwave ## - Pre-Wired for High Speed Internet ## - Private Garages * ## - Full Size Washer/Dryer ## - Pantry * ## - Oval Roman Soaking Tub * ## - Cozy Gas Fireplace with Mantel * ## - Covered Parking * ## - Spacious Walk-In Closet(s) * ## - Energy-Saving Multi-Cycle Dishwasher ## Community Amenities ## - Community Garden ## - Executive Business Center ## - Sand Volleyball ## - Close to Shopping ## - Beautiful Landscaped Court Yards ## - Playground ## - Fitness Center ## - Clubhouse ## - Open Air Cabanas ## - Easy Access to Freeways ## - Pool and Spa ## - Gated Community ## - Professional Onsite Management w/ 24-Hour Emergency Maintenance ## - Picnic Area with Barbecue ## Office Hours ## Monday - Friday 9:00 AM - 6:00 PM ## Saturday 10:00 AM - 5:00 PM ## Sunday 12:00 PM - 5:00 PM ## Pet Policy ## Maximum of 2 pets cats or dogs. No weight limit. Additional $25 rent per month and additional $500 deposit per pet. Inquire about our breed restrictions. ## Equal Housing Opportunity ## VJWLzl1wXG Based on the text, this apartment is 819 square feet, not 8190 square feet. So we can reassign the value: cl$sqft[1261] = 819 If other features don’t help with correction, try getting information from external sources. If you can’t correct the outlier but know it’s invalid, replace it with a missing value NA. 11.4 Apply Functions Section 5.7.2 introduced vectorization, a convenient and efficient way to compute multiple results. That section also mentioned that some of R’s functions—the ones that summarize or aggregate data—are not vectorized. The class function is an example of a function that’s not vectorized. If we call the class function on the Craigslist data set, we get just one result for the data set as a whole: class(cl) ## [1] &quot;data.frame&quot; What if we want to get the class of each column? We can get the class for a single column by selecting the column with $, the dollar sign operator: class(cl$pets) ## [1] &quot;factor&quot; But what if we want the classes for all the columns? We could write a call to class for each column, but that would be tedious. When you’re working with a programming language, you should try to avoid tedium; there’s usually a better, more automated way. Section 5.9.1 pointed out that data frames are technically lists, where each column is one element. With that in mind, what we need here is a line of code that calls class on each element of the data frame. The idea is similar to vectorization, but since we have a list and a non-vectorized function, we have to do a bit more than just call class(cl). The lapply function calls, or applies, a function on each element of a list or vector. The syntax is: lapply(X, FUN, ...) The function FUN is called once for each element of X, with the element as the first argument. The ... is for additional arguments to FUN, which are held constant across all the elements. Let’s try this out with the earnings data and the class function: lapply(cl, class) ## $title ## [1] &quot;character&quot; ## ## $text ## [1] &quot;character&quot; ## ## $latitude ## [1] &quot;numeric&quot; ## ## $longitude ## [1] &quot;numeric&quot; ## ## $city ## [1] &quot;character&quot; ## ## $date_posted ## [1] &quot;Date&quot; ## ## $date_updated ## [1] &quot;Date&quot; ## ## $price ## [1] &quot;integer&quot; ## ## $deleted ## [1] &quot;logical&quot; ## ## $sqft ## [1] &quot;numeric&quot; ## ## $bedrooms ## [1] &quot;integer&quot; ## ## $bathrooms ## [1] &quot;numeric&quot; ## ## $pets ## [1] &quot;factor&quot; ## ## $laundry ## [1] &quot;factor&quot; ## ## $parking ## [1] &quot;factor&quot; ## ## $craigslist ## [1] &quot;character&quot; ## ## $shp_place ## [1] &quot;character&quot; ## ## $shp_city ## [1] &quot;character&quot; ## ## $shp_state ## [1] &quot;character&quot; ## ## $shp_county ## [1] &quot;character&quot; The result is similar to if the class function was vectorized. In fact, if we use a vector and a vectorized function with lapply, the result is nearly identical to the result from vectorization: x = c(1, 2, pi) sqrt(x) ## [1] 1.000000 1.414214 1.772454 lapply(x, sqrt) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 1.414214 ## ## [[3]] ## [1] 1.772454 The only difference is that the result from lapply is a list. In fact, the lapply function always returns a list with one element for each element of the input data. The “l” in lapply stands for “list”. The lapply function is one member of a family of functions called apply functions. All of the apply functions provide ways to apply a function repeatedly to different parts of a data structure. We’ll meet a few more apply functions soon. When you have a choice between using vectorization or an apply function, you should always choose vectorization. Vectorization is clearer—compare the two lines of code above—and it’s also significantly more efficient. In fact, vectorization is the most efficient way to call a function repeatedly in R. As we saw with the class function, there are some situations where vectorization is not possible. That’s when you should think about using an apply function. 11.4.1 The sapply Function The related sapply function calls a function on each element of a list or vector, and simplifies the result. That last part is the crucial difference compared to lapply. When results from the calls all have the same type and length, sapply returns a vector or matrix instead of a list. When the results have different types or lengths, the result is the same as for lapply. The “s” in sapply stands for “simplify”. For instance, if we use sapply to find the classes of the columns in the earnings data, we get a character vector: sapply(cl, class) ## title text latitude longitude city date_posted ## &quot;character&quot; &quot;character&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;character&quot; &quot;Date&quot; ## date_updated price deleted sqft bedrooms bathrooms ## &quot;Date&quot; &quot;integer&quot; &quot;logical&quot; &quot;numeric&quot; &quot;integer&quot; &quot;numeric&quot; ## pets laundry parking craigslist shp_place shp_city ## &quot;factor&quot; &quot;factor&quot; &quot;factor&quot; &quot;character&quot; &quot;character&quot; &quot;character&quot; ## shp_state shp_county ## &quot;character&quot; &quot;character&quot; Likewise, if we use sapply to compute the sqrt values, we get a numeric vector, the same as from vectorization: sapply(x, sqrt) ## [1] 1.000000 1.414214 1.772454 In spite of that, vectorization is still more efficient than sapply, so use vectorization instead when possible. Apply functions are incredibly useful for summarizing data. For example, suppose we want to compute the medians for all of the columns in the earnings data set that are numeric. First, we need to identify the columns. One way to do this is with the is.numeric function. Despite the name, this function actually tests whether its argument is a real number, not whether it its argument is a numeric vector. In other words, it also returns true for integer values. We can use sapply to apply this function to all of the columns in the Craigslist data set: is_number = sapply(cl, is.numeric) is_number ## title text latitude longitude city date_posted ## FALSE FALSE TRUE TRUE FALSE FALSE ## date_updated price deleted sqft bedrooms bathrooms ## FALSE TRUE FALSE TRUE TRUE TRUE ## pets laundry parking craigslist shp_place shp_city ## FALSE FALSE FALSE FALSE FALSE FALSE ## shp_state shp_county ## FALSE FALSE In general, it’s a good habit to use R to do things rather than do them manually. You’ll get more practice programming, and your code will be more flexible if you want to adapt it to other data sets. Now that we know which columns are numeric, we can use the median function to compute medians. We only want to compute medians for those columns, so we need to subset the data: sapply(cl[, is_number], median, na.rm = TRUE) ## latitude longitude price sqft bedrooms bathrooms ## 38.5878 -121.4410 1730.0000 801.0000 2.0000 1.0000 11.4.2 The Split-Apply Pattern In a data set with categorical features, it’s often useful to compute something for each category. The lapply and sapply functions can compute something for each element of a data structure, but categories are not necessarily elements. For example, the Craigslist data set has four different categories in the laundry column. If we want all of the rows in one category, one way to get them is by indexing: shared = cl[cl$laundry == &quot;shared&quot;, ] head(shared) ## title ## 1 $1,125 / 1br - 550ft2 - 1Bedroom Prime Location -2520 S Limited Access/Gated $1125 Avail Now (2520 S St) ## 7 $1,615 / 2br - 816ft2 - 2x1 with w/d in unit.. available NOW! APPLY TODAY! (The Phoenix/Sacramento/Folsom/SF) ## 8 $1,660 / 2br - 816ft2 - 2x1 with w/d in unit.. available NOW! APPLY TODAY! (The Phoenix/Sacramento/Folsom/SF) ## 9 $1,877 / 2br - 916ft2 - 2x2 with w/d in unit.. available NOW! APPLY TODAY! (The Phoenix/Sacramento/Folsom/SF) ## 10 $1,611 / 2br - 916ft2 - 2x2 with w/d in unit.. available NOW! APPLY TODAY! (The Phoenix/Sacramento/Folsom/SF) ## 11 $1,736 / 2br - 916ft2 - 2x2 with w/d in unit.. available NOW! APPLY TODAY! (The Phoenix/Sacramento/Folsom/SF) ## text ## 1 QR Code Link to This Post\\n \\n \\n* SEE MY OTHER MIDTOWN 1 bedroom apts-text for web site\\n*An upstairs apt @ 2520 S is coming available 3/18/21\\n*I have 4 apts coming avail in midtown\\n*New flooring in lower apt and redone hardwood flooring in upper unit\\n*1 Bedroom lower unit in 20 unit complex (2-10 unit buildings-courtyard in middle) with manager on site\\n*Gated front and back\\n*9 parking spots in back\\n*Laundry on site with new washers and dryers (coin op)\\n*Owner pays water/sewer/garbage\\n*Wall heat and window air\\n*New paint and new Pergo-type wood flooring \\n*Updated lighting\\n*Nicely maintained building and grounds\\n*$500 deposit\\n*Non-Smoking/vaping Complex\\n*Long time Mgr on Site\\n*No dogs\\n*Pictures of a like unit\\n*Text/call showing Wes show contact info\\n to get copy of video\\n*You need to make 3X rent, have good rental history and credit score of 600 or greater to qualify-no dogs. ## 7 QR Code Link to This Post\\n \\n \\n Lease our 2x1 Apartment with Juliet starting at $1615+ Prices Subject to change daily! \\n Apartment Features: \\nPrivate Balconies &amp; Patios \\nLuxury Vinyl Wood Flooring \\nDesigner Interior Paint \\nUpgraded cabinets and countertops \\nModern white doors and trim \\nModern white base boards \\nEnergy efficient appliances \\nSpacious bathroom \\nTONS of closet and storage space \\nSac State Shuttle-including Sac State Shuttle App! PET FRIENDLY - MEOW! WOOF! \\n\\nUpgraded Fitness Centers \\nTanning Booth \\nHydro Massage Table \\nResident Business Center \\nResort-Style Pool Furniture \\nSparkling Waterfall Wall \\nFire pit locations \\nBeach Area for lounging \\nOutdoor Recreation Area \\nSand Volleyball Court \\nGrilling Locations Call today to schedule your tour ... show contact info\\n -or- \\nStop in to visit Mon - Sat 10:00a - 5:00p for more info! :D \\n\\n\\nCommunity Amenities: \\nComplete fitness center \\nFour sparkling pools \\nLighted tennis courts \\nFull basketball court \\nWhirlpool Jacuzzi \\n24 hour Emergency Maintenance Hornet Shuttle Access to Sacramento State University \\n Easy Access to Public Transportation (lightrail and bus) \\n Easy Access to the American River Trail And so much more! \\n\\n Check us out online @ www.thephoenixsacramento.com TODAY! \\n Connect with us: \\nFacebook = The Phoenix Apartment Living \\nTwitter = @thephoenixlife Prices and availability are subject to change. Offered prices are starting prices for base rent only. Other charges, conditions, fees, and terms may apply. Equal housing opportunity provider ## 8 QR Code Link to This Post\\n \\n \\n Lease our 2x1 Apartment with Juliet starting at $1660+ Prices Subject to change daily! \\n Apartment Features: \\nPrivate Balconies &amp; Patios \\nLuxury Vinyl Wood Flooring \\nDesigner Interior Paint \\nUpgraded cabinets and countertops \\nModern white doors and trim \\nModern white base boards \\nEnergy efficient appliances \\nSpacious bathroom \\nTONS of closet and storage space \\nSac State Shuttle-including Sac State Shuttle App! PET FRIENDLY - MEOW! WOOF! \\n\\nUpgraded Fitness Centers \\nTanning Booth \\nHydro Massage Table \\nResident Business Center \\nResort-Style Pool Furniture \\nSparkling Waterfall Wall \\nFire pit locations \\nBeach Area for lounging \\nOutdoor Recreation Area \\nSand Volleyball Court \\nGrilling Locations Call today to schedule your tour ... show contact info\\n -or- \\nStop in to visit Mon - Sat 10:00a - 5:00p for more info! :D \\n\\n\\nCommunity Amenities: \\nComplete fitness center \\nFour sparkling pools \\nLighted tennis courts \\nFull basketball court \\nWhirlpool Jacuzzi \\n24 hour Emergency Maintenance Hornet Shuttle Access to Sacramento State University \\n Easy Access to Public Transportation (lightrail and bus) \\n Easy Access to the American River Trail And so much more! \\n\\n Check us out online @ www.thephoenixsacramento.com TODAY! \\n Connect with us: \\nFacebook = The Phoenix Apartment Living \\nTwitter = @thephoenixlife Prices and availability are subject to change. Offered prices are starting prices for base rent only. Other charges, conditions, fees, and terms may apply. Equal housing opportunity provider ## 9 QR Code Link to This Post\\n \\n \\n Lease our 2x2 Apartment with Juliet starting at $1877+ Prices Subject to change daily! \\n Apartment Features: \\nPrivate Balconies &amp; Patios \\nLuxury Vinyl Wood Flooring \\nDesigner Interior Paint \\nUpgraded cabinets and countertops \\nModern white doors and trim \\nModern white base boards \\nEnergy efficient appliances \\nSpacious bathroom \\nTONS of closet and storage space \\nSac State Shuttle-including Sac State Shuttle App! PET FRIENDLY - MEOW! WOOF! \\n\\nUpgraded Fitness Centers \\nTanning Booth \\nHydro Massage Table \\nResident Business Center \\nResort-Style Pool Furniture \\nSparkling Waterfall Wall \\nFire pit locations \\nBeach Area for lounging \\nOutdoor Recreation Area \\nSand Volleyball Court \\nGrilling Locations Call today to schedule your tour ... show contact info\\n -or- \\nStop in to visit Mon - Sat 10:00a - 5:00p for more info! :D \\n\\n\\nCommunity Amenities: \\nComplete fitness center \\nFour sparkling pools \\nLighted tennis courts \\nFull basketball court \\nWhirlpool Jacuzzi \\n24 hour Emergency Maintenance Hornet Shuttle Access to Sacramento State University \\n Easy Access to Public Transportation (lightrail and bus) \\n Easy Access to the American River Trail And so much more! \\n\\n Check us out online @ www.thephoenixsacramento.com TODAY! \\n Connect with us: \\nFacebook = The Phoenix Apartment Living \\nTwitter = @thephoenixlife Prices and availability are subject to change. Offered prices are starting prices for base rent only. Other charges, conditions, fees, and terms may apply. Equal housing opportunity provider ## 10 QR Code Link to This Post\\n \\n \\n Lease our 2x2 Apartment with Che starting at $1611+ Prices Subject to change daily! \\n Apartment Features: \\nPrivate Balconies &amp; Patios \\nLuxury Vinyl Wood Flooring \\nDesigner Interior Paint \\nUpgraded cabinets and countertops \\nModern white doors and trim \\nModern white base boards \\nEnergy efficient appliances \\nSpacious bathroom \\nTONS of closet and storage space \\nSac State Shuttle-including Sac State Shuttle App! PET FRIENDLY - MEOW! WOOF! \\n\\nUpgraded Fitness Centers \\nTanning Booth \\nHydro Massage Table \\nResident Business Center \\nResort-Style Pool Furniture \\nSparkling Waterfall Wall \\nFire pit locations \\nBeach Area for lounging \\nOutdoor Recreation Area \\nSand Volleyball Court \\nGrilling Locations Call today to schedule your tour ... show contact info\\n -or- \\nStop in to visit Mon - Sat 10:00a - 5:00p for more info! :D \\n\\n\\nCommunity Amenities: \\nComplete fitness center \\nFour sparkling pools \\nLighted tennis courts \\nFull basketball court \\nWhirlpool Jacuzzi \\n24 hour Emergency Maintenance Hornet Shuttle Access to Sacramento State University \\n Easy Access to Public Transportation (lightrail and bus) \\n Easy Access to the American River Trail And so much more! \\n\\n Check us out online @ www.thephoenixsacramento.com TODAY! \\n Connect with us: \\nFacebook = The Phoenix Apartment Living \\nTwitter = @thephoenixlife Prices and availability are subject to change. Offered prices are starting prices for base rent only. Other charges, conditions, fees, and terms may apply. Equal housing opportunity provider ## 11 QR Code Link to This Post\\n \\n \\n Lease our 2x2 Apartment with Che starting at $1736+ Prices Subject to change daily! \\n Apartment Features: \\nPrivate Balconies &amp; Patios \\nLuxury Vinyl Wood Flooring \\nDesigner Interior Paint \\nUpgraded cabinets and countertops \\nModern white doors and trim \\nModern white base boards \\nEnergy efficient appliances \\nSpacious bathroom \\nTONS of closet and storage space \\nSac State Shuttle-including Sac State Shuttle App! PET FRIENDLY - MEOW! WOOF! \\n\\nUpgraded Fitness Centers \\nTanning Booth \\nHydro Massage Table \\nResident Business Center \\nResort-Style Pool Furniture \\nSparkling Waterfall Wall \\nFire pit locations \\nBeach Area for lounging \\nOutdoor Recreation Area \\nSand Volleyball Court \\nGrilling Locations Call today to schedule your tour ... show contact info\\n -or- \\nStop in to visit Mon - Sat 10:00a - 5:00p for more info! :D \\n\\n\\nCommunity Amenities: \\nComplete fitness center \\nFour sparkling pools \\nLighted tennis courts \\nFull basketball court \\nWhirlpool Jacuzzi \\n24 hour Emergency Maintenance Hornet Shuttle Access to Sacramento State University \\n Easy Access to Public Transportation (lightrail and bus) \\n Easy Access to the American River Trail And so much more! \\n\\n Check us out online @ www.thephoenixsacramento.com TODAY! \\n Connect with us: \\nFacebook = The Phoenix Apartment Living \\nTwitter = @thephoenixlife Prices and availability are subject to change. Offered prices are starting prices for base rent only. Other charges, conditions, fees, and terms may apply. Equal housing opportunity provider ## latitude longitude city date_posted date_updated ## 1 38.5728 -121.4675 2520 S St 2021-02-04 2021-03-03 ## 7 38.5511 -121.4068 The Phoenix/Sacramento/Folsom/SF 2021-03-02 &lt;NA&gt; ## 8 38.5511 -121.4068 The Phoenix/Sacramento/Folsom/SF 2021-03-04 &lt;NA&gt; ## 9 38.5511 -121.4068 The Phoenix/Sacramento/Folsom/SF 2021-03-02 &lt;NA&gt; ## 10 38.5511 -121.4068 The Phoenix/Sacramento/Folsom/SF 2021-03-02 &lt;NA&gt; ## 11 38.5511 -121.4068 The Phoenix/Sacramento/Folsom/SF 2021-03-03 &lt;NA&gt; ## price deleted sqft bedrooms bathrooms pets laundry parking craigslist ## 1 1125 FALSE 550 1 1 &lt;NA&gt; shared off-street sacramento ## 7 1615 FALSE 816 2 1 both shared covered sacramento ## 8 1660 FALSE 816 2 1 both shared covered sacramento ## 9 1877 FALSE 916 2 2 both shared covered sacramento ## 10 1611 FALSE 916 2 2 both shared covered sacramento ## 11 1736 FALSE 916 2 2 both shared covered sacramento ## shp_place shp_city shp_state shp_county ## 1 Sacramento Sacramento CA Sacramento ## 7 Sacramento Sacramento CA Sacramento ## 8 Sacramento Sacramento CA Sacramento ## 9 Sacramento Sacramento CA Sacramento ## 10 Sacramento Sacramento CA Sacramento ## 11 Sacramento Sacramento CA Sacramento To get all four categories, we’d have to do this four times. If we want to compute something for each category, say the mean of the price column, we also have to repeat that computation four times. Here’s what it would look like for just the shared category: mean(shared$price, na.rm = TRUE) ## [1] 1522.869 If the categories were elements, we could avoid writing code to index each category, and just use the sapply (or lapply) function to apply the mean function to each. The split function splits a vector or data frame into groups based on a vector of categories. The first argument to split is the data, and the second argument is a congruent vector of categories. We can use split to elegantly compute means of price broken down by laundry. First, we split the data by category. Since we only want to compute on the price column, we only split that column: by_laundry = split(cl$price, cl$laundry) class(by_laundry) ## [1] &quot;list&quot; names(by_laundry) ## [1] &quot;hookup&quot; &quot;in-unit&quot; &quot;none&quot; &quot;shared&quot; The result from split is a list with one element for each category. The individual elements contain pieces of the original price column: head(by_laundry$hookup) ## [1] 1250 3300 3328 3300 3280 3328 Since the categories are elements in the split data, now we can use sapply the same way we did in previous examples: sapply(by_laundry, mean, na.rm = TRUE) ## hookup in-unit none shared ## 2059.6111 1880.8592 982.2381 1522.8691 This two-step process is a data science idiom called the split-apply pattern. First you use split to convert categories into list elements, then you use an apply function to compute something on each category. Any time you want to compute results by category, you should think of this pattern. The split-apply pattern is so useful that R provides the tapply function as a shortcut. The tapply function is equivalent to calling split and then sapply. Like split, the first argument is the data and the second argument is a congruent vector of categories. The third argument is a function to apply, like the function argument in sapply. We can use tapply to compute the price means by laundry type for the Craigslist data: tapply(cl$price, cl$laundry, mean, na.rm = TRUE) ## hookup in-unit none shared ## 2059.6111 1880.8592 982.2381 1522.8691 Notice that the result is identical to the one we computed before. The “t” in tapply stands for “table”, because the tapply function is a generalization of the table function. If you use length as the third argument to tapply, you get the same results as you would from using the table function on the category vector. The aggregate function is closely related to tapply. It computes the same results, but organizes them into a data frame with one row for each category. In some cases, this format is more convenient. The arguments are the same, except that the second argument must be a list or data frame rather than a vector. As an example, here’s the result of using aggregate to compute the price means: aggregate(cl$price, list(cl$laundry), mean, na.rm = TRUE) ## Group.1 x ## 1 hookup 2059.6111 ## 2 in-unit 1880.8592 ## 3 none 982.2381 ## 4 shared 1522.8691 The lapply, sapply, and tapply functions are the three most important functions in the family of apply functions, but there are many more. You can learn more about all of R’s apply functions by reading this StackOverflow post. "],["data-structures.html", "12 Data Structures 12.1 Tabular Data 12.2 Tree / Document Data Structures 12.3 Relational Databases 12.4 Non-Hierarchical Relational Data 12.5 Geospatial Data", " 12 Data Structures Merriam-Webster’s Dictionary defines data as: factual information (such as measurements or statistics) used as a basis for reasoning, discussion, or calculation information in digital form that can be transmitted or processed information output by a sensing device or organ that includes both useful and irrelevant or redundant information and must be processed to be meaningful Several key principals are introduced in the above definition: data is an intermediary step leading towards some form of analysis or or presentation, not typically an end in itself data comes in multiple formats, both digital and analogue data can be collected by both humans and machines not all data in a given dataset is necessarily meaningful, correct nor useful Data Scientists (as differentiated from statisticians or computer scientists, for example) are expert in understanding the nature of data itself and the steps necessary to assess the suitability of a given data set for answering specific research questions and the work required to properly prepare data for successful analysis. In the broadest terms, we call this process data forensics. The first step in the data forensics process is understanding the format(s) through which data are stored and transferred. 12.1 Tabular Data Tabular Data is the most ubiquitous form of data storage and the one most familiar to most users. Tabular data consists of organizing data in a table of rows and columns. Traditionally, each column in the table represents a Field of Variable and each row represents an observation or entity. For example, the table below shows a tabular organization of a subset of the mtcars dataset: Table 1.1: mpg cyl disp hp Mazda RX4 21.0 6 160.0 110 Mazda RX4 Wag 21.0 6 160.0 110 Datsun 710 22.8 4 108.0 93 Hornet 4 Drive 21.4 6 258.0 110 Hornet Sportabout 18.7 8 360.0 175 Valiant 18.1 6 225.0 105 Duster 360 14.3 8 360.0 245 Merc 240D 24.4 4 146.7 62 Merc 230 22.8 4 140.8 95 Merc 280 19.2 6 167.6 123 12.2 Tree / Document Data Structures Another popular form of data structure is the Tree structure, sometimes referred to as a Document based data structure. Tree data structures present data in a hierarchical tree-like structure in which all items related back to a single, root node. A “Family Tree” is a good example of tree structured data: The mtcars data from the above table can also be represented using a tree structure: The above image visually depicts the mtcars data as a tree, which works well for a human reader but is no parsable by the computer. There are a variety of ways to represent tree data as a computer file (or data stream) so that it can be read and parsed by the computer. In this class, we will cover two of the most popular formats: XML and JSON. 12.2.1 Structuring Data as XML XML is stands for Extensible Markup Language. Markup languages have been around since the 1960’s and were originally developed as a means to adding structured information to an existing unstructured text. In the days of analogue text preparation, professional editors typically used a blue or red pencil to make notes on typed manuscripts. The use of a specially collored pen or pencil for “marking up” documents, as the procedure was known in the industry, easily allowed subsequent readers to distinguish between editorial comment and formatting notes placed on typed manuscripts from the texts themselves. Computerized markup languages were developed as a means of allowing data specialists to markup a text in a manner that would allow the computer to distinguish between textual content and meta-information (information about the text) about the text when both types of information appear in the same file. XML is the most widely used form of markup today. In fact, nearly every webpage that you have ever viewed is actually an XML document that contains both content to be displayed and instructions for the computer on how to display that content embedded in the file using XML Tags, which are simply instructions contained with the special charcters “&lt;” and “&gt;”. For example, consider the following short email text: To: Tavi From: Jonna Subject: Meeting Date: Thursday, February 4, 2021 at 2:46 PM Don&#39;t forget about meeting with Sarah next week, 2pm in room 242. Thanks, Jonna This email contains quite a bit of structured email (sender, receiver, date/time, etc.), but there is no easy way for the computer easily extract this structure. We can solve this problem by using XML to embed information about the structure directly in the document as follows: &lt;head&gt; &lt;to&gt;Tavi&lt;/to&gt; &lt;from&gt;Jonna&lt;/from&gt; &lt;subject&gt;Meeting&lt;/subject&gt; &lt;datetime&gt; &lt;dayofweek&gt;Thursday&lt;/dayofweek&gt; &lt;month&gt;February&lt;/month&gt; &lt;day&gt;4&lt;/day&gt; &lt;year&gt;2021&lt;/year&gt; &lt;time&gt;2:46 PM&lt;/time&gt; &lt;/datetime&gt; &lt;/head&gt; &lt;body&gt; Don&#39;t forget about meeting with Sarah next week, 2pm in room 242. Thanks, &lt;signature&gt;Jonna&lt;/signuature&gt; &lt;/body&gt; By using XML, we are able to identify specific information in the email in a way that the computer is a capable of parsing. This allows us to use computational methods to easily extract information in bulk from many emails and it also allows us to program a computer program, such as an email client, to organize and properly display all of the parts of the email. The above XML example illustrates several important aspects of XML: All XML tags are enclosed in “&lt;” and “&gt;” symbols. There are 2 primary types of tags, opening tags, which designate the beginning character that is defined by the tag, and closing tags, which designate the end of the portion of the text to be associated with the opening tag. Closing tags are always indicated by slash character where tag is the name of the opening tag that is being closed. Tags be be embedded within each other in a tree-like structure. However, any tags opened within a tag must be closed before parent tag can be closed. For example, &lt;name&gt;&lt;first&gt;John&lt;/first&gt; &lt;last&gt;Doe&lt;/last&gt;&lt;/name&gt; is valid, but &lt;name&gt;&lt;first&gt;John&lt;/first&gt; &lt;last&gt;Doe&lt;/name&gt;&lt;/last&gt; is not valid. While XML was oringinally developed as a means of embedding meta information about a text directly in a text, it also quickly evolved into a stand-alone means of representing tree-structured data for exchange between computer systems. To this end, many computer applications use XML to store, share, and retrieve data. For exmaple, we can represent the data in our truncated mtcars dataset as XML as follows: &lt;cars&gt; &lt;make id=&quot;mazda&quot;&gt; &lt;model id=&quot;RX4&quot;&gt; &lt;mpg&gt;21.0&lt;/mpg&gt; &lt;cyl&gt;6&lt;/cyl&gt; &lt;disp&gt;160.0&lt;/disp&gt; &lt;hp&gt;110&lt;/hp&gt; &lt;/model&gt; &lt;model id=&quot;RX4 Wag&quot;&gt; &lt;mpg&gt;21.0&lt;/mpg&gt; &lt;cyl&gt;6&lt;/cyl&gt; &lt;disp&gt;160.0&lt;/disp&gt; &lt;hp&gt;110&lt;/hp&gt; &lt;/model&gt; &lt;/make&gt; &lt;make id=&quot;Datsun&quot;&gt; &lt;model id=&quot;710&quot;&gt; &lt;mpg&gt;22.8&lt;/mpg&gt; &lt;cyl&gt;4&lt;/cyl&gt; &lt;disp&gt;108.0&lt;/disp&gt; &lt;hp&gt;93&lt;/hp&gt; &lt;/model&gt; &lt;/make&gt; &lt;make id=&quot;Hornet&quot;&gt; &lt;model id=&quot;4 Drive&quot;&gt; &lt;mpg&gt;21.4&lt;/mpg&gt; &lt;cyl&gt;6&lt;/cyl&gt; &lt;disp&gt;258.0&lt;/disp&gt; &lt;hp&gt;110&lt;/hp&gt; &lt;/model&gt; &lt;model id=&quot;Sportabout&quot;&gt; &lt;mpg&gt;18.7&lt;/mpg&gt; &lt;cyl&gt;8&lt;/cyl&gt; &lt;disp&gt;360.0&lt;/disp&gt; &lt;hp&gt;175&lt;/hp&gt; &lt;/model&gt; &lt;/make&gt; &lt;make id=&quot;Valiant&quot;&gt; &lt;model id=&quot;valiant&quot;&gt; &lt;mpg&gt;18.1&lt;/mpg&gt; &lt;cyl&gt;6&lt;/cyl&gt; &lt;disp&gt;225.0&lt;/disp&gt; &lt;hp&gt;105&lt;/hp&gt; &lt;/model&gt; &lt;/make&gt; &lt;make id=&quot;Duster&quot;&gt; &lt;model id=&quot;360&quot;&gt; &lt;mpg&gt;14.3&lt;/mpg&gt; &lt;cyl&gt;8&lt;/cyl&gt; &lt;disp&gt;360.0&lt;/disp&gt; &lt;hp&gt;245&lt;/hp&gt; &lt;/model&gt; &lt;/make&gt; &lt;make id=&quot;Merc&quot;&gt; &lt;model id=&quot;240D&quot;&gt; &lt;mpg&gt;24.4&lt;/mpg&gt; &lt;cyl&gt;4&lt;/cyl&gt; &lt;disp&gt;146.7&lt;/disp&gt; &lt;hp&gt;62&lt;/hp&gt; &lt;/model&gt; &lt;model id=&quot;230&quot;&gt; &lt;mpg&gt;22.8&lt;/mpg&gt; &lt;cyl&gt;4&lt;/cyl&gt; &lt;disp&gt;140.8&lt;/disp&gt; &lt;hp&gt;95&lt;/hp&gt; &lt;/model&gt; &lt;model id=&quot;280&quot;&gt; &lt;mpg&gt;19.2&lt;/mpg&gt; &lt;cyl&gt;6&lt;/cyl&gt; &lt;disp&gt;167.6&lt;/disp&gt; &lt;hp&gt;123&lt;/hp&gt; &lt;/model&gt; &lt;/make&gt; &lt;/cars&gt; For an XML dataset to be technically valid, the tags used to markup the dataset must themselves be defined according to a schema, another XML document that defines all tags that can be used in marking up a dataset and the allowable tree structure of the markup (for example, which tags can be parents of which other tags, etc.). You do not need to understand, or even know, the schema being used to present data in order to read and parse an XML document. However, schemas are extremely useful (and often necessary) for building applications that perform advanced processing of XML documents, such as web browsers, emial clients, etc. For more information on XML and XML Schemas see the w3schools XML Tutorial at https://www.w3schools.com/xml/. 12.2.2 Structuring Data as JSON XML provides an excellent framework for encoding, saving, and transfering all kinds of data, and it was the dominant mode of transfering data across the internet for many years. However, XML has an Achilles’ Heel from the data transfer perspective: a lack of sparcity. If you look closely at the mtcars dataset XML example above, you will note that the markup accounts for more of the total characters in the document than the data itself. In a world where data is regularly being exchanged in real time across the network, the use of XML can result in the necessity to exchange a lot more data to accomplish the same task. This adds both time and cost to every data transaction. JavaScript Object Notation (JSON) was developed as a standard to address this problem and provides a sparse framework for representing data that introduces minimal, non-data elements into the overal data structure. JSON uses a key/value pair structure to represent data elements: &quot;model&quot;:&quot;RX4&quot; Individual data elements are then grouped to reflect more complex data structures: {&quot;model&quot;: {&quot;id&quot;: &quot;2&quot;, &quot;hp&quot;: &quot;120&quot;}} The example below shows the subsetted mtcars dataset represented as JSON. Note the use of the [ character to indicated repeated elements in the data: { &quot;cars&quot;: [{ &quot;make&quot;: &quot;Mazda&quot;, &quot;model&quot;: [{ &quot;id&quot;: &quot;RX4&quot;, &quot;mpg&quot;: &quot;21.0&quot;, &quot;cyl&quot;: &quot;6&quot;, &quot;disp&quot;: &quot;160.0&quot;, &quot;hp&quot;: &quot;110&quot; }, { &quot;id&quot;: &quot;RX4 Wag&quot;, &quot;mpg&quot;: &quot;21.0&quot;, &quot;cyl&quot;: &quot;6&quot;, &quot;disp&quot;: &quot;160.0&quot;, &quot;hp&quot;: &quot;110&quot; } ] }, { &quot;make&quot;: &quot;Datsun&quot;, &quot;model&quot;: { &quot;id&quot;: &quot;710&quot;, &quot;mpg&quot;: &quot;22.8&quot;, &quot;cyl&quot;: &quot;4&quot;, &quot;disp&quot;: &quot;108.0&quot;, &quot;hp&quot;: &quot;93&quot; } }, { &quot;make&quot;: &quot;Hornet&quot;, &quot;model&quot;: [{ &quot;id&quot;: &quot;4 Drive&quot;, &quot;mpg&quot;: &quot;21.4&quot;, &quot;cyl&quot;: &quot;6&quot;, &quot;disp&quot;: &quot;258.0&quot;, &quot;hp&quot;: &quot;110&quot; }, { &quot;id&quot;: &quot;Sportabout&quot;, &quot;mpg&quot;: &quot;18.7&quot;, &quot;cyl&quot;: &quot;8&quot;, &quot;disp&quot;: &quot;360.0&quot;, &quot;hp&quot;: &quot;175&quot; } ] }, { &quot;make&quot;: &quot;Valiant&quot;, &quot;model&quot;: { &quot;id&quot;: &quot;valiant&quot;, &quot;mpg&quot;: &quot;18.1&quot;, &quot;cyl&quot;: &quot;6&quot;, &quot;disp&quot;: &quot;225.0&quot;, &quot;hp&quot;: &quot;105&quot; } }, { &quot;make&quot;: &quot;Duster&quot;, &quot;model&quot;: { &quot;id&quot;: &quot;360&quot;, &quot;mpg&quot;: &quot;14.3&quot;, &quot;cyl&quot;: &quot;8&quot;, &quot;disp&quot;: &quot;360.0&quot;, &quot;hp&quot;: &quot;245&quot; } }, { &quot;make&quot;: &quot;Merc&quot;, &quot;model&quot;: [{ &quot;id&quot;: &quot;240D&quot;, &quot;mpg&quot;: &quot;24.4&quot;, &quot;cyl&quot;: &quot;4&quot;, &quot;disp&quot;: &quot;146.7&quot;, &quot;hp&quot;: &quot;62&quot; }, { &quot;id&quot;: &quot;230&quot;, &quot;mpg&quot;: &quot;22.8&quot;, &quot;cyl&quot;: &quot;4&quot;, &quot;disp&quot;: &quot;140.8&quot;, &quot;hp&quot;: &quot;95&quot; }, { &quot;id&quot;: &quot;280&quot;, &quot;mpg&quot;: &quot;19.2&quot;, &quot;cyl&quot;: &quot;6&quot;, &quot;disp&quot;: &quot;167.6&quot;, &quot;hp&quot;: &quot;123&quot; } ] } ] } For information on the JSON format, see the Tutorialspoint JSON Tutorial at https://www.tutorialspoint.com/json/index.htm. You can also use the JSONLint Json Validator at https://jsonlint.com/ to check the syntax of any JSON representation. 12.3 Relational Databases Relational Databases, frequently referred to as Relational Database Management Systems (RDBMS), provide another way of structuring data. Unlike tabular, XML, and JSON data representations, RDBMS data is not easily human readable and specialized software is usually required to interact with data stored as relational data. Most programming environments (including r) provide specialized drivers for communicating with RDBMS in order to facilitate working with data stored in these systems. RDBMS have three primary purposes as a data storage format: To reduce duplication of data; To speed-up access and insertion of new data; To insure data integrity. Items 2 and 3 above are accomplished at the software level, by deploying strict checks on data input, complex data indexing systems, and implementing redundant, automated, backup systems, to name just a few of the functionalities offered by RDBMS. Item 1 above, reducing duplication of data, is accomplished by using a specific, “relational” data structure that encourages the use of controlled lists of data mapped to individual observations. Looking at our mtcars subset data, for example, we see that while there are ten observations, there are only 6 makes of cars. To represent this in RDBMS, we first create a “Table,” a named collection of data, that contains a unique list of car makes: Table 1.2: MAKE_TABLE id Make 1 Mazda 2 Datsun 3 Hornet 4 Valiant 5 Duster 6 Merc Once we have a table of unique lists, we then create and populate a table of our cars, associating each car with its appropriate make from the MAKE_TABLE table: Table 12.1: CARS_TABLE Make Model mpg cyl disp hp 1 RX4 21.0 6 160.0 110 1 RX4 Wag 21.0 6 160.0 110 2 710 22.8 4 108.0 93 3 4 Drive 21.4 6 258.0 110 3 Sportabout 18.7 8 360.0 175 4 Valiant 18.1 6 225.0 105 5 360 14.3 8 360.0 245 6 240D 24.4 4 146.7 62 6 230 22.8 4 140.8 95 6 280 19.2 6 167.6 123 In the above table, we only normalized the car Make field. In a fully normalized RDBMS data structure, we would also create a control table for the Model field in anticipation of the fact that we could have more than one observation for a given model. Fully normalized RBDMS data structures use control tables for all fields that contain string data. The image below shows a sample Entry Relationship Diagram (ERD) for a more complex dataset relating to course offerings and enrollments. Each line connecting two tables marks a field in a “join” table that uses the id field in a control table (known as a foreign key) to associate information in the control table with the records in the join table. 12.4 Non-Hierarchical Relational Data In the era of the social network, it is becoming increasingly necessary to represent relationships between entities that are not hierarchical. Unlike a family tree, the fact that you are connected to someone on Facebook or Instagram does not imply any type of hierarchical relationship. Such networks are typically represented using the Graph data structure: Graphs consists of collections of vertices or nodes, the entities being graphed, and edges, the relationships between nodes. Another important aspect of graph data is the concept of directionality. A directed graph indicates the direction of the relationship identified by the edge. We might, for example, wish to draw edges that indicate that one node was influenced by another node, in which case we could identify an “influence” edge and use directionality to indicate who influenced whom: Graph data can be stored and or transferred using any of the data formats discussed above or using specialized graph databases management software. 12.5 Geospatial Data Geospatial data represents a final type of data with it’s own, unique data structure. Geospatial data unique because it always relates directly to the physical world and, because it relies on world-wide standards which have been in development and communally accepted for hundreds of years. Because of its uniqueness as a data type, geospatial data will be covered as a stand-alone topic later in the course. "],["data-visualization.html", "13 Data Visualization 13.1 Learning Objectives 13.2 R Graphics Overview 13.3 The Grammar of Graphics 13.4 Designing a Visualization 13.5 Remembering factors", " 13 Data Visualization 13.1 Learning Objectives After this lesson, you should be able to: Explain the difference between strings and factors Convert strings to factors Explain the grammar of graphics With the ggplot2 package: Make various kinds of plots Save plots Choose an appropriate kind of plot based on the data 13.2 R Graphics Overview There are three popular systems for creating visualizations in R: The base R functions (primarily the plot function) The lattice package The ggplot2 package These three systems are not interoperable! Consequently, it’s best to choose one to use exclusively. Compared to base R, both lattice and ggplot2 are better at handling grouped data and generally require less code to create a nice-looking visualization. The ggplot2 package is so popular that there are now knockoff packages for other data-science-oriented programming languages like Python and Julia. The package is also part of the Tidyverse. Because of these advantages, we’ll use ggplot2 for visualizations in this and all future lessons. 13.3 The Grammar of Graphics ggplot2 has detailed documentation and also a cheatsheet. The “gg” in ggplot2 stands for grammar of graphics. The idea of a grammar of graphics is that visualizations can be built up in layers. In ggplot2, the three layers every plot must have are: Data Geometry Aesthetics There are also several optional layers. Here are a few: Layer Description scales Title, label, and axis value settings facets Side-by-side plots guides Axis and legend position settings annotations Shapes that are not mapped to data coordinates Coordinate systems (Cartesian, logarithmic, polar) 13.3.1 Making a Plot As an example, let’s plot the class survey data from the Intro to R and Functions lessons – by now this should be saved into your working directory. First, let’s load in our data. survey = read.csv(&quot;data/class_survey.csv&quot;) str(survey) ## &#39;data.frame&#39;: 11 obs. of 5 variables: ## $ place : chr &quot;Temple&quot; &quot;Yakitori&quot; &quot;Panera&quot; &quot;Yakitori&quot; ... ## $ distance.mi: num 0.9 0.6 0.8 0.6 2 100 0.6 0.7 0.8 1 ... ## $ time.min : int 5 4 4 12 10 2 3 4 4 5 ... ## $ major : chr &quot;chicanx studies&quot; &quot;human development&quot; &quot;economics&quot; &quot;undeclared&quot; ... ## $ pets : chr &quot;woof&quot; &quot;woof&quot; &quot;cat&quot; &quot;woof&quot; ... Next we need to load ggplot2. As always, if this is your first time using the package, you’ll have to install it. Then you can load the package: # install.packages(&quot;ggplot2&quot;) library(ggplot2) ## Registered S3 methods overwritten by &#39;tibble&#39;: ## method from ## format.tbl pillar ## print.tbl pillar What kind of plot should we make? It depends on what data we want the plot to show. Let’s make a plot that shows the distance in miles (from the library) against the biking time in minutes for each place in the data. Both the distance and the biking time are recorded as numbers. A scatter plot is a good choice for displaying two numeric features. Later we’ll learn about other options, but for now we’ll make a scatter plot. Layer 1: Data The data layer determines the data set used to make the plot. ggplot and most other Tidyverse packages are designed for working with tidy data frames. Tidy means: Each observation has its own row. Each feature has its own column. Each value has its own cell. Tidy data sets are convenient in general. A later lesson will cover how to make an untidy data set tidy. Until then, we’ll take it for granted that the data sets we work with are tidy. To set up the data layer, call the ggplot function on a data frame: ggplot(survey) This returns a blank plot. We still need to add a few more layers. Layer 2: Geometry The geometry layer determines the shape or appearance of the visual elements of the plot. In other words, the geometry layer determines what kind of plot to make: one with points, lines, boxes, or something else. There are many different geometries available in ggplot2. The package provides a function for each geometry, always prefixed with geom_. To add a geometry layer to the plot, choose the geom_ function you want and add it to the plot with the + operator: ggplot(survey) + geom_point() ## Error: geom_point requires the following missing aesthetics: x and y This returns an error message that we’re missing aesthetics x and y. We’ll learn more about aesthetics in the next section, but this error message is especially helpful: it tells us exactly what we’re missing. When you use a geometry you’re unfamiliar with, it can be helpful to run the code for just the data and geometry layer like this, to see exactly which aesthetics need to be set. As we’ll see later, it’s possible to add multiple geometries to a plot. Layer 3: Aesthetics The aesthetic layer determines the relationship between the data and the geometry. Use the aesthetic layer to map features in the data to aesthetics (visual elements) of the geometry. The aes function creates an aesthetic layer. The syntax is: aes(AESTHETIC = FEATURE, ...) The names of the aesthetics depend on the geometry, but some common ones are x, y, color, fill, shape, and size. There is more information about and examples of aesthetic names in the documentation. For example, if we want to put the distance_mi feature on the x-axis, the aesthetic layer should be: aes(x = distance_mi) In the aes function, column names are never quoted. Unlike most layers, the aesthetic layer is not added to the plot with the + operator. Instead, you can pass the aesthetic layer as the second argument to the ggplot function: ggplot(survey, aes(x = distance.mi, y = time.min)) + geom_point() If you want to set an aesthetic to a constant value, rather than one that’s data dependent, do so outside of the aesthetic layer. For instance, suppose we want to make the points blue: ggplot(survey, aes(x = distance.mi, y = time.min)) + geom_point(color = &quot;blue&quot;) If you set an aesthetic to a constant value inside of the aesthetic layer, the results you get might not be what you expect: ggplot(survey, aes(x = distance.mi, y = time.min, color = &quot;blue&quot;)) + geom_point() This plot also demonstrates the importance of visualization. We can see that one of the survey response distances is around 100 miles in 2 minutes, which we’ve identified is likely an error. Before we continue, let’s remove all responses that are more than 99 miles from campus (later we will talk about cleaning and correcting data, but for now we will just remove it): survey = survey[survey$distance.mi &lt;= 99, ] Per-geometry Aesthetics When you pass an aesthetic layer to the ggplot function, it applies to the entire plot. You can also set an aesthetic layer individually for each geometry, by passing the layer as the first argument in the geom_ function: ggplot(survey) + geom_point(aes(x = distance.mi, y = time.min)) This is really only useful when you have multiple geometries. As an example, let’s color-code the points by major: ggplot(survey, aes(x = distance.mi, y = time.min, color = major)) + geom_point() Now let’s also add labels to each point. To do this, we need to add another geometry: ggplot(survey, aes(x = distance.mi, y = time.min, color = major, label = major)) + geom_point() + geom_text(size = 2) Where we put the aesthetics matters: ggplot(survey, aes(x = distance.mi, y = time.min, label = major)) + geom_point() + geom_text(aes(color = major), size = 2) Layer 4: Scales The scales layer controls the title, axis labels, and axis scales of the plot. Most of the functions in the scales layer are prefixed with scale_, but not all of them. The labs function is especially important, because it’s used to set the title and axis labels: ggplot(survey, aes(x = distance.mi, y = time.min)) + geom_point() + labs(title = &quot;Biking time vs distance&quot;, x = &quot;Distance (mi)&quot;, y = &quot;Biking time (min)&quot;) 13.3.2 Saving Plots In ggplot2, use the ggsave function to save the most recent plot you created: ggsave(&quot;scatter_plot.png&quot;) The file format is selected automatically based on the extension. Common formats are PNG and PDF. The Plot Device You can also save a plot with one of R’s “plot device” functions. The steps are: Call a plot device function: png, jpeg, pdf, bmp, tiff, or svg. Run your code to make the plot. Call dev.off to indicate that you’re done plotting. This strategy works with any of R’s graphics systems (not just ggplot2). Here’s an example: # Run these lines in the console, not the notebook! jpeg(&quot;scatter_plot.jpeg&quot;) ggplot(survey, aes(x = distance.mi, y = time.min)) + geom_point() dev.off() 13.3.3 Example: Bar Plot Let’s say we want to plot the number of responses for each major. A bar plot is an appropriate way to represent this visually. The geometry for a bar plot is geom_bar. Since bar plots are mainly used to display frequencies, the geom_bar function automatically computes frequencies when given mapped to a categorical feature. So we can write: ggplot(survey, aes(x = major)) + geom_bar() + labs(title = &quot;Majors in Responses&quot;, x = &quot;Major&quot;, y = &quot;Count&quot;) To prevent geom_bar from computing frequencies automatically, set stat = \"identity\". This is mainly useful if you want to plot quantities you’ve computed manually on the y-axis. 13.4 Designing a Visualization What plot is appropriate? Variable Versus Plot categorical bar, dot categorical categorical bar, dot, mosaic numerical box, density, histogram numerical categorical box, density, ridge numerical numerical line, scatter, smooth scatter If you want to add a: 3rd numerical variable, use it to change point/line sizes. 3rd categorical variable, use it to change point/line styles. 4th categorical variable, use side-by-side plots. Also: Always add a title and axis labels. These should be in plain English, not variable names! Specify units after the axis label if the axis has units. For instance, “Height (ft)”. Don’t forget that many people are colorblind! Also, plots are often printed in black and white. Use point and line styles to distinguish groups; color is optional. Add a legend whenever you’ve used more than one point or line style. Always write a few sentences explaining what the plot reveals. Don’t describe the plot, because the reader can just look at it. Instead, explain what they can learn from the plot and point out important details that are easily overlooked. Sometimes points get plotted on top of each other. This is called overplotting. Plots with a lot of overplotting can be hard to read and can even misrepresent the data by hiding how many points are present. Use a two-dimensional density plot or jitter the points to deal with overplotting. For side-by-side plots, use the same axis scales for both plots so that comparing them is not deceptive. 13.5 Remembering factors A feature is categorical if it measures a qualitative category. For example, the genres rock, blues, alternative, folk, pop are categories. R uses the class factor to represent categorical data. Visualizations and statistical models sometimes treat factors differently than other data types, so it’s important to make sure you have the right data type. If you’re ever unsure, remember that you can check the class of an object with the class function. When you load a data set, R usually can’t tell which features are categorical. That means identifying and converting the categorical features is up to you. This can be especially helpful to know if you are plotting categories, but want them to have a particular order, other than the default (alphabetical). Let’s think about which features are categorical in the class survey data. str(survey) ## &#39;data.frame&#39;: 10 obs. of 5 variables: ## $ place : chr &quot;Temple&quot; &quot;Yakitori&quot; &quot;Panera&quot; &quot;Yakitori&quot; ... ## $ distance.mi: num 0.9 0.6 0.8 0.6 2 0.6 0.7 0.8 1 3.7 ## $ time.min : int 5 4 4 12 10 3 4 4 5 19 ## $ major : chr &quot;chicanx studies&quot; &quot;human development&quot; &quot;economics&quot; &quot;undeclared&quot; ... ## $ pets : chr &quot;woof&quot; &quot;woof&quot; &quot;cat&quot; &quot;woof&quot; ... The numeric columns in this data set (bike_min, walk_min, and distance_mi) are all quantitative, so they’re not categorical. That leaves the character columns. The major, place, and pets columns are all characters and it is easy to imagine grouping these survey responses to summarize the data. Especially major, where there are multiple of the same majors, we can say this is categorical. For place, there are more distinct responses, so it’s not as clear. The locations are: table(survey$place) ## ## Guads Lazi Cow Pachamama Panera Raising Canes ## 1 1 1 1 1 ## Tea List Temple Wok of Flame Yakitori ## 1 1 1 2 So there are a couple of locations that aren’t unique, but most of them are. For this data set, we probably shouldn’t treat place as categorical because the groups wouldn’t be interesting. However, if we had more survey responses (or more at each location) it might make sense to do so. Let’s convert the major column to a factor. To do this, use the factor function: factor(survey$major) ## [1] chicanx studies human development economics undeclared ## [5] psychology psychology undeclared human development ## [9] undeclared GG ## 6 Levels: chicanx studies economics GG human development ... undeclared survey$major = factor(survey$major) Notice that factors are printed differently than strings. The categories of a factor are called levels. You can list the levels with the levels function: levels(survey$major) ## [1] &quot;chicanx studies&quot; &quot;economics&quot; &quot;GG&quot; ## [4] &quot;human development&quot; &quot;psychology&quot; &quot;undeclared&quot; Notice that factors default the level order to be in alphabetical order. This is also true when we plot characters, which you can notice if we again call the bar plot we made that counted up the number of majors in our survey. ggplot(survey, aes(x = major)) + geom_bar() + labs(title = &quot;Majors in Responses&quot;, x = &quot;Major&quot;, y = &quot;Count&quot;) 13.5.1 Assigning factor levels But what if we wanted to change the order of the x axis to be in descending order? We can change the level assignments. The easier way to do this is to re-write the variable as a factor again, but specify the order of the levels as an argument in the factor() function. survey$major &lt;- factor(survey$major, levels = c(&quot;undeclared&quot;, &quot;psychology&quot;, &quot;human development&quot;, &quot;GG&quot;, &quot;economics&quot;, &quot;chicanx studies&quot;)) levels(survey$major) ## [1] &quot;undeclared&quot; &quot;psychology&quot; &quot;human development&quot; ## [4] &quot;GG&quot; &quot;economics&quot; &quot;chicanx studies&quot; Now we can run the same plotting code, but see that the order of the x axis changed, based on the levels of the major factor. ggplot(survey, aes(x = major)) + geom_bar() + labs(title = &quot;Majors in Responses&quot;, x = &quot;Major&quot;, y = &quot;Count&quot;) "],["how-the-web-works.html", "14 How the Web Works 14.1 Client-Server Architecture 14.2 Understanding URLs", " 14 How the Web Works The discipline of Data Science was, in a large part, ushered into being by the increasing availability of information available on the World Wide Web or through other internet sources. Prior to the popularization of the internet as a publishing and communications platform, the majority of scientific research involved controlled studies in which researchers would collect their own data through various direct means of data collection (surveys, medical testing, etc.) in order to test a stated hypothesis. The vast amount of information available on the internet disrupted this centuries long dominance. Today, the dominant form of scientific research involves using data collected or produced by others for reasons having little or nothing to do with the research question being investigated by scholar. Users who post items about their favorite political candidate are not, for example, doing this so that sociologists can better under how politics function in America. However, their Tweets are being used in that and many other unforseen capacities. Because the internet provides such a rich trove of information for study, understanding how to effectively get, process, and prepare information from the internet for scientific research is a crucial skill for any data scientist. And in order to understand these workflows, the data scientist must first understand how the internet itself functions. 14.1 Client-Server Architecture The base architecture and functioning of the internet is quite simple: A content producer puts information on a computer called the server for others to retrieve; A user uses their local computer, called the client, to request the information from the sever; The server delivers the information to the client. Each of the above detailed steps is accomplished using a technically complex but conceptually simple set of computer protocols. The technical details are beyond the scope of this course. We are here concerned with their conceptual architecture. 14.1.1 Communication Between Clients and Servers Anytime a computer connects to any network, that computer is assigned a unique identifier known as an internet protocol (IP) address that uniquely identifies that computer on the network. IP addresses have the form x.x.x.x, where each x can be any integer from 0 to 255. For example, 169.237.102.141 is the current IP address of the computer that hosts the DataLab website. IP addresses are sometimes pre-designated for particular computers. A pre-designated IP address is known as static IP address. In other cases IP addresses are dynamically assigned from a range of available IP Address using a system known as the Dynamic Host Configuration Protocol (DHCP). Servers are typically assigned static IP addresses and clients are typically assigned dynamic IP addresses. As humans, we are used to accessing websites via a domain name (which we’ll discuss shortly), but you can also contact any server on the internet by simply typing the IP address into your browser address bar where you would normally enter the URL. For example, you can simply click on https://169.237.102.141 to access the DataLab website. (note: your browser may give you a security warning if you try to access a server directly using an IP address. For the link above, it is safe to proceed to the site.) 14.1.2 Domain Name Resolution IP addresses are the unique identifiers that make the internet work, but they are not very human friendly. To solve this problem, a system of domain name resolution was created. Under this system, internet service providers access a universal domain registry database that associates human readable domain names with machine readable IP addresses, and a secondary set of of internet connected servers known as domain name servers (DNS) provide a lookup service that translates domain names into IP addresses in the background. As the end-user, you enter and see only domain names, but the actual request process is a multi-step process in which domain names are translated to IP address in the background: A content produce puts information on a computer called the server for others to retrieve; A user uses their local computer, called the client, to request the information from the sever using a domain name using request software such as a web browser; The user’s client software first sends a request to a DNS server to retrieve the IP address of the server on the network associated with the entered domain name; The DNS server returns the associated IP address to the client; The client then makes the information request to the server using its retrieved IP address; The server delivers the information to the client. 14.1.3 Request Routing Our simple digram of the client server process shows only two computers. But when you connect to the internet you are not, of course, creating a direct connection to a single computer. Rather, you are connecting to vase network of literally millions of computers, what we have come to refer to as the cloud. In order to solve this problem, the internet backbone also deploys a routing system that directs requests and responses across the network to the appropriate servers and clients. When you connect to the WiFi network in your home, office, or the local coffee house, you are connecting to a router. That router receives all of your requests and, provided you are not requesting something directly from another computer that is connected to the same router, passes that request on to a larger routing network at the Internet Service Provider (ISP). When the ISP routers receive your request, they check to see if you’re requesting something from a computer that is connected to their network. If it is, they deliver the request. If it is not, they pass the request on to another, regional routing network. And this routing process is repeated until your request if finally routed to the correct server. A content produce puts information on a computer called the server for others to retrieve; A user uses their local computer, called the client, to request the information from the sever using a domain name using request software such as a web browser; The user’s client software first sends a request to a DNS server to retrieve the IP address of the server on the network associated with the entered domain name; The DNS server returns the associated IP address to the client; The client sends the request to the local (in home, office, etc.) router; After check of IP addresses on local network, request is routed to the ISP’s routing system; The request is passed through the internet routing network until it reaches the routing system of the server’s ISP and, finally, the server itself. 14.1.4 The Server Response When a request is sent to a server across the internet, the request includes both the specific URL of the resource being request and also an hidden request header. The request header provides information to the server such as the IP address and the operating system of the client, the transfer protocol being used, and the software on the client that is making the request. The server uses this information to properly format it’s response and to route it back to the requesting client using the same IP routing process as described above. 14.1.5 Internet Transfer Protocols All of the information transferred between computers over the network is transfered as streams of binary data. In order to ensure data integrity, these streams are usually broken up into smaller packets of data which are transmitted independent of each other and then reassembled by the receiving computer once it has received all of the packets in the stream. The first packet returned (a header packet) typically delivers information about how many packets the client should expect to receive and about how they should be reassembled to recreate the original data stream. There are many different standards for how data streams are divided into packets. One standard might, for example, break the stream into a collection of 50 byte packets, while another might use 100 byte packages. These standards are called protocols. The two protocols that are familiar to most users are http and https, which define the hypertext transfer protocol and its sibling the hypertext transfer secure protocol. When you type a url like https://datalab.ucdavis.edu into your browser, you are instructing the browser to use the https protocol to exchange information. Because http and https are so common, most modern browsers do not require you to type the protocol name. They will simply insert the protocol for you in the background. 14.2 Understanding URLs URL is an acronym for Uniform Resource Locators. “Uniform” is a key term in this context. URLs are not arbitrary pointers to information. They are machine and human readable and parsable and contain a lot of information in them. All URLs are constructed using a standardized format. Consider the following URL: https://sfbaywildlife.info/species/common_birds.htm There are actually several distinct components to the above URL Table 1.1: protocol server path to file https:// sfbaywildlife.info /species/common_birds.htm We’ve already discussed Internet Protocols and domain names. The file path portion of the URL can also provide valuable information about the server. It reads exactly like a Unix file path on the command line. The path /species/common_birds.htm indicates that the file common_birds.htm is in the species directory on the server. 14.2.1 Dynamic Files In the above example, when you enter the URL https://sfbaywildlife.info/species/common_birds.htm, your browser requests the file at /species/common_birds.html on the server. The server simply finds the file and delivers it to your web browser. We call this a static web server because the server itself does not do any processing of files prior to delivery. It simply receives requests for files living on the server and then sends them to the client, whose browser renders the file for viewing. Many websites, however, use dynamic processing. Pages with file extensions such as .php or .jsp, for example, include computer code in them. When these pages are requested by the server, the server executes the code in the designated file and sends the output of that execution to the requesting client rather than the actual file. Many sites, such as online stores and blogs, use this functionality to connect their web pages to active databases that track inventory and orders, for example. 14.2.2 Query Strings Dynamic websites, such as e-commerce sites that are connected to databases, require a mechanism for users to submit information to the server for processing. This is accomplished through one of two HTTP commands: GET or POST. POST commands send submitted information to the server via a hidden HTTP header that is invisible to the end user. Scraping sites that require POST transactions is possible but can require significant sleuthing to determine the correct parameters and is beyond the scope of this course. GET requests, which are, happily for web scrapers more ubiquitous than POST requests, are much easier to understand. They are submitted via a query string that is simply appended to the request URL as in the following example: https://ebba.english.ucsb.edu/search_combined/?ft=dragon&amp;numkw=52 Here we see a Query String appended to the end of the actual URL: Table 1.2: protocol server path to file query string https:// ebba.english.ucsb.edu /search_combined/index.php ?ft=dragon&amp;numkw=52 Query strings always appear at the end of the URL and begin with the ? character followed by a series of key/value pairs separated by the &amp; character. In the above example we see that two parameters are submitted to the server via the query string as follows: ft=dragon numkw=52 The server will use these parameter values as input to perform a dynamic operation, in this case searching a database. "],["web-scraping.html", "15 Web Scraping 15.1 Getting Data from the Web 15.2 R’s XML Parsers 15.3 XPath 15.4 The Web Scraping Workflow 15.5 Case Study: CA Cities 15.6 Case Study: The CA Aggie 15.7 CSS Selectors", " 15 Web Scraping Scraping a web page means extracting information so that it can be used programmatically (for instance, in R). After this lesson, you should be able to: Explain and read hypertext markup language (HTML) View the HTML source of a web page Use Firefox or Chrome’s web developer tools to locate tags within a web page With the rvest package: Read HTML into R Extract HTML tables as data frames With the xml2 package: Use XPath to extract specific elements of a page 15.1 Getting Data from the Web Ways you can get data from the web, from most to least convenient: Direct download or “data dump” R or Python package (there are packages for many popular web APIs) Documented web API Undocumented web API Scraping 15.1.1 What’s in a Web Page? Modern web pages usually consist of many files: Hypertext markup language (HTML) for structure and formatting Cascading style sheets (CSS) for more formatting JavaScript (JS) for interactivity Images HTML is the only component that always has to be there. Since HTML is what gives a web page structure, it’s what we’ll focus on when scraping. HTML is closely related to eXtensible markup language (XML). Both languages use tags to mark structural elements of data. In HTML, the elements literally correspond to the elements of a web page: paragraphs, links, tables, and so on. Most tags come in pairs. The opening tag marks the beginning of an element and the closing tag marks the end. Opening tags are written &lt;NAME&gt;, where NAME is the name of the tag. Closing tags are written &lt;/NAME&gt;. A singleton tag is a tag that stands alone, rather than being part of a pair. Singleton tags are written &lt;NAME /&gt;. In HTML (but not XML) they can also be written &lt;NAME&gt;. Fortunately, HTML only has a few singleton tags, so they can be distinguished by name regardless of which way they’re written. For example, here’s some HTML that uses the em (emphasis, usually italic) and strong (usually bold) tags, as well as the singleton br (line break) tag: &lt;em&gt;&lt;strong&gt;This text&lt;/strong&gt; is emphasized.&lt;br /&gt;&lt;/em&gt; Not emphasized A pair of tags can contain other elements (paired or singleton tags), but not a lone opening or closing tag. This creates a strict, treelike hierarchy. Opening and singleton tags can have attributes that contain additional information. Attributes are name-value pairs written NAME=\"VALUE\" after the tag name. For instance, the HTML a (anchor) tag creates a link to the URL provided for the href attribute: &lt;a href=&quot;http://www.google.com/&quot; id=&quot;mytag&quot;&gt;My Search Engine&lt;/a&gt; In this case the tag also has a value set for the id attribute. Now let’s look at an example of HTML for a complete, albeit simple, web page: &lt;html&gt; &lt;head&gt; &lt;title&gt;This is the page title!&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;This is a header!&lt;/h1&gt; &lt;p&gt;This is a paragraph. &lt;a href=&quot;http://www.r-project.org/&quot;&gt;Here&#39;s a website!&lt;/a&gt; &lt;/p&gt; &lt;p id=&quot;hello&quot;&gt;This is another paragraph.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; In most web browsers, you can examine the HTML for a web page by right-clicking and choosing “View Page Source”. See here for a more detailed explanation of HTML, and here for a list of valid HTML elements. 15.2 R’s XML Parsers A parser converts structured data into familiar data structures. R has two popular packages for parsing XML (and HTML): The “XML” package The “xml2” package The XML package has more features. The xml2 package is more user-friendly, and as part of the Tidyverse, it’s relatively well-documented. This lesson focuses on xml2, since most of the additional features in the XML package are related to writing (rather than parsing) XML documents. The xml2 package is often used in conjunction with the “rvest” package, which provides support for CSS selectors (described later in this lesson) and automates scraping HTML tables. The first time you use these packages, you’ll have to install them: install.packages(&quot;xml2&quot;) install.packages(&quot;rvest&quot;) Let’s start by parsing the example of a complete web page from earlier. The xml2 function read_xml reads an XML document, and the rvest function read_html reads an HTML document. Both accept an XML/HTML string or a file path (including URLs): html = r&quot;( &lt;html&gt; &lt;head&gt; &lt;title&gt;This is the page title!&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;This is a header!&lt;/h1&gt; &lt;p&gt;This is a paragraph. &lt;a href=&quot;http://www.r-project.org/&quot;&gt;Here&#39;s a website!&lt;/a&gt; &lt;/p&gt; &lt;p id=&quot;hello&quot;&gt;This is another paragraph.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; )&quot; library(xml2) library(rvest) doc = read_html(html) doc ## {html_document} ## &lt;html&gt; ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... ## [2] &lt;body&gt;\\n &lt;h1&gt;This is a header!&lt;/h1&gt;\\n &lt;p&gt;This is a paragraph.\\n ... The xml_children function returns all of the immediate children of a given element. The top element of our document is the html tag, and its immediate children are the head and body tags: tags = xml_children(doc) The result from xml_children is a node set (xml_nodeset object). Think of a node set as a vector where the elements are tags rather than numbers or strings. Just like a vector, you can access individual elements with the indexing (square bracket [) operator: length(tags) ## [1] 2 head = tags[1] head ## {xml_nodeset (1)} ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... The xml_text function returns the text contained in a tag. Let’s get the text in the title tag, which is beneath the head tag. First we isolate the tag, then use xml_text: title = xml_children(head) xml_text(title) ## [1] &quot;&quot; &quot;This is the page title!&quot; Navigating through the tags by hand is tedious and easy to get wrong, but fortunately there’s a better way to find the tags we want. 15.3 XPath An XML document is a tree, similar to the file system on your computer: html ├── head │ └── title └── body ├── h1 ├── p └── p └── a When we wanted to find files, we wrote file paths. We can do something similar to find XML elements. XPath is a language for writing paths to elements in an XML document. XPath is not R-specific. At a glance, an XPath looks similar to a file path: XPath Description / root, or element separator . current tag .. parent tag * any tag (wildcard) The xml2 function xml_find_all finds all elements at given XPath: xml_find_all(doc, &quot;/html/body/p&quot;) ## {xml_nodeset (2)} ## [1] &lt;p&gt;This is a paragraph.\\n &lt;a href=&quot;http://www.r-project.org/&quot;&gt;Here&#39;s ... ## [2] &lt;p id=&quot;hello&quot;&gt;This is another paragraph.&lt;/p&gt; Unlike a file path, an XPath can identify multiple elements. If you only want a specific element, use indexing to get it from the result. XPath also has some features that are different from file paths. The // separator means “at any level beneath.” It’s a useful shortcut when you want to find a specific element but don’t care where it is. Let’s get all of the p elements at any level of the document: xml_find_all(doc, &quot;//p&quot;) ## {xml_nodeset (2)} ## [1] &lt;p&gt;This is a paragraph.\\n &lt;a href=&quot;http://www.r-project.org/&quot;&gt;Here&#39;s ... ## [2] &lt;p id=&quot;hello&quot;&gt;This is another paragraph.&lt;/p&gt; Let’s also get all a elements at any level beneath a p element: xml_find_all(doc, &quot;//p/a&quot;) ## {xml_nodeset (1)} ## [1] &lt;a href=&quot;http://www.r-project.org/&quot;&gt;Here&#39;s a website!&lt;/a&gt; The vertical bar | means “or.” You can use it to get two different sets of elements in one query. Let’s get all h1 or p tags: xml_find_all(doc, &quot;//h1|//p&quot;) ## {xml_nodeset (3)} ## [1] &lt;h1&gt;This is a header!&lt;/h1&gt; ## [2] &lt;p&gt;This is a paragraph.\\n &lt;a href=&quot;http://www.r-project.org/&quot;&gt;Here&#39;s ... ## [3] &lt;p id=&quot;hello&quot;&gt;This is another paragraph.&lt;/p&gt; 15.3.1 Predicates In XPath, the predicate operator [] gets elements at a position or matching a condition. Most conditions are about the attributes of the element. In the predicate operator, attributes are always prefixed with @. For example, suppose we want to find all tags where the id attribute is equal to \"hello\": xml_find_all(doc, &quot;//*[@id = &#39;hello&#39;]&quot;) ## {xml_nodeset (1)} ## [1] &lt;p id=&quot;hello&quot;&gt;This is another paragraph.&lt;/p&gt; Notice that the equality operator in XPath is =, not ==. Strings in XPath can be quoted with single or double quotes. You can combine multiple conditions in the predicate operator with and and or. There are also several XPath functions you can use in the predicate operator. These functions are not R functions, but rather built into XPath. Here are a few: XPath Description not() negation contains() check string x contains y text() get tag text substring() get a substring For instance, suppose we want to get elements that contain the word “paragraph”: xml_find_all(doc, &quot;//*[contains(text(), &#39;paragraph&#39;)]&quot;) ## {xml_nodeset (2)} ## [1] &lt;p&gt;This is a paragraph.\\n &lt;a href=&quot;http://www.r-project.org/&quot;&gt;Here&#39;s ... ## [2] &lt;p id=&quot;hello&quot;&gt;This is another paragraph.&lt;/p&gt; Finally, note that you can also use the predicate operator to get elements at a specific position. For example, to get the second p element anywhere in the document: xml_find_all(doc, &quot;//p[2]&quot;) ## {xml_nodeset (1)} ## [1] &lt;p id=&quot;hello&quot;&gt;This is another paragraph.&lt;/p&gt; Notice that this is the same as if we had used R to get the second element: xml_find_all(doc, &quot;//p&quot;)[2] ## {xml_nodeset (1)} ## [1] &lt;p id=&quot;hello&quot;&gt;This is another paragraph.&lt;/p&gt; Beware that although the XPath predicate operator resembles R’s indexing operator, the syntax is not always the same. We’ll learn more XPath in the examples. There’s a complete list of XPath functions on Wikipedia. 15.4 The Web Scraping Workflow Scraping a web page is part technology, part art. The goal is to find an XPath that’s concise but specific enough to identify only the elements you want. If you plan to scrape the web page again later or want to scrape a lot of similar web pages, the XPath also needs to be general enough that it still works even if there are small variations. Firefox and Chrome include “web developer tools” that are invaluable for planning a web scraping strategy. Press Ctrl + Shift + i (Cmd + Shift + i on OS X) in Firefox or Chrome to open the web developer tools. We can also use the web developer tools to interactively identify the element that corresponds to a specific part of a web page. Press Ctrl + Shift + c and then click on the part of the web page you want to identify. The best way to approach web scraping (and programming in general) is as an incremental, iterative process. Use the web developer tools to come up with a basic strategy, try it out in R, check which parts don’t work, and then repeat to adjust the strategy. Expect to go back and forth between your web browser and R several times when you’re scraping. Most scrapers follow the same four steps, regardless of the web page and the language of the scraper: Download pages with an HTTP request (usually GET) Parse pages to extract text Clean up extracted text with string methods or regex Save cleaned results In R, xml2’s read_xml function takes care of step 1 for you, although you can also use httr functions to make the request yourself. 15.4.1 Being Polite Making an HTTP request is not free! It has a real cost in CPU time and also cash. Server administrators will not appreciate it if you make too many requests or make requests too quickly. So: If you’re making multiple requests, slow them down by using R’s Sys.sleep function to make R do nothing for a moment. Aim for no more than 20-30 requests per second, unless you’re using an API that says more are okay. Avoid requesting the same page twice. One way to do this is by caching (saving) the results of the requests you make. You can do this manually, or use a package that does it automatically, like the httpcache package. Failing to be polite can get you banned from websites! Also check the website’s terms of service to make sure scraping is not explicitly forbidden. 15.5 Case Study: CA Cities Wikipedia has many pages that are just tables of data. For example, there’s this list of cities and towns in California. Let’s scrape the table to get a data frame. Step 1 is to download the page: wiki_url = &quot;https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_California&quot; wiki_doc = read_html(wiki_url) Step 2 is to extract the table element from the page. We can use Firefox or Chrome’s web developer tools to identify the table. HTML tables usually use the table tag. Let’s see if it’s the only table in the page: tables = xml_find_all(wiki_doc, &quot;//table&quot;) tables ## {xml_nodeset (4)} ## [1] &lt;table class=&quot;wikitable&quot;&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th scope=&quot;row&quot; style=&quot;background ... ## [2] &lt;table class=&quot;wikitable plainrowheaders sortable&quot;&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th scop ... ## [3] &lt;table class=&quot;nowraplinks hlist mw-collapsible autocollapse navbox-inner&quot; ... ## [4] &lt;table class=&quot;nowraplinks mw-collapsible autocollapse navbox-inner&quot; style ... The page has 4 tables. We can either make our XPath more specific, or use indexing to get the table we want. Refining the XPath makes our scraper more robust, but indexing is easier. For the sake of learning, let’s refine the XPath. Going back to the browser, we can see that the table includes \"wikitable\" and \"sortable\" in its class attribute. So let’s search for these among the table elements: tab = xml_find_all(tables, &quot;//*[contains(@class, &#39;sortable&#39;)]&quot;) tab ## {xml_nodeset (1)} ## [1] &lt;table class=&quot;wikitable plainrowheaders sortable&quot;&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th scop ... Now we get just one table! Here we used a second XPath applied only to the results from the first, but we also could’ve done this all with one XPath: //table[contains(@class, 'sortable')]. The next part of extracting the data is to extract the value from each individual cell in the table. HTML tables have a strict layout order, with tags to indicate rows and cells. We could extract each cell by hand and then reassemble them into a data frame, but the rvest function html_table can do it for us automatically: cities = html_table(tab, fill = TRUE) cities = cities[[1]] head(cities) ## Name Type County Population (2010)[1][8][9] Land area[1] ## 1 Name Type County Population (2010)[1][8][9] sq mi ## 2 Adelanto City San Bernardino 31,765 56.01 ## 3 Agoura Hills City Los Angeles 20,330 7.79 ## 4 Alameda City Alameda 73,812 10.61 ## 5 Albany City Alameda 18,539 1.79 ## 6 Alhambra City Los Angeles 83,089 7.63 ## Land area[1] Incorporated[7] ## 1 Land area[1] km2 ## 2 145.1 December 22, 1970 ## 3 20.2 December 8, 1982 ## 4 27.5 April 19, 1854 ## 5 4.6 September 22, 1908 ## 6 19.8 July 11, 1903 The fill = TRUE argument ensures that empty cells are filled with NA. We’ve successfully imported the data from the web page into R, so we’re done with step 2. 15.5.1 Data Cleaning Step 3 is to clean up the data frame. The column names contain symbols, the first row is part of the header, and the column types are not correct. # Fix column names. names(cities) = c(&quot;city&quot;, &quot;type&quot;, &quot;county&quot;, &quot;population&quot;, &quot;mi2&quot;, &quot;km2&quot;, &quot;date&quot;) # Remove fake first row. cities = cities[-1, ] # Reset row names. rownames(cities) = NULL How can we clean up the date column? The as.Date function converts a string into a date R understands. The idea is to match the date string to a format string where the components of the date are indicated by codes that start with %. For example, %m stands for the month as a two-digit number. You can read about the different date format codes in ?strptime. Here’s the code to convert the dates in the data frame: dates = as.Date(cities$date, &quot;%B %m, %Y&quot;) cities$date = dates We can also convert the population to a number: class(cities$population) ## [1] &quot;character&quot; # Remove commas and footnotes (e.g., [1]) before conversion library(stringr) pop = str_replace_all(cities$population, &quot;,&quot;, &quot;&quot;) pop = str_replace_all(pop, &quot;\\\\[[0-9]+\\\\]&quot;, &quot;&quot;) pop = as.numeric(pop) # Check for missing values, which can mean conversion failed any(is.na(pop)) ## [1] FALSE cities$population = pop 15.6 Case Study: The CA Aggie Suppose we want to scrape The California Aggie. In particular, we want to get all the links to news articles on the features page https://theaggie.org/features/. This could be one part of a larger scraping project where we go on to scrape individual articles. First, let’s download the features page so we can extract the links: url = &quot;https://theaggie.org/features/&quot; doc = read_html(url) We know that links are in a tags, but we only want links to articles. Looking at the features page with the web developer tools, the links to feature articles are all inside of a section tag. So let’s get the section tag: xml_find_all(doc, &quot;//section&quot;) # OR html_nodes(doc, &quot;section&quot;) ## {xml_nodeset (1)} ## [1] &lt;section id=&quot;blog-grid&quot;&gt;&lt;div class=&quot;blog-grid-container&quot;&gt;\\n\\n\\t\\t\\t\\t\\t&lt;d ... Just to be safe, let’s also use the id attribute, which is \"blog-grid\". Usually the id of an element is unique, so this ensures that we get the right section even if later the web developer for The Aggie adds other section tags. We can also add in a part about getting links now: section = xml_find_all(doc, &quot;//section[@id = &#39;blog-grid&#39;]&quot;) # OR html_nodes(doc, &quot;section#blog-grid&quot;) links = xml_find_all(section, &quot;//a&quot;) # OR html_nodes(section, &quot;a&quot;) length(links) ## [1] 267 That gives us 267 links, but there are only 15 articles on the page, so something’s still not right. Inspecting the page again, there are actually three links to each article: on the image, on the title, and on “Continue Reading”. Let’s focus on the title link. There are a couple different ways we can identify the title link: Always inside an h2 tag Always has title attribute that starts with “Permanent” Generally it’s more robust to rely on tags (structure) than to rely on attributes (other than id and class). So let’s use the h2 tag here: links = xml_find_all(section, &quot;//h2/a&quot;) # OR html_nodes(section, &quot;h2 &gt; a&quot;) length(links) ## [1] 15 Now we’ve got the 15 links, so let’s get the URLs from the href attribute. feature_urls = xml_attr(links, &quot;href&quot;) The other article listings (Sports, Science, etc) on The Aggie have a similar structure, so we can potentially reuse our code to scrape those. So let’s turn our code into a function. The input will be a downloaded page, and the output will be the article links. parse_article_links = function(page) { section = xml_find_all(page, &quot;//section[@id = &#39;blog-grid&#39;]&quot;) links = xml_find_all(section, &quot;//h2/a&quot;) xml_attr(links, &quot;href&quot;) } We can test this out on the Sports page. First we download the page: sports = read_html(&quot;https://theaggie.org/sports&quot;) Then we call the function on the document: sports_urls = head(parse_article_links(sports)) head(sports_urls) ## [1] &quot;https://theaggie.org/2021/02/19/pickleball-the-rising-sport/&quot; ## [2] &quot;https://theaggie.org/2021/02/19/2021-olympic-games-are-in-limbo/&quot; ## [3] &quot;https://theaggie.org/2021/02/12/wild-year-for-football-culminates-in-super-bowl/&quot; ## [4] &quot;https://theaggie.org/2021/02/12/american-sports-sees-a-lack-of-diversity-in-team-coach-composition/&quot; ## [5] &quot;https://theaggie.org/2021/02/10/preview-of-the-uefa-champions-league-knockout-stage/&quot; ## [6] &quot;https://theaggie.org/2021/02/05/out-of-the-bubble-trouble/&quot; It looks like the function works even on other pages! We can also set up the function to extract the link to the next page, in case we want to scrape multiple pages of links. The link to the next page of features (an arrow at the bottom) is an a tag with class next. Let’s see if that’s specific enough to isolate the tag: next_page = xml_find_all(doc, &quot;//a[contains(@class, &#39;next&#39;)]&quot;) # OR html_nodes(doc, &quot;a.next&quot;) It looks like it is. We use contains here rather than = because it is common for the class attribute to have many parts. It only has one here, but using contains makes our code robust against changes in the future. We can now modify our parser function to return the link to the next page: parse_article_links = function(page) { # Get article URLs section = xml_find_all(page, &quot;//section[@id = &#39;blog-grid&#39;]&quot;) links = xml_find_all(section, &quot;//h2/a&quot;) urls = xml_attr(links, &quot;href&quot;) # Get next page URL next_page = xml_find_all(page, &quot;//a[contains(@class, &#39;next&#39;)]&quot;) next_url = xml_attr(next_page, &quot;href&quot;) # Using a list allows us to return two objects list(urls = urls, next_url = next_url) } Since our function gets URL for the next page, what happens on the last page? Looking at the last page in the browser, there is no link to the next page. Let’s see what our scraper function does: last_page = read_html(&quot;https://theaggie.org/features/page/180/&quot;) parse_article_links(last_page) ## $urls ## [1] &quot;https://theaggie.org/2008/03/14/daily-calendar/&quot; ## [2] &quot;https://theaggie.org/2008/03/14/facial-hair-takes-root-at-uc-davis/&quot; ## [3] &quot;https://theaggie.org/2008/03/13/daily-calendar/&quot; ## [4] &quot;https://theaggie.org/2008/03/12/corrections/&quot; ## [5] &quot;https://theaggie.org/2008/03/12/daily-calendar/&quot; ## [6] &quot;https://theaggie.org/2008/03/12/five-years-in-iraq-part-two/&quot; ## [7] &quot;https://theaggie.org/2008/03/11/daily-calendar/&quot; ## [8] &quot;https://theaggie.org/2008/03/11/deals-around-davis/&quot; ## [9] &quot;https://theaggie.org/2008/03/11/five-years-in-iraq-part-one/&quot; ## ## $next_url ## character(0) We get an empty character vector as the URL for the next page. This is because the xml_find_all function returns an empty node set for the next page URL, so there aren’t any href fields for xml_attr to extract. It’s convenient that the xml2 functions behave this way, but we could also add an if-statement to the function to check (and possibly return NA as the next URL in this case). Then the code becomes: parse_article_links = function(page) { # Get article URLs section = xml_find_all(page, &quot;//section[@id = &#39;blog-grid&#39;]&quot;) links = xml_find_all(section, &quot;//h2/a&quot;) urls = xml_attr(links, &quot;href&quot;) # Get next page URL next_page = xml_find_all(page, &quot;//a[contains(@class, &#39;next&#39;)]&quot;) if (length(next_page) == 0) { next_url = NA } else { next_url = xml_attr(next_page, &quot;href&quot;) } # Using a list allows us to return two objects list(urls = urls, next_url = next_url) } Now our function should work well even on the last page. If we want to scrape links to all of the articles in the features section, we can use our function in a loop: # NOTE: This code is likely to take a while to run, and is meant more for # reading than for you to run and try out. url = &quot;https://theaggie.org/features/&quot; article_urls = list() i = 1 # On the last page, the next URL will be `NA`. while (!is.na(url)) { # Download and parse the page. page = read_html(url) result = parse_article_links(page) # Save the article URLs in the `article_urls` list. The variable `i` is the # page number. article_urls[[i]] = result$url i = i + 1 # Set the URL to the next URL. url = result$next_url # Sleep for 1/30th of a second so that we never make more than 30 requests # per second. Sys.sleep(1/30) } Now we’ve got the basis for a simple scraper. 15.7 CSS Selectors Cascading style sheets (CSS) is a language used to control the formatting of an XML or HTML document. CSS selectors are the CSS way to write paths to elements. CSS selectors are more concise than XPath, so many people prefer them. Even if you prefer CSS selectors, it’s good to know XPath because CSS selectors are less flexible. Here’s the basic syntax of CSS selectors: CSS Description a tags a a &gt; b tags b directly beneath a a b tags b anywhere beneath a a, b tags a or b #hi tags with attribute id=\"hi\" .hi tags with attribute class that contains \"hi\" [foo=\"hi\"] tags with attribute foo=\"hi\" [foo*=\"hi\"] tags with attribute foo that contains \"hi\" If you want to learn more, CSS Diner is an interactive tutorial that covers the entire CSS selector language. In Firefox, you can get CSS selectors from the web developer tool. Right-click the tag you want a selector for and choose “Copy Unique Selector”. Beware that the selectors Firefox generates are often too specific to be useful for anything beyond the simplest web scrapers. The rvest package uses CSS selectors by default. Behind the scenes, the package translates these into XPath and passes them to xml2. Here are a few examples of CSS selectors, using rvest’s html_nodes function: html = r&quot;( &lt;html&gt; &lt;head&gt; &lt;title&gt;This is the page title!&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;This is a header!&lt;/h1&gt; &lt;p&gt;This is a paragraph. &lt;a href=&quot;http://www.r-project.org/&quot;&gt;Here&#39;s a website!&lt;/a&gt; &lt;/p&gt; &lt;p id=&quot;hello&quot;&gt;This is another paragraph.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; )&quot; doc = read_html(html) # Get all p elements html_nodes(doc, &quot;p&quot;) ## {xml_nodeset (2)} ## [1] &lt;p&gt;This is a paragraph.\\n &lt;a href=&quot;http://www.r-project.org/&quot;&gt;Here&#39;s ... ## [2] &lt;p id=&quot;hello&quot;&gt;This is another paragraph.&lt;/p&gt; # Get all links html_nodes(doc, &quot;a&quot;) ## {xml_nodeset (1)} ## [1] &lt;a href=&quot;http://www.r-project.org/&quot;&gt;Here&#39;s a website!&lt;/a&gt; # Get all tags with id=&quot;hello&quot; html_nodes(doc, &quot;#hello&quot;) ## {xml_nodeset (1)} ## [1] &lt;p id=&quot;hello&quot;&gt;This is another paragraph.&lt;/p&gt; "],["strings-and-regular-expressions.html", "16 Strings and Regular Expressions 16.1 Printing Output 16.2 Escape Sequences 16.3 Character Encodings 16.4 The Tidyverse 16.5 The stringr Package 16.6 Regular Expressions", " 16 Strings and Regular Expressions After this lesson, you should be able to: Print strings with cat Read and write escape sequences and raw strings With the stringr package: Split strings on a pattern Replace parts of a string that match a pattern Extract parts of a string that match a pattern Read and write regular expressions, including: Anchors ^ and $ Character classes [] Quantifiers ?, *, and + Groups () 16.1 Printing Output The cat function prints a string in the R console. If you pass multiple arguments, they will be concatenated: cat(&quot;Hello&quot;) ## Hello cat(&quot;Hello&quot;, &quot;Nick&quot;) ## Hello Nick Pitfall 1: Printing a string is different from returning a string. The cat function only prints (and always returns NULL). For example: f = function() { cat(&quot;Hello&quot;) } x = f() ## Hello x ## NULL If you just want to concatenate some strings (but not necessarily print them), use paste instead of cat. The paste function returns a string. The str_c function in stringr (a package we’ll learn about later in this lesson) can also concatenate strings. Pitfall 2: Remember to print strings with the cat function, not the print function. The print function prints R’s representation of an object, the same as if you had entered the object in the console without calling print. For instance, print prints quotes around strings, whereas cat does not: print(&quot;Hello&quot;) ## [1] &quot;Hello&quot; cat(&quot;Hello&quot;) ## Hello 16.2 Escape Sequences In a string, an escape sequence or escape code consists of a backslash followed by one or more characters. Escape sequences make it possible to: Write quotes or backslashes within a string Write characters that don’t appear on your keyboard (for example, characters in a foreign language) For example, the escape sequence \\n corresponds to the newline character. Notice that the cat function translates \\n into a literal new line, whereas the print function doesn’t: x = &quot;Hello\\nNick&quot; cat(x) ## Hello ## Nick print(x) ## [1] &quot;Hello\\nNick&quot; As another example, suppose we want to put a literal quote in a string. We can either enclose the string in the other kind of quotes, or escape the quotes in the string: x = &#39;She said, &quot;Hi&quot;&#39; cat(x) ## She said, &quot;Hi&quot; y = &quot;She said, \\&quot;Hi\\&quot;&quot; cat(y) ## She said, &quot;Hi&quot; Since escape sequences begin with backslash, we also need to use an escape sequence to write a literal backslash. The escape sequence for a literal backslash is two backslashes: x = &quot;\\\\&quot; cat(x) ## \\ There’s a complete list of escape sequences for R in the ?Quotes help file. Other programming languages also use escape sequences, and many of them are the same as in R. 16.2.1 Raw Strings A raw string is a string where escape sequences are turned off. Raw strings are especially useful for writing regular expressions, which we’ll do later in this lesson. Raw strings begin with r\" and an opening delimiter (, [, or {. Raw strings end with a matching closing delimiter and quote. For example: x = r&quot;(quotes &quot; and backslashes \\)&quot; cat(x) ## quotes &quot; and backslashes \\ Raw strings were added to R in version 4.0 (April 2020), and won’t work correctly in older versions. 16.3 Character Encodings Computers store data as numbers. In order to store text on a computer, we have to agree on a character encoding, a system for mapping characters to numbers. For example, in ASCII, one of the most popular encodings in the United States, the character a maps to the number 97. Many different character encodings exist, and sharing text used to be an inconvenient process of asking or trying to guess the correct encoding. This was so inconvenient that in the 1980s, software engineers around the world united to create the Unicode standard. Unicode includes symbols for nearly all languages in use today, as well as emoji and many ancient languages (such as Egyptian hieroglyphs). Unicode maps characters to numbers, but unlike a character encoding, it doesn’t dictate how those numbers should be mapped to bytes (sequences of ones and zeroes). As a result, there are several different character encodings that support and are synonymous with Unicode. The most popular of these is UTF-8. In R, we can write Unicode characters with the escape sequence \\U followed by the number for the character in base 16. For instance, the number for a in Unicode is 97 (the same as in ASCII). In base 16, 97 is 61. So we can write an a as: x = &quot;\\U61&quot; # or &quot;\\u61&quot; x ## [1] &quot;a&quot; Unicode escape sequences are usually only used for characters that are not easy to type. For example, the cat emoji is number 1f408 (in base 16) in Unicode. So the string \"\\U1f408\" is the cat emoji. Note that being able to see printed Unicode characters also depends on whether the font your computer is using has a glyph (image representation) for that character. Many fonts are limited to a small number of languages. The NerdFont project patches fonts commonly used for programming so that they have better Unicode coverage. Using a font with good Unicode coverage is not essential, but it’s convenient if you expect to work with many different natural languages or love using emoji. 16.3.0.1 Character Encodings in Text Files Most of the time, R will handle character encodings for you automatically. However, if you ever read or write a text file (including CSV and other formats) and the text looks like gibberish, it might be an encoding problem. This is especially true on Windows, the only modern operating system that does not (yet) use UTF-8 as the default encoding. Encoding problems when reading a file can usually be fixed by passing the encoding to the function doing the reading. For instance, the code to read a UTF-8 encoded CSV file on Windows is: read.csv(&quot;my_data.csv&quot;, fileEncoding = &quot;UTF-8&quot;) Other reader functions may use a different parameter to set the encoding, so always check the documentation. On computers where the native language is not set to English, it can also help to set R’s native language to English with Sys.setlocale(locale = \"English\"). Encoding problems when writing a file are slightly more complicated to fix. See this blog post for thorough explanation. 16.4 The Tidyverse The Tidyverse is a popular collection of packages for doing data science in R. The packages are made by many of the same people that make RStudio. They provide alternatives to R’s built-in tools for: Manipulating strings (package stringr) Making visualizations (package ggplot2) Reading files (package readr) Manipulating data frames (packages dplyr, tidyr, tibble) And more Think of the Tidyverse as a different dialect of R. Sometimes the syntax is different, and sometimes ideas are easier or harder to express concisely. Whether to use base R or the Tidyverse is mostly subjective. As a result, the Tidyverse is somewhat polarizing in the R community. It’s useful to be literate in both, since both are popular. One advantage of the Tidyverse is that the packages are usually well-documented. For example, there are documentation websites and cheat sheets for most Tidyverse packages. 16.5 The stringr Package The rest of this lesson uses stringr, the Tidyverse package for string processing. R also has built-in functions for string processing. The main advantage of stringr is that all of the functions use a common set of parameters, so they’re easier to learn and remember. The first time you use stringr, you’ll have to install it with install.packages (the same as any other package). Then you can load the package with the library function: # install.packages(&quot;stringr&quot;) library(stringr) The typical syntax of a stringr function is: str_NAME(string, pattern, ...) Where: NAME describes what the function does string is the string to search within or transform pattern is the pattern to search for ... is additional, function-specific arguments For example, the str_detect function detects whether the pattern appears within the string: str_detect(&quot;hello&quot;, &quot;el&quot;) ## [1] TRUE str_detect(&quot;hello&quot;, &quot;ol&quot;) ## [1] FALSE Most of the stringr functions are vectorized: str_detect(c(&quot;hello&quot;, &quot;goodbye&quot;, &quot;lo&quot;), &quot;lo&quot;) ## [1] TRUE FALSE TRUE There are a lot of stringr functions. The remainder of this lesson focuses on three that are especially important, as well as some of their variants: str_split_fixed str_replace str_match You can find a complete list of stringr functions with examples in the documentation or cheat sheet. 16.5.1 Splitting Strings The str_split function splits the string at each position that matches the pattern. The characters that match are thrown away. For example, suppose we want to split a sentence into words. Since there’s a space between each word, we can use a space as the pattern: x = &quot;The students in this class are great!&quot; result = str_split(x, &quot; &quot;) result ## [[1]] ## [1] &quot;The&quot; &quot;students&quot; &quot;in&quot; &quot;this&quot; &quot;class&quot; &quot;are&quot; &quot;great!&quot; The str_split function always returns a list with one element for each input string. Here the list only has one element because x only has one element. We can get the first element with: result[[1]] ## [1] &quot;The&quot; &quot;students&quot; &quot;in&quot; &quot;this&quot; &quot;class&quot; &quot;are&quot; &quot;great!&quot; We have to use the double square bracket [[ operator here because x is a list (for a vector, we could use the single square bracket operator instead). Notice that in the printout for result, R gives us a hint that we should use [[ by printing [[1]]. To see why the function returns a list, consider what happens if we try to split two different sentences at once: x = c(x, &quot;Are you listening?&quot;) result = str_split(x, &quot; &quot;) result[[1]] ## [1] &quot;The&quot; &quot;students&quot; &quot;in&quot; &quot;this&quot; &quot;class&quot; &quot;are&quot; &quot;great!&quot; result[[2]] ## [1] &quot;Are&quot; &quot;you&quot; &quot;listening?&quot; Each sentence has a different number of words, so the vectors in the result have different lengths. So a list is the only way to store both. The str_split_fixed function is almost the same as str_split, but takes a third argument for the maximum number of splits to make. Because the number of splits is fixed, the function can return the result in a matrix instead of a list. For example: str_split_fixed(x, &quot; &quot;, 3) ## [,1] [,2] [,3] ## [1,] &quot;The&quot; &quot;students&quot; &quot;in this class are great!&quot; ## [2,] &quot;Are&quot; &quot;you&quot; &quot;listening?&quot; The str_split_fixed function is often more convenient than str_split because the nth piece of each input string is just the nth column of the result. For example, suppose we want to get the area code from some phone numbers: phones = c(&quot;717-555-3421&quot;, &quot;629-555-8902&quot;, &quot;903-555-6781&quot;) result = str_split_fixed(phones, &quot;-&quot;, 3) result[, 1] ## [1] &quot;717&quot; &quot;629&quot; &quot;903&quot; 16.5.2 Replacing Parts of Strings The str_replace function replaces the pattern the first time it appears in the string. The replacement goes in the third argument. For instance, suppose we want to change the word \"dog\" to \"cat\": x = c(&quot;dogs are great, dogs are fun&quot;, &quot;dogs are fluffy&quot;) str_replace(x, &quot;dog&quot;, &quot;cat&quot;) ## [1] &quot;cats are great, dogs are fun&quot; &quot;cats are fluffy&quot; The str_replace_all function replaces the pattern every time it appears in the string: str_replace_all(x, &quot;dog&quot;, &quot;cat&quot;) ## [1] &quot;cats are great, cats are fun&quot; &quot;cats are fluffy&quot; We can also use the str_replace and str_replace_all functions to delete part of a string by setting the replacement to the empty string \"\". For example, suppose we want to delete the comma: str_replace(x, &quot;,&quot;, &quot;&quot;) ## [1] &quot;dogs are great dogs are fun&quot; &quot;dogs are fluffy&quot; In general, stringr functions with the _all suffix affect all matches. Functions without _all only affect the first match. We’ll learn about str_match at the end of the next section. 16.6 Regular Expressions The stringr functions (including the ones we just learned) use a special language called regular expressions or regex for the pattern. The regular expressions language is also used in many other programming languages besides R. A regular expression can describe a complicated pattern in just a few characters, because some characters, called metacharacters, have special meanings. Letters and numbers are never metacharacters. They’re always literal. Here are a few examples of metacharacters (we’ll look at examples in the subsequent sections): Metacharacter Meaning . any single character (wildcard) \\ escape character (in both R and regex) ^ beginning of string $ end of string [ab] 'a' or 'b' [^ab] any character except 'a' or 'b' ? previous character appears 0 or 1 times * previous character appears 0 or more times + previous character appears 1 or more times () make a group More metacharacters are listed on the stringr cheatsheet, or in ?regex. The str_view function is especially helpful for testing regular expressions. It opens a browser window with the first match in the string highlighted. We’ll use it in the subsequent regex examples. The RegExr website is also helpful for testing regular expressions; it provides an interactive interface where you can write regular expressions and see where they match a string. 16.6.1 The Wildcard The regex wildcard character is . and matches any single character. For example: x = &quot;dog&quot; str_view(x, &quot;d.g&quot;) By default, regex searches from left to right: str_view(x, &quot;.&quot;) 16.6.2 Escape Sequences Like R, regular expressions can contain escape sequences that begin with a backslash. These are computed separately and after R escape sequences. The main use for escape sequences in regex is to turn a metacharacter into a literal character. For example, suppose we want to match a literal dot .. The regex for a literal dot is \\.. Since backslashes in R strings have to be escaped, the R string for this regex is \"\\\\.. Then the regex works: str_view(&quot;this.string&quot;, &quot;\\\\.&quot;) The double backslash can be confusing, and it gets worse if we want to match a literal backslash. We have to escape the backslash in the regex (because backslash is the regex escape character) and then also have to escape the backslashes in R (because backslash is also the R escape character). So to match a single literal backslash in R, the code is: str_view(&quot;this\\\\that&quot;, &quot;\\\\\\\\&quot;) Raw strings are helpful here, because they make the backslash literal in R strings (but still not in regex). We can use raw strings to write the above as: str_view(r&quot;(this\\that)&quot;, r&quot;(\\\\)&quot;) You can turn off regular expressions entirely in stringr with the fixed function: str_view(x, fixed(&quot;.&quot;)) It’s good to turn off regular expressions whenever you don’t need them, both to avoid mistakes and because they take longer to compute. 16.6.3 Anchors By default, a regex will match anywhere in the string. If you want to force a match at specific place, use an anchor. The beginning of string anchor is ^. It marks the beginning of the string, but doesn’t count as a character in the match. For example, suppose we want to match an a at the beginning of the string: x = c(&quot;abc&quot;, &quot;cab&quot;) str_view(x, &quot;a&quot;) str_view(x, &quot;^a&quot;) It doesn’t make sense to put characters before ^, since no characters can come before the beginning of the string. Likewise, the end of string anchor is $. It marks the end of the string, but doesn’t count as a character in the match. 16.6.4 Character Classes In regex, square brackets [ ] create a character class. A character class counts as one character, but that character can be any of the characters inside the square brackets. The square brackets themselves don’t count as characters in the match. For example, suppose we want to match a c followed by either a or t: x = c(&quot;ca&quot;, &quot;ct&quot;, &quot;cat&quot;, &quot;cta&quot;) str_view(x, &quot;c[ta]&quot;) You can use a dash - in a character class to create a range. For example, to match letters p through z: str_view(x, &quot;c[p-z]&quot;) Ranges also work with numbers and capital letters. To match a literal dash, place the dash at the end of the character class (instead of between two other characters), as in [abc-]. Most metacharacters are literal when inside a character class. For example, [.] matches a literal dot. A hat ^ at the beginning of the character class negates the class. So for example, [^abc] matches any one character except for a, b, or c: str_view(&quot;abcdef&quot;, &quot;[^abc]&quot;) 16.6.5 Quantifiers Quantifiers are metacharacters that affect how many times the preceeding character must appear in a match. The quantifier itself doesn’t count as a character in the match. For example, the ? quantifier means the preceeding character can appear 0 or 1 times. In other words, ? makes the preceeding character optional. For example: x = c(&quot;abc&quot;, &quot;ab&quot;, &quot;ac&quot;, &quot;abbc&quot;) str_view(x, &quot;ab?c&quot;) The * quantifier means the preceeding character can appear 0 or more times. In other words, * means the preceeding character can appear any number of times or not at all. str_view(x, &quot;ab*c&quot;) The + quantifier means the preceeding character must appear 1 or more times. Quantifiers are greedy, meaning they always match as many characters as possible. 16.6.6 Groups In regex, parentheses create a group. Groups can be affected by quantifiers, making it possible to repeat a pattern (rather than just a character). The parentheses themselves don’t count as characters in the match. For example: x = c(&quot;cats, dogs, and frogs&quot;, &quot;cats and frogs&quot;) str_view(x, &quot;cats(, dogs,)? and frogs&quot;) 16.6.7 Extracting Matches Groups are espcially useful with the stringr functions str_match and str_match_all. The str_match function extracts the overall match to the pattern, as well as the match to each group. So you can use str_match to split a string in more complicated ways than str_split, or to extract specifc pieces of a string. For example, suppose we want to split an email address: str_match(&quot;naulle@ucdavis.edu&quot;, &quot;([^@]+)@(.+)[.](.+)&quot;) ## [,1] [,2] [,3] [,4] ## [1,] &quot;naulle@ucdavis.edu&quot; &quot;naulle&quot; &quot;ucdavis&quot; &quot;edu&quot; "],["optical-character-recognition.html", "17 Optical Character Recognition 17.1 Loading Page Images 17.2 Running OCR 17.3 Accuracy 17.4 Unreadable Text", " 17 Optical Character Recognition Much of the data we’ve used in the course thus far has been born-digital. That is, we’ve used data that originates from a digital source and does not exist elsewhere in some other form. Think back, for example, to the lecture on strings in R: your homework required you to type text directly into RStudio, manipulate it, and print it to screen. But millions, even billions, of data-rich documents do not originate from digital sources. The United States Census, for example, dates back to 1790; we still have these records and could go study them to get a sense of what the population was like hundreds of years ago. Likewise, printing and publishing far precedes the advent of computers; much of the literary record is still bound up between the covers books or stowed away in archives. Computers, however, can’t read the way we read, so if we wanted to use digital methods to analyze such materials, we’d need to convert them into a computationally tractable form. How do we do so? One way would be to transcribe documents by hand, either by typing out plaintext versions with word processing software or by using other data entry methods like keypunching to record the information those documents contain. Amazon’s Mechanical Turk service is an example of this kind of data entry. It’s also worth noting that, for much of the history of computing, data entry was highly gendered and considered to be “dumb”, secretarial work that young women would perform. Much of the divisions between “cool” coding and computational grunt work that, in a broad, cultural sense, continue to inform how we think about programming, and indeed who gets to program, stem from such perceptions. In spite of (or perhaps because of) such perceptions, huge amounts of data owe their existence to manual data entry. That said, the process itself is expensive, time consuming, error-prone, and, well, dull. Optical character recognition, or OCR, is an attempt to offload the work of digitization onto computers. Speaking in a general sense, this process ingests images of print pages (such as those available on Google Books or HathiTrust), applies various preprocessing procedures to those images to make them a bit easier to read, and then scans through them, trying to match the features it finds with a “vocabulary” of text elements it keeps as a point of reference. When it makes a match, OCR records a character and enters it into a text buffer (a temporary data store). Oftentimes this buffer also includes formatting data for spaces, new lines, paragraphs, and so on. When OCR is finished, it outputs its matches as a data object, which you can then further manipulate or analyze using other code. 17.1 Loading Page Images OCR “reads” by tracking pixel variations across page images. This means every page you want to digitize must be converted into an image format. For the purposes of introducing you to OCR, we won’t go through the process of creating these images from scratch; instead, we’ll be using ready-made examples. The most common page image formats you’ll encounter are pdf and png. They’re lightweight, portable, and usually retain the image quality OCR software needs to find text. The pdftools package is good for working with these files: # install.packages(&quot;pdftools&quot;) library(pdftools) ## Using poppler version 22.01.0 Once you’ve downloaded/installed it, you can load a pdf into RStudio from your computer by entering its filesystem location as a string and assigning that string to a variable, like so: pdf &lt;- &quot;./pdf_sample.pdf&quot; Note that we haven’t used a special load function, like read.csv() or readRDS(). pdftools will grab this file from its location and load it properly when you run a process on it. (You can also just write the string out in whatever function you want to call, but we’ll keep our pdf location in a variable for the sake of clarity.) The same method works with web addresses. We’ll be using web material. First, write out an address and assign it to a variable. pdf &lt;- &quot;https://datalab.ucdavis.edu/adventures-in-datascience/pdf_sample.pdf&quot; Some pdf files will have text data already encoded into them. This is especially the case if someone made a file with word processing software (like when you write a paper in Word and email a pdf to your TA or professor). You can check whether a pdf has text data with pdf_text(). Assign this function’s output to a variable and print it to screen with cat(), like so: text_data &lt;- pdf_text(pdf) cat(text_data) ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## ## ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## ## ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## ## ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. ## ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## The quick brown fox jumps over the lazy dog. ## See how the RStudio terminal recreates the original formatting from the pdf. If you were to use print() on the text output, you’d see all the line breaks and spaces pdf_text() created to match its output with the file. This re-creation would be even more apparent if you were to save the output to a new file with write(). Doing so would produce a close, plaintext approximation of the original pdf. You can also process multi-page pdf files with pdf_text(), with more or less success. It can transcribe whole books and will keep them in a single text buffer, which you can then assign to a variable or save to a file. Keep in mind, however, that if your pdf files have headers, footers, page numbers, chapter breaks, or other such paratextual information, pdf_text() will pick this up and include it in its output. A later lecture in the course will discuss how to go about dealing with this extra data. If, when you run pdf_text(), you find that your file already contains text data, you’re set! There’s no need to perform OCR and you can immediately start working with your data. However, if you run the function and find that it outputs a blank character string, you’ll need to OCR it. The next section shows you how. 17.2 Running OCR First, you’ll need to download/install another package, tesseract, which complements pdftools. The latter only loads/reads pdfs, whereas tesseract actually performs OCR. Download/install tesseract: # install.packages(&quot;tesseract&quot;) library(tesseract) And assign a new pdf to a new variable: new_pdf &lt;- &quot;https://jeroen.github.io/images/ocrscan.pdf&quot; To run OCR on this pdf, use the following: ocr_output &lt;- ocr(new_pdf) ## Converting page 1 to filec29aa847639ocrscan_1.png... done! Print the output to screen with cat() and see if the process worked: cat(ocr_output) ## | SAPORS LANE - BOOLE - DORSET - BH 25 8 ER ## TELEPHONE BOOLE (945 13) 51617 - TELEX 123456 ## ## Our Ref. 350/PJC/EAC 18th January, 1972, ## Dr. P.N. Cundall, ## Mining Surveys Ltd., ## Holroyd Road, ## Reading, ## Berks. ## Dear Pete, ## ## Permit me to introduce you to the facility of facsimile ## transmission. ## ## In facsimile a photocell is caused to perform a raster scan over ## ## ‘ the subject copy. The variations of print density on the document ## cause the photocell to generate an analogous electrical video signal. ## This signal is used to modulate a carrier, which is transmitted to a ## remote destination over a radio or cable communications link. ## ## At the remote terminal, demodulation reconstructs the video ## signal, which is used to modulate the density of print produced by a ## printing device. This device is scanning in a raster scan synchronised ## with that at the transmitting terminal. As a result, a facsimile ## copy of the subject document is produced. ## ## Probably you have uses for this facility in your organisation. ## ## Yours sincerely, ## f{ {. ## P.J. CROSS ## Group Leader - Facsimile Research ## Registered in England: No. 2038 ## No. 1 Registered Office: 680 Vicara Lane, Ilford. Esaex, Voila! You’ve just digitized text. The formatting is a little off, but things look good overall. And most importantly, it looks like everything has been transcribed correctly. As you ran this process, you might’ve noticed that a new png file briefly appeared on your computer. This is because tesseract converts pdfs to png under the hood as part of its pre-processing work and then silently deletes that png when it outputs its matches. If you have a collection of pdf files that you’d like to OCR, it can sometimes be faster and less memory intensive to convert them all to png first. You can perform this conversion like so: png &lt;- pdf_convert(new_pdf, format=&quot;png&quot;, filenames=&quot;./img/png_example.png&quot;) ## Warning in sprintf(filenames, pages, format): 2 arguments not used by format &#39;./ ## img/png_example.png&#39; ## Converting page 1 to ./img/png_example.png... done! In addition to making a png object in your RStudio environment, pdf_convert() will also save that file directly to your working directory. Imagine, for example, how you might put a vector of pdf files through a for loop and save them to a directory, where they can be stored until you’re ready to OCR them. pdfs &lt;- c(&quot;list.pdf&quot;, &quot;of.pdf&quot;, &quot;files.pdf&quot;, &quot;to.pdf&quot;, &quot;convert.pdf&quot;) outfiles &lt;- c(&quot;list.png&quot;, &quot;of.png&quot;, &quot;files.png&quot;, &quot;to.png&quot;, &quot;convert.png&quot;) for (i in 1:length(pdfs)) { pdf_convert(pdfs[i], format=&quot;png&quot;, filenames=outfiles[i]) } ocr() can work with a number of different image types. It takes pngs in the same way as it takes pdfs: png_ocr_output &lt;- ocr(png) 17.3 Accuracy If you cat() the output from the above png file, you might notice that the text is messier than it was when we used pdf_text_ocr(). cat(png_ocr_output) ## THE SLEREXE COMPANY LIMITED ## SAPORS LANE - BOOLE - DORSET - BH 25 $ER ## e sous (4513) 617 - Tk 12345 ## Our Ref. 350/BIC/EAC 186 Janvary, 1972. ## De. PN, Cundall, ## Kining Surveys Lid., ## Holroyd Road, ## Reading, ## Berks. ## Dear Pece, ## ## Pernit ne to introduce you to the facility of facsinile ## transmission. ## ## In facainile a photocell is coused to perforn a raster scan over ## the subject copy. The varistions of princ density on the docunent ## cause the photecell o generate an analogous electrical video signal. ## This signal is used to mdulate a carrier, vhich is cransmitted to o ## cemote destination over &amp; radio or cable communications link. ## ## A¢ the remote cerminal, demodulation reconstructs the video ## signal, which is used to modulate the density of print produced by @ ## printing device. Tnis device is scanning in 4 raster scan synchronised ## Uich that at the cransmitring terminal. As &amp; result, a facsisile ## Copy of the subject document is produced. ## ## Probably you have uees for this facility in your organisation. ## ## Yours sincerely, ## P.J. cross ## Group Leader - Facsinile Research This doesn’t have to do with the png file format per se but rather with the way we created our file. If you open it, you’ll see that it’s quite blurry, which has made it harder for ocr() to match the text it represents: This blurriness is because pdf_convert() defaults its conversions to 72 dpi, or dots per inch. Dpi is a measure of image resolution (originally from inkjet printing), which describes the amount of pixels your computer uses to create images. More pixels means higher image resolution, though this comes with a trade off: images with a high dpi are also bigger and take up more space on your computer. Usually, a dpi of 150 is sufficient for most OCR jobs, especially if your documents were printed with technologies like typewriters, dot matrix printers, and so on and if they feature fairly legible typefaces (Times New Roman, for example). A dpi of 300, however, is ideal. You can set the dpi in pdf_convert() by adding a dpi argument in the function: hi_res_png &lt;- pdf_convert(new_pdf, format=&quot;png&quot;, dpi=150, filenames=&quot;./img/hi_res_png_example.png&quot;) ## Warning in sprintf(filenames, pages, format): 2 arguments not used by format &#39;./ ## img/hi_res_png_example.png&#39; ## Converting page 1 to ./img/hi_res_png_example.png... done! Another function, ocr_data()outputs a data frame that contains all of the words tesseract found when it scanned through your image, along with a column of confidence scores. These scores, which range from 0-100, provide valuable information about how well the OCR process has performed, which in turn may tell you whether you need to modify your pdf or png files further before OCRing them (more on this below). Generally, you can trust scores of 93 and above. To get confidence scores for an OCR job, call ocr_data() and subset the confidence column, like so: ocr_data &lt;- ocr_data(hi_res_png) confidence_scores &lt;- ocr_data$confidence print(confidence_scores) ## [1] 92.13857 91.38475 91.23807 91.23807 93.29996 92.56295 93.26220 70.64086 ## [9] 93.01996 89.35526 89.52242 89.52242 42.67688 42.67688 96.07313 92.71711 ## [17] 87.83458 96.88795 91.54445 87.78777 87.78777 87.65559 91.62273 92.34843 ## [25] 89.77596 90.55594 91.76022 92.06514 92.69786 89.87141 92.24236 92.18182 ## [33] 91.00060 85.50681 92.09026 91.46124 92.45944 91.51769 91.51769 92.03702 ## [41] 92.01013 90.96066 92.24164 92.24998 90.49390 79.35881 48.94280 91.96535 ## [49] 91.78830 93.19865 90.28869 90.28869 91.80365 91.80365 92.41957 93.18517 ## [57] 91.35117 91.66459 92.26031 59.55725 91.65620 92.85319 88.85143 88.85143 ## [65] 91.43013 92.53618 91.01624 46.33485 90.00519 92.77856 92.17099 91.57552 ## [73] 92.91947 91.35083 91.35083 89.52011 92.17740 92.17690 91.17965 92.13211 ## [81] 90.32191 92.12690 92.57925 91.50474 92.65192 91.54854 92.03278 92.12846 ## [89] 92.12846 92.59993 93.13262 91.75418 92.46236 92.56086 92.50134 91.19598 ## [97] 91.44257 89.06452 92.32296 92.01079 92.46326 91.14729 90.52568 91.94347 ## [105] 91.94347 90.66655 91.42926 91.71403 86.76886 92.14970 92.17909 90.94286 ## [113] 92.76601 93.08193 92.06605 92.16745 91.70026 91.56775 91.12645 91.00975 ## [121] 92.05129 93.12570 93.27691 93.27691 91.80145 91.35611 71.01427 92.27676 ## [129] 89.75899 83.44016 89.61749 87.48296 92.16154 93.15559 92.01822 91.42703 ## [137] 91.79588 92.10561 92.70488 91.68903 90.74780 90.74780 91.49944 92.60815 ## [145] 92.56828 91.94787 92.36919 90.87371 91.16226 91.95363 91.07956 91.07956 ## [153] 91.82436 92.90215 92.83604 92.72958 91.67342 92.18327 92.68385 92.30362 ## [161] 92.14813 92.26443 90.58723 90.73596 92.27166 50.64243 59.75372 56.96629 ## [169] 85.22475 88.63677 91.58677 91.72172 93.29646 92.40235 90.71992 86.72282 ## [177] 84.96437 84.08060 91.98026 95.90424 88.61440 81.63094 90.19997 75.30827 ## [185] 82.44823 75.63014 57.31424 63.12554 44.16006 The mean is a good indicator of the overall OCR quality: confidence_mean &lt;- mean(confidence_scores) print(confidence_mean) ## [1] 88.55039 Looks pretty good, though there were a few low scores that dragged the score down a bit. Let’s look at the median: confidence_median &lt;- median(confidence_scores) print(confidence_median) ## [1] 91.67342 We can work with that! If we want to check our output a bit more closely, we can do two things. First, we can look directly at ocr_data and compare, row by row, a given word and its confidence score. print(ocr_data) ## word confidence bbox ## 1 SAPORS 92.13857 422,194,497,208 ## 2 LANE 91.38475 508,194,560,208 ## 3 - 91.23807 570,203,575,205 ## 4 BOOLE 91.23807 585,193,651,208 ## 5 - 93.29996 661,202,666,205 ## 6 DORSET 92.56295 676,193,755,208 ## 7 - 93.26220 764,203,769,205 ## 8 BH25 70.64086 780,193,831,208 ## 9 8 93.01996 842,193,850,208 ## 10 ER 89.35526 856,194,883,208 ## 11 TELEPHONE 89.52242 449,232,534,243 ## 12 BOOLE 89.52242 544,232,589,243 ## 13 (945 42.67688 600,229,634,246 ## 14 13) 42.67688 640,229,664,246 ## 15 51617 96.07313 675,229,719,244 ## 16 - 92.71711 730,237,735,240 ## 17 TELEX 87.83458 746,232,792,243 ## 18 123456 96.88795 804,228,857,244 ## 19 Our 91.54445 211,392,246,408 ## 20 Ref. 87.78777 261,391,306,407 ## 21 350/PJC/EAC 87.78777 325,389,459,409 ## 22 18th 87.65559 863,389,910,405 ## 23 January, 91.62273 924,389,1020,408 ## 24 1972. 92.34843 1038,388,1095,405 ## 25 Dr. 89.77596 212,492,244,508 ## 26 P.N. 90.55594 262,492,307,508 ## 27 Cundall, 91.76022 325,491,419,510 ## 28 Mining 92.06514 212,516,286,538 ## 29 Surveys 92.69786 301,518,383,536 ## 30 Ltd., 89.87141 398,516,457,535 ## 31 Holroyd 92.24236 212,543,298,562 ## 32 Road, 92.18182 313,542,369,561 ## 33 Reading, 91.00060 213,566,308,587 ## 34 Berks. 85.50681 213,593,282,608 ## 35 Dear 92.09026 213,668,261,683 ## 36 Pete, 91.46124 275,668,333,686 ## 37 Permit 92.45944 276,715,348,732 ## 38 me 91.51769 362,721,386,732 ## 39 to 91.51769 402,719,423,732 ## 40 introduce 92.03702 439,715,548,731 ## 41 you 92.01013 562,720,599,735 ## 42 to 90.96066 614,718,636,731 ## 43 the 92.24164 652,716,685,731 ## 44 facility 92.24998 702,714,799,735 ## 45 of 90.49390 813,715,836,731 ## 46 facsimile 79.35881 852,713,961,731 ## 47 transmission. 48.94280 215,740,371,757 ## 48 In 91.96535 278,793,300,807 ## 49 facsimile 91.78830 316,790,424,807 ## 50 a 93.19865 439,796,450,806 ## 51 photocell 90.28869 463,791,573,811 ## 52 is 90.28869 589,790,611,806 ## 53 caused 91.80365 627,791,700,807 ## 54 to 91.80365 714,793,736,806 ## 55 perform 92.41957 751,790,839,810 ## 56 a 93.18517 852,794,862,806 ## 57 raster 91.35117 876,793,950,806 ## 58 scan 91.66459 965,794,1013,806 ## 59 over 92.26031 1026,793,1075,805 ## 60 . 59.55725 16,824,18,826 ## 61 the 91.65620 215,817,249,832 ## 62 subject 92.85319 265,815,349,836 ## 63 copy. 88.85143 364,820,421,835 ## 64 The 88.85143 451,816,486,831 ## 65 variations 91.43013 501,814,623,831 ## 66 of 92.53618 639,815,661,831 ## 67 print 91.01624 675,814,736,835 ## 68 denmsity 46.33485 752,814,837,834 ## 69 on 90.00519 851,819,875,830 ## 70 the 92.77856 890,814,924,830 ## 71 document 92.17099 939,814,1037,830 ## 72 cause 91.57552 215,846,275,857 ## 73 the 92.91947 290,841,324,857 ## 74 photocell 91.35083 338,841,449,861 ## 75 to 91.35083 465,844,487,857 ## 76 generate 89.52011 502,844,599,860 ## 77 an 92.17740 614,845,637,857 ## 78 analogous 92.17690 652,841,761,861 ## 79 electrical 91.17965 777,839,898,856 ## 80 video 92.13211 914,838,975,856 ## 81 signal. 90.32191 989,838,1071,859 ## 82 This 92.12690 215,865,261,881 ## 83 signal 92.57925 278,865,349,886 ## 84 is 91.50474 365,865,386,881 ## 85 used 92.65192 402,866,450,882 ## 86 to 91.54854 465,868,487,881 ## 87 modulate 92.03278 501,865,599,881 ## 88 a 92.12846 615,870,626,881 ## 89 carrier, 92.12846 639,865,735,884 ## 90 which 92.59993 750,864,812,881 ## 91 is 93.13262 828,864,849,881 ## 92 transmitted 91.75418 865,863,1000,881 ## 93 to 92.46236 1015,867,1037,880 ## 94 a 92.56086 1052,868,1063,879 ## 95 remote 92.50134 215,894,287,907 ## 96 destination 91.19598 302,890,438,907 ## 97 over 91.44257 451,895,500,906 ## 98 a 89.06452 514,895,525,906 ## 99 radio 92.32296 539,889,599,906 ## 100 or 92.01079 614,895,637,906 ## 101 cable 92.46326 653,891,712,906 ## 102 communications 91.14729 727,888,899,906 ## 103 link. 90.52568 915,888,972,905 ## 104 At 91.94347 277,941,300,956 ## 105 the 91.94347 316,941,350,956 ## 106 remote 90.66655 365,943,437,956 ## 107 terminal, 91.42926 453,939,560,962 ## 108 demodulation 91.71403 577,939,725,956 ## 109 reconstructs 86.76886 740,942,886,956 ## 110 the 92.14970 903,940,937,956 ## 111 video 92.17909 951,938,1013,955 ## 112 signal, 90.94286 215,964,297,986 ## 113 which 92.76601 314,964,375,981 ## 114 is 93.08193 390,964,411,981 ## 115 used 92.06605 427,965,475,981 ## 116 to 92.16745 490,968,512,981 ## 117 modulate 91.70026 526,965,625,981 ## 118 the 91.56775 641,965,675,981 ## 119 density 91.12645 690,964,775,984 ## 120 of 91.00975 789,964,812,980 ## 121 print 92.05129 826,963,887,984 ## 122 produced 93.12570 901,964,1001,984 ## 123 by 93.27691 1013,964,1038,983 ## 124 a 93.27691 1052,968,1063,979 ## 125 printing 91.80145 215,989,314,1010 ## 126 device. 91.35611 327,989,410,1006 ## 127 This 71.01427 440,989,486,1006 ## 128 device 92.27676 502,989,575,1006 ## 129 is 89.75899 590,989,611,1006 ## 130 scanning 83.44016 628,989,726,1010 ## 131 in 89.61749 741,989,763,1005 ## 132 a 87.48296 777,994,788,1005 ## 133 raster 92.16154 802,992,876,1005 ## 134 scan 93.15559 890,994,938,1005 ## 135 synchronised 92.01822 953,988,1101,1009 ## 136 with 91.42703 213,1015,263,1031 ## 137 that 91.79588 278,1015,325,1031 ## 138 at 92.10561 340,1018,362,1031 ## 139 the 92.70488 379,1015,413,1031 ## 140 transmitting 91.68903 428,1014,576,1035 ## 141 terminal. 90.74780 591,1014,697,1031 ## 142 As 90.74780 727,1015,749,1031 ## 143 a 91.49944 765,1019,775,1030 ## 144 result, 92.60815 789,1015,873,1034 ## 145 a 92.56828 890,1019,901,1030 ## 146 facsimile 91.94787 915,1012,1026,1030 ## 147 copy 92.36919 215,1045,263,1060 ## 148 of 90.87371 278,1040,300,1056 ## 149 the 91.16226 317,1041,350,1056 ## 150 subject 91.95363 365,1039,450,1060 ## 151 document 91.07956 465,1040,562,1056 ## 152 is 91.07956 578,1039,599,1055 ## 153 produced. 91.82436 614,1040,722,1060 ## 154 Probably 92.90215 278,1091,375,1110 ## 155 you 92.83604 389,1095,427,1110 ## 156 have 92.72958 439,1091,488,1106 ## 157 uses 91.67342 503,1094,550,1106 ## 158 for 92.18327 566,1090,601,1106 ## 159 this 92.68385 616,1089,662,1106 ## 160 facility 92.30362 678,1088,775,1110 ## 161 in 92.14813 792,1088,814,1105 ## 162 your 92.26443 827,1094,876,1110 ## 163 organisation. 90.58723 890,1087,1049,1110 ## 164 Yours 90.73596 678,1141,737,1156 ## 165 sincerely, 92.27166 753,1139,874,1160 ## 166 f{ 50.64243 699,1202,778,1272 ## 167 &#39;/ 59.75372 808,1209,823,1266 ## 168 . 56.96629 828,1257,834,1266 ## 169 P.J. 85.22475 678,1304,724,1319 ## 170 CROSS 88.63677 742,1304,802,1319 ## 171 Group 91.58677 678,1329,739,1347 ## 172 Leader 91.72172 753,1328,827,1344 ## 173 - 93.29646 840,1334,852,1338 ## 174 Facsimile 92.40235 867,1326,978,1344 ## 175 Research 90.71992 992,1327,1092,1344 ## 176 Registered 86.72282 521,1574,600,1587 ## 177 in 84.96437 607,1574,621,1584 ## 178 England: 84.08060 629,1573,695,1586 ## 179 No. 91.98026 725,1574,749,1584 ## 180 2038 95.90424 756,1574,793,1584 ## 181 No. 88.61440 72,1597,99,1610 ## 182 1 81.63094 110,1597,114,1609 ## 183 Registered 90.19997 457,1592,535,1605 ## 184 Office: 75.30827 545,1592,595,1603 ## 185 80 82.44823 627,1592,644,1602 ## 186 Vicara 75.63014 651,1592,700,1602 ## 187 Lane, 57.31424 723,1592,763,1605 ## 188 Ilford. 63.12554 771,1592,817,1602 ## 189 Esasx, 44.16006 825,1592,873,1603 That’s a lot of information though. Something a little more sparse might be better. We can use base R’s table() function to count the number of times unique words appear in the OCR data. We do this with the word column in our ocr_data variable from above: ocr_vocabulary &lt;- table(ocr_data$word) ocr_vocabulary &lt;- as.data.frame(ocr_vocabulary) Let’s look at the first 30 words: head(ocr_vocabulary, 30) ## Var1 Freq ## 1 - 5 ## 2 . 2 ## 3 &#39;/ 1 ## 4 (945 1 ## 5 1 1 ## 6 123456 1 ## 7 13) 1 ## 8 18th 1 ## 9 1972. 1 ## 10 2038 1 ## 11 350/PJC/EAC 1 ## 12 51617 1 ## 13 8 1 ## 14 80 1 ## 15 a 9 ## 16 an 1 ## 17 analogous 1 ## 18 As 1 ## 19 at 1 ## 20 At 1 ## 21 Berks. 1 ## 22 BH25 1 ## 23 BOOLE 2 ## 24 by 1 ## 25 cable 1 ## 26 carrier, 1 ## 27 cause 1 ## 28 caused 1 ## 29 communications 1 ## 30 copy 1 This representation makes it easy to spot errors like discrepancies in spelling. We could correct those either manually or with string matching. One way to further examine this table is to look for words that only appear once or twice in the output; among such entries you’ll often find misspellings. The table does, however, have its limitations. Looking at this data can quickly become overwhelming if you send in too much text. Additionally, notice that punctuation “sticks” to words and that uppercase and lowercase variants of words are counted separately, rather than together. These quirks are fine, useful even, if we’re just spot-checking for errors, but we’d need to further clean this data if we wanted to use it in computational text analysis. A later lecture will discuss other methods that we can use to clean text. When working in a data-forensic mode with page images, it’s a good idea to pull a few files at random and run them through ocr_data() to see what you’re working with. OCR accuracy is often wholly reliant on the quality of the page images, and most of the work that goes into digitizing text involves properly preparing those images for OCR. Adjustments include making sure images are converted to black and white, increasing image contrast and brightness, increasing dpi, and rotating images so that their text is more or less horizontal. tesseract performs some of these tasks itself, but you can also do them ahead of time and often you’ll have more control over quality this way. The tesseract documentation goes into detail about what you can do to improve accuracy before even opening RStudio; we can’t cover this in depth, but keep the resource in mind as you work with this type of material. And remember: the only way to completely trust your accuracy is to go through the OCR output yourself. It’s a very common thing to have to make small tweaks to output. In this sense, we haven’t quite left the era of hand transcription. 17.4 Unreadable Text All that said, these various strategies for improving accuracy will only get you so far if your page images are composed in a way OCR just can’t read. OCR systems contain a lot of in-built assumptions about what “normal” text is, and they are incredibly brittle when they encounter text that diverges from that norm. Early systems, for example, required documents to be printed with special, machine-readable typefaces; texts that contained anything other than this design couldn’t be read. Now, OCR is much better at handling a variety of text styling, but systems still struggle with old print materials like blackletter. Running: ballad &lt;- &quot;https://ebba.english.ucsb.edu/images/cache/hunt_1_18305_2448x2448.jpg&quot; ballad_out &lt;- ocr(ballad) Produces: cat(ballad_out) ## @€ QADifcriptionof Roxtons falechod ’ ## 9 Pozkie Myze, and of hig fatall favewel, ## fThe fatal fineof Lraitours loes ; ## % 4Bp Fufice due, deferupng foe. - 1 ## T late (alas) the great bntrath Ehe Crane wwoldefipebptothe punte, Roto, bis futerpug long (be fure) - ## Df Traitours, hoto ft fped g heacdit once of olde s _ adiplipay bis foes atlaf: : , ## T boliff toknotn, Hhal berez..wae Sndoith the Hpng of bpaoes did frine Pig mercpe moued once atwap, i ## Potv late allegeance fev, 1Bp Fame, 3 beardittolves = Be Hall them &amp;uigbt out caff i ## « 3 Refucrsrage aganit the Hea. GAnddofvoe ke weloe not fal benoy aaith fentence infk fo2 theit butruth, : ## And ffuell Wwith foddeine rapnes 1But higher Epld nipMOUAY 2 and bacakpng of his fwpll 3 : ## Potu glao are they to fall agapne, STl pait her teach(faitoroevepote) £Lhe fruits ofthetr fedicfous (208, : ## Andtracethei wonted traine? Shame madea backevecour SThebarnes of cavth thail fpll. ## Bl 36 fice by fozce Wwolde fozge the fall 3 touch no Armes herein atall, TCheir foules God ot foze clogd 1 crime ## Df any fumptuoule place, AButfhetva fable fopfes _ Qnd theirpofteritie ## 8 36 water flods byd bim leaueof, T hofemozall fence doth repar 4B efpofted foze With theirabule, ## s flames he wylldifgrace, 2DE clpmers bye theguple. Ano Eand by theit follie, ## | 3t Goo command the Wwpides to ceafe, Tuho buploes a boule of many 5 heiclinpngs left theiv namea hanre, ## 1 is blatfes ave lapd full lotu ¢ anblaith not ground Wogkr heir dedes oith poplon fped: ; ## i 3fBod command the feas tocalme, 4Butdoth ertoatethe groundk g, Theivdeathesa wage fo wantofgrace # ## : 2Chep Wyl notrage 02 flolu, Pis bufldpng can notdure. SCheir honours quite is dead. ## [ Gl thinges at Gods commandemet be, @ TWhofekes furmiGingtodilp SCheir flefh tofedethe kptes and crolues | ## 3f betheir fateregarde: : a Kuler fentbp GOD 2 ACheir arnies a mase foa men s i ## il andnoman liues whole veffinie s fubiect fure, Deuoide of qrace. Cheir guerdon as eramples are L ## 1By him i8 bupzeparde. Checauleof bis otonerod. 2o dafy dolte Dunces den. ## {15t when a man foxfakes theHip, - q byve that Wyl bernefdefple SChaotn bp pout fouts pou Auggifh foxte fi ## 8 - anorowlesin alloning Wwanes 1By vight Hould lofe a wyng: 3 oumumming malkyng route : ! ## And of bis boluntarfe wyll, Gud then is e no fping fo 1wl CErtoll your erclamations by, g ## ! PBis oiie god hap depraues 1But flotw agother thyng. ABaals chapletnes,champions ffoute, ## i Pow Hal hehopeto lcape the gulfe 2 anphe that lofeth all at games, Datke fute fo2 pardons, papiflts braue, ## : Bot fhal be thinke todeale D2 fpendes infoiole ercefle: Foztraitours indulgence : ## Bl 1o Mal bis fanfiebaing him lound Qnbhopes by hapsto bealehisharme, . fend ot fome purgatosic (craps, ) ## £ 4T o @afties MHoze Wwith faple 2 Mufk vzinkeof deare piffrefe, Some WBulls tuith Peter pence. ## ; Botw hall bis fratght in fine (uccede 2 o fpeatie of bapdles fovelirapne D (wacine of D2ones, how dare pefipl | ## i Alas what Hall hegapne 2 T his wylfull wapardereive $ TWith labourpng Wees contend ## | ubatfeare by oams Do matke bimquake hey cave not foy the boke of G0, Pou fonght fo2bonie from thehiues, v ## i 1Hotw ofte [ubieceto papne 2 1o P2inces, men bnfrue. ABut gall pou foundinend. b ## il ot funaie times in Dangers ven o cunftepe, caufers of much woe, SChele tafpes oo wafk, their fings beont &amp; ## 33 thzotone thentan bnivple 2 ﬁts&#39;i‘faitbfull frdenves, afall: heir fpight topll nofanaple: E ## 5l @uhoclimes Withouten bolde ot bye, and Btheivotone effates, aftpng, TChefle Peacocks pronvearenaked lefee [ ## = ABeware, 3 hint aduise. T 5 others, avpeas gall. ODF theiv difplayed tayle. b ## i Qllfuchas teaft to falfe confrags, D 1Lozoe, hotu long thefe Liservsluvke, Chele Lurkpe cocks fu cullourred, ## 3 D2 feceet harmes confpive? BGos GO D, bolv greata tuhple _ Holong banelurkt alofe S ## { 15¢(ure, with Hoxtonsithey hal taffe Wlere thep in hand with fefgnedbarts Ehe WBeare (althongh but ot of fwofe) &quot; ## Q right deferucd bire, heir cuntrye to defple? Path pluct bis wynges by piafe, ## 8| Thep can not loke foz better (pede, oto did thep frame their turniture?&gt; SCbe Pone ber bozowenlighthath lof, [ ## Do death foz fuch to fell 7 Yol fitthey mave theirtoles ¢ Sbe wapnedas wele ## 8 Gobgrantthe futticeof the Wozive ot Hymon feught our englplh Trofe TWhoboped by bap of othersharnes, 4 ## Put by the papnes of bell, o bapng to Romaine feoles. 4 full Pone once tobie, ## il oz fuchapentiuccale it is, Potu Himon PHagus plapd bis parte, 4The Lpon (uffred long the Wull, ## E TChat Cnglifh havts diddare 391 1Babilon balvde dibrage: 1is noble mpnd totrye: ## 0o palie the bounves of dutieslatve, Potw WBafan bulles begon to bell, Wntpll the 1Bull Wwas rageypng wod, F ## G D3 of their cuntriecare. Hofu Judas (ought bis wage. dndfrombisfakedid hye. @ ## ¥ dnpmerciehath (o longreleal 13oin Jannes and Jambzes 0id abyde 2Chen tine it was to bid him Fap ## i Dfenvours (God doth knov) TChe baunt of baaineficke ads, Perfozce, bishomestocut ## [ andbountic of our curteous Nuéctie Yot Dathan, Choze, Abivam (md Andmake him leauehis rageing tunes ¢ ## &amp; T long hath fpared her foe. o dath out Poyp(les faas. 3In(cilence to be put. &amp; ## b 15ut Gov, Wwhofegrace fipitesherharte, Poln Romaine marchant feta frefh Andall the calues of WBafan kynd i ## § W pll not abyoethe fpight g pardons baauea fale, Are weaned from their Wifh s i ## i Df RAebels rage, Wwho rampets veach Potw alwapes fomeagain( the aenty LheBivcan Tigers tamed notn, | ## i Fromhber, hee title quight. Tioloe deame afencelestale, _ dlemathon eatesno fifh). i ## B4 qithough e fowe inpitifull seale, BGobs bicar frombis god receaucd 4Beholoe befoze pour balefull epes ## QAnd loueth to fucke noblod 3 Che kepes to lofeand bynds Lhepurchace of pour pacte, ## i1 et Goba caueat opll her lend 4Baals chaplein thoght b fire wolk ™ 1o SHuruey pour fodefnefo2roluful fight ## L appealethole Tipers mode, Huch was his pagan mynd. i ith fighes of dubble harte, ## B q man that (s bis bouleon fire, Gob 1Lozve hotw bits the tet theiv ts Lament thelackeof pouralies i ## vl feke to quench the flame 5 TThat faith fuch men thall bé _ Religiousvebellgalls i ## i Clsfrom thefpoylefomepacteconuey, Futheic reltgﬂonbo;no;tnlhz ABetuepethat pll fuccele ofyours, i ## C1s feke the heate to tame, D much bavietie. Come curfe pour (odeine fall, G ## IR who (@ea penthoule wether beate, and fund2p (o02ts of fects furk and Iben pehaue pour guiles out fought ## | And heares a boiftroule fopndes iuifion MHall appeare . And all your craft app2oued, ## i 3utbheoefull fafetic of him(elte, dgaintE thefather, fonnefhe ue, Peccanimus Hall be your fong : ## T pll foace him fuccour fynde 2 Gaint mother, daughter 2 Pour ground wozke is renoued, ## AChepitifull pacient Pellican, 9 it not come to pafe trofy pua? Qnd lokehotv Poztons (ped their wills = ## ] Per blod although Hee Hed e ea, baftards furethey bes € uen (o thetr fee MHall haue, B ## Kl 3pct opll (hee femeber dateto end, qrho our gwd mother Nuane o, . 5 o better et thenrhope to gapne &quot;B ## !‘ s ﬂbzcarle hﬁzwnung befped. b m(lgthtzang rzbelllnu?xz.g ol 15ut gallotwes without graue, ; ## 81 he Cagle fipnges ber yong ones dotone Can 00 bis bengeance long etaty ey . ## it~ That ﬁfght lnf fg]nne vefufe 2 mbell;z})is e (pruants ficle CEINTS léha ibl&#39;nn. L ## Bl wnperfed fotles (e deadly hates, gniuvioule (pights of godleemeit, @Xg * i ## 4 Anv rightlp fuch mirble, | oo turne as doth &amp; Wheele2 A 0 @‘@ @A@ 2 ## e » i ## i @ Tmpzinted at London by Fleyandet Lacfe, foz Henrie Tipskeham, divellyng at the figne ## - . of the blacke 3Bope, at themiddle {po2th Doze of Paules chuveh, 2 : ## } y B (Note by the way that this example used a jpg file. ocr() can handle those too.) Even though every page is an “image” to OCR, OCR struggles with imagistic or unconventional page layouts, as well as inset graphics. Add to that sub-par scans of archival documents, as in the newspaper page below, and the output will contain way more errors than correct matches. newspaper &lt;- &quot;https://chroniclingamerica.loc.gov/data/batches/mimtptc_inkster_ver01/data/sn88063294/00340589130/1945120201/0599.pdf&quot; newspaper_ocr &lt;- ocr(newspaper) Beautifully messy output results: cat(newspaper_ocr) ## A T ## ## v 7 ’ ta&#39;.&#39;a:;:r‘a:;J ## ## Reorm-slypewriin®=£ == ## a?q&quot;i One strategy you might use to work with sources like this is to crop out everything you don’t want to OCR. This would be especially effective if, for example, you had a newspaper column that always appeared in the top left-hand corner of the page. You could preprocess your page images so that they only showed that part of the newspaper and left out any ads, images, or extra text. Doing so would likely increase the quality of your OCR output. Such a strategy can be achieved outside of R with software ranging from Adobe Photoshop or the open-source GIMP to Apple’s Automator workflows. Within R, packages like tabulizer and magick enable this. You won’t, however, be required to use these tools in the course, though we may have a chance to demonstrate some of them during lecture. There are several other scenarios where OCR might not be able to read text. Two final (and major) ones are worth highlighting. First, for a long time OCR support for non-alphabetic writing systems was all but nonexistent. New datasets have been released in recent years that mostly rectify these absences, but sometimes support remains spotty and your milage may vary. Second, OCR continues to struggle with handwriting. While it is possible to train unsupervised learning processes on datasets of handwriting and get good results, as of yet there is no general purpose method for OCRing handwritten texts. The various ways people write just don’t conform to the standardized methods of printing that enable computers to recognize text in images. If, someday, you figure out a solution for this, you’ll have solved one of the most challenging problems in computer vision and pattern recognition to date! "],["network-analysis.html", "18 Network Analysis 18.1 Overview 18.2 What is Network Analysis 18.3 Network Data 18.4 Graph Level Properties 18.5 Node Level Properties 18.6 Network Workflow 18.7 Network Tools", " 18 Network Analysis 18.1 Overview This reader will introduce the theoretical and logistical underpinnings of network analysis. It will define what networks are, their limitations, and their use cases. It will then cover some of the most commonly used network measures, what they mean, and how to generate them. 18.1.1 While you wait Make sure you have installed all the packages you will need: install.packages(c(\"statnet\", \"visNetwork\", \"learnr\")) Download the data and class code-along from Canvas Play around with the Oracle of Bacon 18.1.2 Learning Objectives By the end of this class meeting, students should be able to: Understand what a network is. Understand what is and is not relational data. Understand the shortcomings and limitations of network analysis. Evaluate a network dataset and interpret the generated metrics. 18.1.3 Roadmap What is Social Network Analysis (SNA) Examples of networks in research SNA Data Network (graph) level properties Individual (node) level properties Network Tools Guided Homework Start/Question Time 18.2 What is Network Analysis You are all most likely familiar now with tabular data; rows and columns containing information. It looks like this: Person Name Age Widgets J 30 1 Y 21 3 G 32 4 Z 48 8 While this is a tidy way to store data, it artificially atomizes or separates many of the things we are interested in as researchers, social or otherwise. Network analysis is a tool to work with relational data, a.k.a. information about how entities are connected with each other. For example, the diagram below shows the same data as the table above, with the added benefit of showing how these individuals are connected to each other. Hover over the people to reveal the data about them. Rather than looking only at attributes of specific data points, we are looking at the connections between data. In network analysis, data points are called nodes or vertices, and the connections between them are called edges or ties. Vertices can be anything—people, places, words, concepts—they are usually mapped into rows in a data frame. Edges contain any information on how these things connect or are related to each other. These components create a network or graph, defined as “finite set or sets of actors and the relation or relations defined on them” (Wasserman and Faust 1994). 18.2.1 Networks in research - Social Sciences One of the first instances of social network analysis was originally published in 1932 as part of Jacob Moreno’s Who Shall Survive (1953). This study used the friendship networks of girls within a reform school to show that the ties between them were a stronger predictor of runaways than any attribute of the girls themselves. Since then, networks have been used widely in the social sciences, but only really picked up as the tools to understand SNA became more available. 18.2.2 Networks in research - Neuroscience Neuroscientists use networks to study the brain, given their ready application to neurons and pathways. Bassett and Sporns (2017) provide an overview of how to translate neuroscience problems into network ones, and the tools available to study them. 18.2.3 Networks in research - Chemistry Chemistry was quick to see the applications of networks. As early at 1985 papers were published detailing the potential networks provided in terms of understanding and finding new ways to measure and understand the bonds between atoms and molecules (Balaban 1985). 18.2.4 Networks in research - The Internet The internet is a network! Beyond the various social network sites, servers themselves act as nodes and the information flows between them along edges. Google used this property in the first version of their search engine, which used the network metric of PageRank to determine which sites to show at the top of search results (Page 2001). 18.2.5 Networks in research - Infrastructure Fand and Mostafavi (2019) showed how you can use social media network data to find where infrastructure is failing during disasters, such as hurricane Harvey in 2017. Their system promises a method to monitor physical infrastructure like roads, bridges, and barriers like more easily monitored infrastructure like the electrical grid. 18.2.6 Networks in research - Security Network analysis has also been used for offensive purposes. One of the most prominent uses is mapping crime or terror networks (Krebs 2002), though it is fraught with ethical concerns. There are specific tools made for this purpose, such as the keyplayer package (An and Liu 2016), which helps find what nodes in a network would fragment them the most if removed. 18.3 Network Data Networks are based on relational data. This means the core data requirement is that we have some measure of how nodes are connected. The two most common network data formats are the edgelist and adjacency matrix. Either of these will work for nearly any network purpose, and it is easy to convert between them. You will also need an attributes file, which gives information about the nodes being connected. 18.3.1 Edgelist An edgelist is a two-column dataframe with a from and to column. Each row represents one edge or tie, with the possibility of adding in more information. Here is an example of a basic edgelist. Let’s load in the example data you downloaded and look at some of it. # load in the data toy_edgelist = read.csv(&quot;./data/toy_edgelist.csv&quot;, header = TRUE, stringsAsFactors = FALSE) # show the first 10 rows kable(head(toy_edgelist, n = 10)) to from b33f00bd1109e1ae3ffa757d0aef0a25942f2ba3 zuko 19d5b2694036f6fab966564c1c44bc74330f22c2 zuko 9483b16c4904908115f4538525e37f776f4596d4 zuko f8452649773eb7e024bfa59c395afa0c302d1928 zuko eea677240a425ed7ccdeff69feb2d377a5542599 zuko 9dbcce359070c879f20843e19564aee545f80d2d zuko 749e81272630eb4755e4a7bca10fe3e3524d77ce zuko toph zuko 5737a840aa867025dcb506f24cb5546f16b4d777 zuko 028f5d1f351d38cd6553ab4674b19725d5ea3d3c zuko 18.3.2 Adjacency Matrix The same data can also be displayed in a table format. The information is the same, but it is presented in a way more usable by our code to create measures we care out. In this format, every node has both a row and column. If there is an edge between two nodes, a 1 is placed in the intersection of their row and column. 028f5d1f351d38cd6553ab4674b19725d5ea3d3c 19d5b2694036f6fab966564c1c44bc74330f22c2 5737a840aa867025dcb506f24cb5546f16b4d777 749e81272630eb4755e4a7bca10fe3e3524d77ce 9483b16c4904908115f4538525e37f776f4596d4 9dbcce359070c879f20843e19564aee545f80d2d b33f00bd1109e1ae3ffa757d0aef0a25942f2ba3 eea677240a425ed7ccdeff69feb2d377a5542599 f8452649773eb7e024bfa59c395afa0c302d1928 toph zuko 028f5d1f351d38cd6553ab4674b19725d5ea3d3c 0 0 0 0 0 0 0 0 0 0 1 19d5b2694036f6fab966564c1c44bc74330f22c2 0 0 0 0 0 0 0 0 0 0 1 5737a840aa867025dcb506f24cb5546f16b4d777 0 0 0 0 0 0 0 0 0 0 1 749e81272630eb4755e4a7bca10fe3e3524d77ce 0 0 0 0 0 0 0 0 0 0 1 9483b16c4904908115f4538525e37f776f4596d4 0 0 0 0 0 0 0 0 0 0 1 9dbcce359070c879f20843e19564aee545f80d2d 0 0 0 0 0 0 0 0 0 0 1 b33f00bd1109e1ae3ffa757d0aef0a25942f2ba3 0 0 0 0 0 0 0 0 0 0 1 eea677240a425ed7ccdeff69feb2d377a5542599 0 0 0 0 0 0 0 0 0 0 1 f8452649773eb7e024bfa59c395afa0c302d1928 0 0 0 0 0 0 0 0 0 0 1 toph 0 0 0 0 0 0 0 0 0 0 1 zuko 0 0 0 0 0 0 0 0 0 0 0 18.3.3 Edge Weights Edges can also have weights, meaning some edges are valued more than others. In an edgelist, you can add a third “weight” column, entering higher numbers to denote a more important connection. In an adjacency matrix, you can put numbers other than 1 in the intersection to denote more important connections. For our example, we’ll stick with un-weighted connections for now. 18.3.4 Attributes Each network also typically has an attributes table, which looks just like typical tabular data, with each row belonging to a specific node in our network. Let’s load in and look at the sample attributes file. # load in data toy_attributes = read.csv(&quot;./data/toy_attributes.csv&quot;, header = TRUE, stringsAsFactors = FALSE) # show top of attributes table kable(head(toy_attributes, n = 10)) id year color zuko 2 purple nezuko 2 purple winnie the pooh 2 blue toph 2 purple chicken joe 2 green the rat from ratatouille 5 blue spider-man 3 blue yamaguchi tadashi 1 purple jude sweetwine 2 purple lord future 2 blue 18.3.5 Create an Example Network Before we start exploring specific measures, we’ll create a toy network to use as an example. Let’s start by loading in some packages. statnet is one of the major network packages in R. It allows you to compute many of the most common network measures, and run simulations called Exponential Random Graph Models. We’ll stick with the basics for now! # Run this to load statnet, if you need to install it, do so now. library(statnet) Now that we have our tools loaded, let’s create out first network. We’ll use the data you loaded in before. This toy network will be used as a visual for learning the measurements below. We are going to turn the attributes file and edgelist into a statnet network object. A network object is a special kind of list in R. It is formatted in a way that the other statnet functions expect. While you could edit it like a normal list, it is highly recommended you use the other statnet functions to manipulate this object to make sure you don’t break any of the data expectations. We’ll use the network function to create our network object. Before we create it, we will sort our attributes file alphabetically. This is super important, as the network object will automatically sort things itself. If we do not sort our attributes dataframe to match, all of our measures later will be misaligned! # sort your attributes frame alphabetically. Super important! toy_attributes = toy_attributes[order(toy_attributes$id), ] # make network! # we will cover the `directed = FALSE` argument soon. toy_network = network(toy_edgelist, directed = FALSE) Before we move on, let’s add a net_id column to our attributes dataframe. This will let us easily check what the network object IDs are for our nodes. # add ID column toy_attributes$net_id = 1:nrow(toy_attributes) We can inspect our new network by calling the summary function on it. Don’t worry too much about the output yet. summary(toy_network) ## Network attributes: ## vertices = 96 ## directed = FALSE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## bipartite = FALSE ## total edges = 88 ## missing edges = 0 ## non-missing edges = 88 ## density = 0.01929825 ## ## Vertex attributes: ## vertex.names: ## character valued attribute ## 96 valid vertex names ## ## No edge attributes ## ## Network edgelist matrix: ## [,1] [,2] ## [1,] 56 96 ## [2,] 5 96 ## [3,] 49 96 ## [4,] 82 96 ## [5,] 76 96 ## [6,] 51 96 ## [7,] 41 96 ## [8,] 93 96 ## [9,] 27 96 ## [10,] 1 96 ## [11,] 25 88 ## [12,] 22 88 ## [13,] 4 88 ## [14,] 16 88 ## [15,] 64 88 ## [16,] 17 94 ## [17,] 73 94 ## [18,] 6 94 ## [19,] 80 94 ## [20,] 66 94 ## [21,] 75 94 ## [22,] 70 94 ## [23,] 61 94 ## [24,] 95 94 ## [25,] 96 94 ## [26,] 46 93 ## [27,] 65 93 ## [28,] 30 93 ## [29,] 39 93 ## [30,] 36 93 ## [31,] 79 69 ## [32,] 7 69 ## [33,] 11 69 ## [34,] 10 69 ## [35,] 43 69 ## [36,] 33 69 ## [37,] 58 69 ## [38,] 83 69 ## [39,] 20 92 ## [40,] 37 92 ## [41,] 57 92 ## [42,] 74 92 ## [43,] 71 92 ## [44,] 18 92 ## [45,] 2 92 ## [46,] 63 92 ## [47,] 50 92 ## [48,] 48 91 ## [49,] 60 91 ## [50,] 34 91 ## [51,] 26 91 ## [52,] 13 91 ## [53,] 38 91 ## [54,] 84 91 ## [55,] 12 95 ## [56,] 28 95 ## [57,] 9 95 ## [58,] 59 95 ## [59,] 29 95 ## [60,] 73 95 ## [61,] 52 95 ## [62,] 62 95 ## [63,] 19 95 ## [64,] 31 86 ## [65,] 78 86 ## [66,] 15 86 ## [67,] 23 86 ## [68,] 73 86 ## [69,] 35 87 ## [70,] 32 87 ## [71,] 54 87 ## [72,] 40 87 ## [73,] 77 87 ## [74,] 24 89 ## [75,] 72 89 ## [76,] 81 89 ## [77,] 45 89 ## [78,] 3 89 ## [79,] 14 85 ## [80,] 67 85 ## [81,] 68 85 ## [82,] 8 85 ## [83,] 55 85 ## [84,] 21 90 ## [85,] 44 90 ## [86,] 47 90 ## [87,] 53 90 ## [88,] 42 90 Then we’ll add the node attributes to the network object. If you run summary again you should see the values from our toy_attributes have been added. # add each attribute to network. # do this by looking at every column, then adding it to the network for(col_name in colnames(toy_attributes)) { toy_network = set.vertex.attribute(x = toy_network, attrname = col_name, value=toy_attributes[,col_name]) } Let’s see what out network looks like! plot(toy_network) There we are. The default plotting in statnet is ugly. For the sake of our eyes, and for exploring some of the measure we create, we’ll use the visNetwork package to visualize our networks. It will make the code a bit more cumbersome, but it will be worth it. From now on, we will need to use the edges and attributes dataframes for plotting. This means we will often need to run commands twice, once for the network and once for the dataframes. When you are working with networks for research, you would usually do everything you need on your network, than create a dataframe from it all at once. We will need to deal with a bit of redundancy to take things one step at a time. Let’s try plotting again with visNetwork, using the dataframes. We’ll give the visNetwork function our edgelist and attributes dataframe. We’ll also tell it to plot the names from our attributes dataframe so we can see them when we hover over the nodes in the plot. # add pop-up tooltips with names # visNetwork uses the &quot;title&quot; column to create pop-up boxes toy_attributes$title = toy_attributes$id # plot! visNetwork(nodes = toy_attributes, edges = toy_edgelist) %&gt;% visInteraction(zoomView = FALSE) Nice. 18.3.6 Components Most often when working with networks you want to limit your analysis to one cluster or component, typically the largest one in your network. If segments of your network aren’t connected, you can’t answer many of the relational questions network analysis is good for! Let’s limit our network to the largest component: # find what nodes are part of the largest component toy_network%v%&quot;lc&quot; = component.largest(toy_network) # delete those nodes that are not ## in the network toy_network = delete.vertices(toy_network, which(toy_network%v%&quot;lc&quot; == FALSE)) ## in our dataframes toy_attributes = toy_attributes[toy_attributes$id %in% as.character(toy_network%v%&quot;id&quot;),] toy_edgelist = toy_edgelist[which(toy_edgelist$to %in% toy_attributes$id | toy_edgelist$from %in% toy_attributes$id),] # plot! visNetwork(nodes = toy_attributes, edges = toy_edgelist) %&gt;% visInteraction(zoomView = FALSE) 18.3.7 Limitations of Network Data Before we move on we should take a moment to talk about some the the caveats when using network data. While powerful, network analysis is particularly picky when in comes to data requirements. I’ll cover the two biggest ones below. You should always keep these in mind when using or interpreting network tools. 18.3.7.1 Missing Data Network analysis is very vulnerable to missing data. A simple way to understand why is to make a small adjustment to our network. I’ve highlighted one node in green. This node is structurally vital to the network; without it, the shape of the network as a whole will change. If we remove this node, the network changes in a major way! Imagine these nodes are people, and that missing node is the one person you forget to survey, or was sick the day data was collected. This could massively change the outcome of your analyses. There is some advanced research going on to detect and replace missing data like this if you have enough context, but it is not something to rely on. 18.3.7.2 Network Boundaries Network analysis is all about looking at the relationships between entities. However, following all connections an entity has can quickly spiral out of hand. For example, if you wanted to map your own social network, where would you start? You would include yourself, then your friends and family, but what about after that? Your friends and family have friends and family, as do their friends and family, as do their … and so on. If you are looking at human networks, every human will be included if you look far enough, so how do you decide when to stop? There is no easy answer. If you are looking at a pre-defined group (e.g. this class), you can set the boundaries to include everyone in this class and the connection between them. However, that doesn’t really capture the social networks of people in this class as most people will have friends elsewhere. Another common method is setting an arbitrary number of “steps” or connections from a target population. If we were interested in a 2-step network from an individual, we would collect all of their relevant connections, and then ask all the people they nominated about their connections. Some sort of justification will be needed as to why you picked the number of steps that you did. 18.3.8 Projected Networks Often, you will not have individual level network data, but you will have data on group membership. For example, if you wanted to map the social networks of student, but don’t know who they actually hang around with, you may be able to use class rosters to build an approximate network. This is call a bipartite network, two-mode, or projected network. You can see an example below. In this figure there are two kinds of nodes, students and classes. You can “collapse” this into a student network by assuming every student connected to a class is connected to each other. The same is true with classes, such that classes are related to each other if a single student is enrolled in both. This assumption may not always be correct, and you need to take care if you are going to make it in your research. If a class has 300 students, it is most likely not correct to assume every student knows every other student in that class. For reference, this is what out projected class network looks like: 18.4 Graph Level Properties Now that we know what networks are and have some examples of how they are used and the data required, let’s get into actually analyzing them. There are a number of measures we can compute to understand the structure of a network as a whole. We will go over some basic network level ones here. These are single measures or attributes used to describe the entire network, and can be used to compare one network against another. Directed or Un-directed Density Centralization 18.4.1 Directed or Un-directed Networks can either be directed or un-directed. A directed network treats the edges between nodes as having a specific direction of flow, while an un-directed network considers all edges to be mutual. An example of each is presented below. Both edgelist and adjacency matrix datasets are inherently directed. For edgelists, the sender is often the first column, and the receiver is the second. For adjacency matrices the rows are considered senders and columns are receivers. Directionality is often specified when the network objects are created. When we created our toy network, we specified directed = FALSE to simplify things. If you want a directed network, the default is directed = TRUE for statnet networks. A directed network tracks which node is the source and which node is the receiver for an edge. Take for example the follow mechanic on Twitter. User A can follow User B, creating a directed edge from A to B, but B does not have to follow A in return. This can be useful when trying to understand the flows of resources that are finite such as money or goods. # visNetwork uses a column called &quot;arrows&quot; to show directionality in its plots. # For our edgelist, we&#39;ll just say every row is &quot;to&quot; for now toy_edgelist$arrows = &quot;to&quot; # this will show us what our network would look like if it was directed. visNetwork(toy_attributes, toy_edgelist, main = &quot;Directed&quot;) %&gt;% visInteraction(zoomView = FALSE) An un-directed network treats all ties as mutual, such that A and B are both involved equally in a tie. An example is the friend mechanic on Facebook. Once a friendship is established, both users are considered equal in the tie. This can be helpful when you do not have information on what node initiates a tie, or when events happen equally to a group of nodes, such as all nodes being connected through co-membership in a group. # lets drop the arrow column for now since our network is un-directed. toy_edgelist = toy_edgelist[,c(&quot;from&quot;, &quot;to&quot;)] # plot visNetwork(toy_attributes, toy_edgelist, main = &quot;Un-directed&quot;) %&gt;% visInteraction(zoomView = FALSE) Which of these will be useful to you will likely change from project to project. However, it is vital to understand what kind of network you are working with, as many network calculations we will talk about later change their behavior based on if the network is directed or not. 18.4.2 Density Density is the first real graph level metric that helps you understand what is particular about the network you are looking at. The density of a network is a numerical score showing how many ties exist in a network, given the max possible in that network. Mathematically that is \\(\\frac{Actual Edges}{Possible Edges}\\), where actual edges is the number of edges in the network, and possible edges is the number of edges if every single node in the network was connected to every other node. Networks that are more densely connected are considered to be more cohesive and robust. This means that the removal of any specific edge or node will not have a great effect of the network as a whole. It also typically means that any one node in the network will be more likely to have access to whatever resources are in the network, as there are more potential connections in the network to search for resources. To calculate the density of a network, we use the network.density() function. You can also see it if you use summary() on your network object. Below is our toy network and a less dense version to try and visualize the difference. Density is all about how many edges exist in the network. Notice that there are the same number of nodes in both of these networks. 18.4.3 Centralization Freeman Centralization (usually just called centralization) gives a sense of the shape of the network, namely how node level measures are distributed in a network. We’ll discuss node level measures next, but for now it is only important to understand that node level measures are numeric scores assigned to specific nodes rather than the network as a whole. This means that each node may have a different value. Consider the two networks below. The first “star” network would be considered highly centralized, as one node connects to all the others, while the rest of the nodes have no connections to each other. This star network would have a edge centralization score of 1, as 100% of the ties are connected with one node. The loop network would have a score of 0, as every node is equally connected to each other. Centralization is a measure of how unevenly node level metrics are distributed in a network. This is helpful when trying to understand if some nodes in the network have a larger influence, or are is some way more important than others. 18.5 Node Level Properties Node level measures are numeric representations of a node’s position and importance in a network. There are several common node level measures, and we will go over some of them here. Each measure tries to quantify a different aspect of a node’s position in the network so we can make an argument about why that specific node or class of nodes is important in some way. We will go over: Degree Geodesic distance Betweenness centrality Eigenvector centrality Most node level measures are only helpful within the context of the network they were generated for. This is because the measures are created in part using network level measures like density. This means it is alright to compare one node to another within the same network, but toy should node compare the node level measures between networks. 18.5.1 Degree Degree counts how many edges are connected to a node. You can count incoming, outgoing, or total (Freeman) degree. Incoming and outgoing degree only matter in directed networks. In un-directed networks, only total edges are applicable. Degree gives a very rough measure of how popular or central a node is in the network. If a node has more ties, it may indicate that node as being more central or important the network as a whole. Degree is a raw count of the number of edges a node has, this makes the interpretation of degree highly dependent on the size of the network. In a small network with only 25 total edges, having 10 of them would be significant. In a larger network with 250 total edges, 10 edges could be less impressive. Degree should thus be interpreted in the cortex of other nodes in the network. Let’s scale the node sizes of our toy network based on their total degree numbers. We’ll get degree counts for each of our nodes using the degree() function. We can save that into our dataframe and network for use later. For now I am naming columns to work specifically with visNetwork, we’ll make a proper dataframe for analyses later using data we saved in the network object. In our visualization, you can click on any node to highlight only the edges connected to that node. # find the degree of each node and save in the network # we will use the special `%v%` operator when assigning values to a network. `%v%` works like `$` for dataframes, allowing you to ask for specific values in the network # in this case `%v%` stands for vertex, and you can use `%e%` if you want to work with edges. # so let&#39;s get the degree counts, and assign them to the &quot;degree&quot; variable in our network object toy_network%v%&quot;degree&quot; = degree(toy_network) # visNetwork uses the &quot;value&quot; column to determine node size, so let&#39;s put it there as well for now. # we&#39;ll square the values just to make them more distinct toy_attributes$value = degree(toy_network)^2 # plot! visNetwork(toy_attributes, toy_edgelist, main = &quot;Degree Example&quot;) %&gt;% visInteraction(zoomView = FALSE) 18.5.2 Geodesic Distance Geodesic Distance is “the length of the shortest path via the edges or binary connections between nodes” (Kadushin 2012). In other words, if we treat the network as a map we can move along, with the nodes being stopping places and the edges being paths, the geodesic is the shortest possible path we can use to walk between two nodes. Nodes that on average have a shorter geodesic distance between all the other nodes in the network are considered to have have greater access to the resources in a network. This is because a node with a low average geodesic distance can theoretically “reach” the other nodes with less effort because it does not need to travel as far. To find the mean geodesic distance for each node in the network we will first need to find the geodesic distance from each node to every other node, then take the mean. Not super difficult, but there isn’t a single function to do it for us. First we will use the geodist() function to get all the geodesics. # get all the geodesics # I use the $gdist so we only get geodesics not counts geodist(toy_network)$gdist ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] ## [1,] 0 2 3 4 4 5 3 4 5 2 4 4 3 ## [2,] 2 0 3 4 4 5 3 4 5 2 4 4 3 ## [3,] 3 3 0 3 3 4 2 3 4 3 3 3 4 ## [4,] 4 4 3 0 2 4 3 2 4 4 2 2 5 ## [5,] 4 4 3 2 0 4 3 2 4 4 2 2 5 ## [6,] 5 5 4 4 4 0 4 4 2 5 4 4 6 ## [7,] 3 3 2 3 3 4 0 3 4 3 3 3 4 ## [8,] 4 4 3 2 2 4 3 0 4 4 2 2 5 ## [9,] 5 5 4 4 4 2 4 4 0 5 4 4 6 ## [10,] 2 2 3 4 4 5 3 4 5 0 4 4 3 ## [11,] 4 4 3 2 2 4 3 2 4 4 0 2 5 ## [12,] 4 4 3 2 2 4 3 2 4 4 2 0 5 ## [13,] 3 3 4 5 5 6 4 5 6 3 5 5 0 ## [14,] 5 5 4 4 4 2 4 4 2 5 4 4 6 ## [15,] 3 3 4 5 5 6 4 5 6 3 5 5 2 ## [16,] 3 3 4 5 5 6 4 5 6 3 5 5 2 ## [17,] 2 2 3 4 4 5 3 4 5 2 4 4 3 ## [18,] 3 3 4 5 5 6 4 5 6 3 5 5 2 ## [19,] 2 2 3 4 4 5 3 4 5 2 4 4 3 ## [20,] 2 2 3 4 4 5 3 4 5 2 4 4 3 ## [21,] 4 4 3 2 2 4 3 2 4 4 2 2 5 ## [22,] 2 2 3 4 4 5 3 4 5 2 4 4 3 ## [23,] 4 4 3 2 2 4 3 2 4 4 2 2 5 ## [24,] 3 3 2 3 3 4 2 3 4 3 3 3 4 ## [25,] 4 4 3 2 2 4 3 2 4 4 2 2 5 ## [26,] 3 3 4 5 5 6 4 5 6 3 5 5 2 ## [27,] 3 3 2 3 3 4 2 3 4 3 3 3 4 ## [28,] 3 3 2 3 3 4 2 3 4 3 3 3 4 ## [29,] 3 3 2 2 2 2 2 2 2 3 2 2 4 ## [30,] 3 3 2 3 3 4 2 3 4 3 3 3 4 ## [31,] 2 2 3 4 4 5 3 4 5 2 4 4 3 ## [32,] 5 5 4 4 4 2 4 4 2 5 4 4 6 ## [33,] 3 3 2 3 3 4 2 3 4 3 3 3 4 ## [34,] 2 2 3 4 4 5 3 4 5 2 4 4 3 ## [35,] 4 4 3 3 3 1 3 3 1 4 3 3 5 ## [36,] 2 2 3 4 4 5 3 4 5 2 4 4 1 ## [37,] 2 2 1 2 2 3 1 2 3 2 2 2 3 ## [38,] 3 3 2 1 1 3 2 1 3 3 1 1 4 ## [39,] 1 1 2 3 3 4 2 3 4 1 3 3 2 ## [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] ## [1,] 5 3 3 2 3 2 2 4 2 4 3 4 ## [2,] 5 3 3 2 3 2 2 4 2 4 3 4 ## [3,] 4 4 4 3 4 3 3 3 3 3 2 3 ## [4,] 4 5 5 4 5 4 4 2 4 2 3 2 ## [5,] 4 5 5 4 5 4 4 2 4 2 3 2 ## [6,] 2 6 6 5 6 5 5 4 5 4 4 4 ## [7,] 4 4 4 3 4 3 3 3 3 3 2 3 ## [8,] 4 5 5 4 5 4 4 2 4 2 3 2 ## [9,] 2 6 6 5 6 5 5 4 5 4 4 4 ## [10,] 5 3 3 2 3 2 2 4 2 4 3 4 ## [11,] 4 5 5 4 5 4 4 2 4 2 3 2 ## [12,] 4 5 5 4 5 4 4 2 4 2 3 2 ## [13,] 6 2 2 3 2 3 3 5 3 5 4 5 ## [14,] 0 6 6 5 6 5 5 4 5 4 4 4 ## [15,] 6 0 2 3 2 3 3 5 3 5 4 5 ## [16,] 6 2 0 3 2 3 3 5 3 5 4 5 ## [17,] 5 3 3 0 3 2 2 4 2 4 3 4 ## [18,] 6 2 2 3 0 3 3 5 3 5 4 5 ## [19,] 5 3 3 2 3 0 2 4 2 4 3 4 ## [20,] 5 3 3 2 3 2 0 4 2 4 3 4 ## [21,] 4 5 5 4 5 4 4 0 4 2 3 2 ## [22,] 5 3 3 2 3 2 2 4 0 4 3 4 ## [23,] 4 5 5 4 5 4 4 2 4 0 3 2 ## [24,] 4 4 4 3 4 3 3 3 3 3 0 3 ## [25,] 4 5 5 4 5 4 4 2 4 2 3 0 ## [26,] 6 2 2 3 2 3 3 5 3 5 4 5 ## [27,] 4 4 4 3 4 3 3 3 3 3 2 3 ## [28,] 4 4 4 3 4 3 3 3 3 3 2 3 ## [29,] 2 4 4 3 4 3 3 2 3 2 2 2 ## [30,] 4 4 4 3 4 3 3 3 3 3 2 3 ## [31,] 5 3 3 2 3 2 2 4 2 4 3 4 ## [32,] 2 6 6 5 6 5 5 4 5 4 4 4 ## [33,] 4 4 4 3 4 3 3 3 3 3 2 3 ## [34,] 5 3 3 2 3 2 2 4 2 4 3 4 ## [35,] 1 5 5 4 5 4 4 3 4 3 3 3 ## [36,] 5 1 1 2 1 2 2 4 2 4 3 4 ## [37,] 3 3 3 2 3 2 2 2 2 2 1 2 ## [38,] 3 4 4 3 4 3 3 1 3 1 2 1 ## [39,] 4 2 2 1 2 1 1 3 1 3 2 3 ## [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] ## [1,] 3 3 3 3 3 2 5 3 2 4 2 2 ## [2,] 3 3 3 3 3 2 5 3 2 4 2 2 ## [3,] 4 2 2 2 2 3 4 2 3 3 3 1 ## [4,] 5 3 3 2 3 4 4 3 4 3 4 2 ## [5,] 5 3 3 2 3 4 4 3 4 3 4 2 ## [6,] 6 4 4 2 4 5 2 4 5 1 5 3 ## [7,] 4 2 2 2 2 3 4 2 3 3 3 1 ## [8,] 5 3 3 2 3 4 4 3 4 3 4 2 ## [9,] 6 4 4 2 4 5 2 4 5 1 5 3 ## [10,] 3 3 3 3 3 2 5 3 2 4 2 2 ## [11,] 5 3 3 2 3 4 4 3 4 3 4 2 ## [12,] 5 3 3 2 3 4 4 3 4 3 4 2 ## [13,] 2 4 4 4 4 3 6 4 3 5 1 3 ## [14,] 6 4 4 2 4 5 2 4 5 1 5 3 ## [15,] 2 4 4 4 4 3 6 4 3 5 1 3 ## [16,] 2 4 4 4 4 3 6 4 3 5 1 3 ## [17,] 3 3 3 3 3 2 5 3 2 4 2 2 ## [18,] 2 4 4 4 4 3 6 4 3 5 1 3 ## [19,] 3 3 3 3 3 2 5 3 2 4 2 2 ## [20,] 3 3 3 3 3 2 5 3 2 4 2 2 ## [21,] 5 3 3 2 3 4 4 3 4 3 4 2 ## [22,] 3 3 3 3 3 2 5 3 2 4 2 2 ## [23,] 5 3 3 2 3 4 4 3 4 3 4 2 ## [24,] 4 2 2 2 2 3 4 2 3 3 3 1 ## [25,] 5 3 3 2 3 4 4 3 4 3 4 2 ## [26,] 0 4 4 4 4 3 6 4 3 5 1 3 ## [27,] 4 0 2 2 2 3 4 2 3 3 3 1 ## [28,] 4 2 0 2 2 3 4 2 3 3 3 1 ## [29,] 4 2 2 0 2 3 2 2 3 1 3 1 ## [30,] 4 2 2 2 0 3 4 2 3 3 3 1 ## [31,] 3 3 3 3 3 0 5 3 2 4 2 2 ## [32,] 6 4 4 2 4 5 0 4 5 1 5 3 ## [33,] 4 2 2 2 2 3 4 0 3 3 3 1 ## [34,] 3 3 3 3 3 2 5 3 0 4 2 2 ## [35,] 5 3 3 1 3 4 1 3 4 0 4 2 ## [36,] 1 3 3 3 3 2 5 3 2 4 0 2 ## [37,] 3 1 1 1 1 2 3 1 2 2 2 0 ## [38,] 4 2 2 1 2 3 3 2 3 2 3 1 ## [39,] 2 2 2 2 2 1 4 2 1 3 1 1 ## [,38] [,39] ## [1,] 3 1 ## [2,] 3 1 ## [3,] 2 2 ## [4,] 1 3 ## [5,] 1 3 ## [6,] 3 4 ## [7,] 2 2 ## [8,] 1 3 ## [9,] 3 4 ## [10,] 3 1 ## [11,] 1 3 ## [12,] 1 3 ## [13,] 4 2 ## [14,] 3 4 ## [15,] 4 2 ## [16,] 4 2 ## [17,] 3 1 ## [18,] 4 2 ## [19,] 3 1 ## [20,] 3 1 ## [21,] 1 3 ## [22,] 3 1 ## [23,] 1 3 ## [24,] 2 2 ## [25,] 1 3 ## [26,] 4 2 ## [27,] 2 2 ## [28,] 2 2 ## [29,] 1 2 ## [30,] 2 2 ## [31,] 3 1 ## [32,] 3 4 ## [33,] 2 2 ## [34,] 3 1 ## [35,] 2 3 ## [36,] 3 1 ## [37,] 1 1 ## [38,] 0 2 ## [39,] 2 0 This output is just like an adjacency matrix, with row and columns being the network node IDs (net_id in our attributes dataframe). Next we would want sum all the columns for each row (so adding up all the geodesics for a node), and divide by the total number of nodes it can have an edge with to get the average geodesic distance for that node. This gives us the average geodesic distance for each node! # colsums gives us the sum of all columns for a row # we subtract one from the denominator becasue a node cannot have a geodesic distance with itself colSums(geodist(toy_network)$gdist) / (nrow(as.sociomatrix(toy_network)) - 1) ## [1] 3.131579 3.131579 2.947368 3.342105 3.342105 4.184211 2.947368 3.342105 ## [9] 4.184211 3.131579 3.342105 3.342105 3.842105 4.184211 3.842105 3.842105 ## [17] 3.131579 3.842105 3.131579 3.131579 3.342105 3.131579 3.342105 2.947368 ## [25] 3.342105 3.842105 2.947368 2.947368 2.447368 2.947368 3.131579 4.184211 ## [33] 2.947368 3.131579 3.210526 2.868421 1.973684 2.368421 2.157895 Lets add thus to our network and plot it. I will also add some color and labels so it is easier to see what this measure means. The red node has the longest average geodesic distance, and would need to travel through the whole network to reach the nodes on the opposite side. Meanwhile, the blue node has the smallest average geodesic distance because it is located near the middle of the network. # add mean geodesic distance to network object toy_network%v%&quot;mean_distance&quot; = (colSums(geodist(toy_network)$gdist))/(nrow(as.sociomatrix(toy_network)) - 1) # set all node colors in visNetwork to grey as default toy_attributes$color = c(&quot;grey&quot;) # add label as geodesic distance, rounding to 3 digits toy_attributes$label = round(toy_network%v%&quot;mean_distance&quot;, 3) # replace min average geodesic with blue, max with red toy_attributes$color[which(toy_network%v%&quot;mean_distance&quot; == max(toy_network%v%&quot;mean_distance&quot;))] = &quot;red&quot; toy_attributes$color[which(toy_network%v%&quot;mean_distance&quot; == min(toy_network%v%&quot;mean_distance&quot;))] = &quot;blue&quot; # make sure edges are grey too toy_edgelist$color = &quot;grey&quot; # plot visNetwork(toy_attributes, toy_edgelist, main = &quot;Geodesic Example&quot;) %&gt;% visInteraction(zoomView = FALSE) Note that while there is a correlation between degree counts (node size) and mean geodesic distance, one does not cause the other. This is our first instance of how network structure, not node attributes, can inform us about the nodes in a network. Essentially, looking at the network as a whole can tell us things about the people in it that is lost if we look only at individuals. 18.5.3 Centrality Centrality scores encompass a wide range of measures computed at the node level. Each tries to understand the importance of a single node within the structure of a network by looking at the nodes connection patterns to other nodes. Any centrality measure can be used to create a network level centralization score like we discussed above. We will go through some of the common centrality measures here, but know there are several more we will not cover. 18.5.3.1 Betweenness Centrality Betweenness Centrality is one of the most common centrality measures. It tries to calculate the extent to which a node acts as a gatekeeper or broker in the network. A broker bridges two otherwise disconnected segments in a network. If there are two parts of a network that would otherwise be broken apart if a node was removed, they would have a high betweenness centrality. The fragmenting of a network is not a prerequisite however, simply acting as an effective “shortcut” in a network can also raise a node’s betweenness centrality. Betweenness is calculated using geodesic distances, and gives a higher score to nodes that lie on more geodesic paths. The next network plot shows the size of nodes as their degree, with a label showing their betweenness centrality score. Centrality score are usually normalized such that their scores all sum to 1. This way, you can easily compare nodes within the network (but not between networks), and understand how nodes relate to each other structurally. It is possible for a node to have a 0 betweenness score if no geodesic distances pass through them. Like last time I’ve colored the nodes so that the node with the highest betweenness centrality is red, while the lowest is blue. Compared to distance however, it is considered advantageous to have a high betweenness centrality, as this means that nodes acts as a gatekeeper in the network, which can be a powerful position. Contrast this with having a low mean distance, which meant you were closer to all other nodes. # add eigenvector centrality to network object as &quot;norm_betweenness&quot; # we also tell it we are treating our data as un-directed (&quot;graph&quot;), rather than the default directed (&quot;digraph&quot;), same with `cmode = &quot;undirected&quot;` # we also say we want a normalized (0-1, sum to 1) score using `rescale = TRUE` toy_network%v%&quot;norm_betweenness&quot; = betweenness(dat = toy_network, gmode = &quot;graph&quot;, rescale = TRUE, cmode = &quot;undirected&quot;) # add label as geodesic distance, rounding to 3 digits toy_attributes$label = round(toy_network%v%&quot;norm_betweenness&quot;, 3) # reset all nodes to grey toy_attributes$color = c(&quot;grey&quot;) # replace min average geodesic with blue, max with red toy_attributes$color[which(toy_network%v%&quot;norm_betweenness&quot; == max(toy_network%v%&quot;norm_betweenness&quot;))] = &quot;red&quot; toy_attributes$color[which(toy_network%v%&quot;norm_betweenness&quot; == min(toy_network%v%&quot;norm_betweenness&quot;))] = &quot;blue&quot; # plot visNetwork(toy_attributes, toy_edgelist, main = &quot;Betweenness Centrality Example&quot;) %&gt;% visInteraction(zoomView = FALSE) 18.5.3.2 Eigenvector Centrality Eigenvector Centrality is commonly known as a measure of “popular friends.” Rather than looking at the network position of a node, it looks at the network positions of nodes connected to it. Nodes with a high eigenvector score will be connected to nodes more prominent in the network. Nodes with low degree can have high eigenvector scores if they are connected to important nodes. In real life networks this can be interpreted as being close to influential others in a network. I’ve colored the nodes so that the node with the highest eigenvector centrality is red, while the lowest is blue. It is considered advantageous to have a high eigenvector centrality, as this means you are well connected to other popular nodes. # add eigenvector centrality to network object as &quot;evc&quot; # we also tell it we are treating our data as un-directed (&quot;graph&quot;), rather than the default directed (&quot;digraph&quot;) # we also say we want a normalized (0-1, sum to 1) score using `rescale = TRUE` toy_network%v%&quot;evc&quot; = evcent(toy_network, gmode = &quot;graph&quot;, rescale = TRUE) # add label as geodesic distance, rounding to 3 digits toy_attributes$label = round(toy_network%v%&quot;evc&quot;, 3) # reset all nodes to grey toy_attributes$color = c(&quot;grey&quot;) # replace min average geodesic with blue, max with red toy_attributes$color[which(toy_network%v%&quot;evc&quot; == max(toy_network%v%&quot;evc&quot;))] = &quot;red&quot; toy_attributes$color[which(toy_network%v%&quot;evc&quot; == min(toy_network%v%&quot;evc&quot;))] = &quot;blue&quot; # plot visNetwork(toy_attributes, toy_edgelist, main = &quot;Eigenvector Centrality Example&quot;) %&gt;% visInteraction(zoomView = FALSE) 18.6 Network Workflow We have been taking our analyses one step at a time and plotting them. This is useful for learning, but slightly annoying in practice. Below I’ve aggregated how you would actually run analyses in practice so you can refer to it later. # data load toy_edgelist = read.csv(&quot;./data/toy_edgelist.csv&quot;, header = TRUE, stringsAsFactors = FALSE) toy_attributes = read.csv(&quot;./data/toy_attributes.csv&quot;, header = TRUE, stringsAsFactors = FALSE) # make a network ## sort your attributes frame alphabetically. Super important! toy_attributes = toy_attributes[order(toy_attributes$id), ] ## make network! toy_network_total = network(toy_edgelist, directed = FALSE) # largest component ## find what nodes are part of the largest component toy_network_total%v%&quot;lc&quot; = component.largest(toy_network_total) ## delete those nodes that are not ### in the network toy_network = delete.vertices(toy_network_total, which(toy_network_total%v%&quot;lc&quot; == FALSE)) ### in our dataframes toy_attributes = toy_attributes[toy_attributes$id %in% as.character(toy_network_total%v%&quot;vertex.names&quot;),] toy_edgelist = toy_edgelist[which(toy_edgelist$to %in% toy_attributes$id | toy_edgelist$from %in% toy_attributes$id),] # generate measures ## degree toy_network_total%v%&quot;degree&quot; = degree(toy_network_total) ## mean geodesic toy_network_total%v%&quot;mean_distance&quot; = (colSums(geodist(toy_network_total)$gdist)) / (nrow(as.sociomatrix(toy_network_total)) - 1) ## normalized betweenness toy_network_total%v%&quot;norm_betweenness&quot; = betweenness(dat = toy_network_total, gmode = &quot;graph&quot;, rescale = TRUE, cmode = &quot;undirected&quot;) ## eigenvector toy_network_total%v%&quot;evc&quot; = evcent(toy_network_total, gmode = &quot;graph&quot;, rescale = TRUE) # add back to attributes dataframe ## degree toy_attributes$degree = toy_network_total%v%&quot;degree&quot; ## mean geodesic toy_attributes$mean_distance = toy_network_total%v%&quot;mean_distance&quot; ## normalized betweenness toy_attributes$norm_betweenness = toy_network_total%v%&quot;norm_betweenness&quot; ## eigenvector toy_attributes$evc = toy_network_total%v%&quot;evc&quot; Finally, we can then look at the network measures for our nodes. This dataframe can be used for plotting or further analyses. kable(head(toy_attributes)) id year color degree mean_distance norm_betweenness evc 22 028f5d1f351d38cd6553ab4674b19725d5ea3d3c NA NA 2 3.131579 0 0.0218579 15 19d5b2694036f6fab966564c1c44bc74330f22c2 NA NA 2 3.131579 0 0.0218579 30 1ae1327030b801f0046278d197378603b51de4b7 NA NA 2 2.947368 0 0.0269529 67 258f00e649e6a452acb67cb9297c88820c05e2a7 NA NA 2 3.342105 0 0.0217338 65 2e9fed34d6b2d42052850b46aeaa97f9fe6542dc NA NA 2 3.342105 0 0.0217338 75 3220545023e21c80db2a4d4e10b3eb4217b90605 NA NA 2 4.184210 0 0.0047224 18.7 Network Tools There are several ways to interact with network data in R. Thus far we have been using a combination of statnet for analysis and visNetwork for visualization. Here we’ll gloss over some other tools and what they are used for. Rather than a comprehensive tutorial, this section is just meant to introduce you to what tools are out there so you can investigate them further if you have a need fro them. 18.7.1 Network Models 18.7.1.1 statnet statnet is one of the two largest network packages in R. It allows you to create network objects and generate the network measures we’ve been looking at this far. statnet’s claim to fame however is it’s ability to run network simulations, called exponential random graph models (ERGMs). These models allow you to keep some aspect of a network constant and generate random networks that fit your specifications. This can help you highlight one structural aspect of a network and prove that, all else being random, it is important. To learn more about ERGMs, see (Robins, Pattison, et al. 2007; Robins, Snijders, et al. 2007). Learn more on the statnet website. 18.7.1.2 igraph igraph is the other big network package in R. It has more network measures than statnet, but less robust simulation capabilities. While the same network concepts you’ve learned with statnet will help you understand all networks, the code syntax for igraph is different, so you can’t use the two tools interchangeably. Notably, some functions are named the same in statnet and igraph, so it is advised not to load both at the same time. Learn more on the igraph website. 18.7.1.3 intergraph intergraph is a utility package in R to convert between statnet and igraph network objects. This means you can prepare your data in your package of choice, then convert your network and use what tools you need from the other package. 18.7.1.4 tidygraph tidygraph is a relatively new tool in R, built to use tidyverse syntax. It can do many of the same basic network analyses of the two older packages, but lacks the variety of igraph or the simulation capability of statnet. Learn more on the tidygraph website. 18.7.2 Network Visualization 18.7.2.1 Built-in While we avoided it today, all network packages have built in visualization capabilities that can look nice if you work on it. The advantage of these is that you can use the network objects themselves to pull attributes from the networks for your plots (e.g. pull degree centrality for node size). 18.7.2.2 visNetwork visNetwork can make some nice interactive network visualizations with relatively simple code. This is great for learning and for exploring networks interactively. However, it does have significant downsides. For one, you have to keep separate dataframes for your edges and attributes as it cannon run directly on network objects. Most importantly it cannot produce static network images! You will most likely need more static plots than interactive ones. If you can only dig deeply into one tool, this one may not be the best to specialize in. 18.7.2.3 ggraph ggraph uses ggplot syntax to plot networks. There are several packages that do this in various stages of development. To my understanding, ggraph is the most recent incarnation still under active development. Learn more of the ggraph website. "],["statistics.html", "19 Statistics 19.1 Introduction 19.2 Uses of simulation 19.3 Mathematical statistics 19.4 Statistical inference 19.5 Regression 19.6 Model selection 19.7 Cross-validation", " 19 Statistics 19.1 Introduction It is useful to begin with some concrete examples of statistical questions to motivate the material that we’ll cover in the workshop. This will also help confirm that your R environment is working. These data files come from the fosdata package, which you can optionally install to your computer in order to get access to all the associated metadata in the help files. I will show loading the data from the workshop website. # optional: install the fosdata and mosaicModels packages using remotes::install_github install.packages( &quot;remotes&quot; ) remotes::install_github( &quot;speegled/fosdata&quot; ) remotes::install_github( &quot;ProjectMOSAIC/mosaicModel&quot; ) library( &quot;ggformula&quot; ) library( &quot;mosaic&quot; ) library( &quot;mosaicModel&quot; ) # load the datasets mice_pot = read.csv( url( &quot;https://raw.githubusercontent.com/ucdavisdatalab/adventures_in_data_science/master/data/mice_pot.csv&quot; )) barnacles = read.csv( url( &quot;https://raw.githubusercontent.com/ucdavisdatalab/adventures_in_data_science/master/data/barnacles.csv&quot; )) Births78 = read.csv( url(&quot;https://raw.githubusercontent.com/ucdavisdatalab/adventures_in_data_science/master/data/births.csv&quot;)) smoking = read.csv( url(&quot;https://raw.githubusercontent.com/ucdavisdatalab/adventures_in_data_science/master/data/smoking.csv&quot;)) adipose = read.csv( url(&quot;https://raw.githubusercontent.com/ucdavisdatalab/adventures_in_data_science/master/data/adipose.csv&quot;)) 19.1.1 mice_pot dataset The mice_pot dataset contains data on an experiment that dosed four groups of mice with different levels of THC. There was a low, medium, and high dosage group, as well as a control group that got no THC. The mice were were then observed for a while and their total movement was quantified as a percentage of the baseline group mean. Some statistical questions that might arise here are: were there differences in the typical amount of movement between mice of different groups? What was the average amount of movement by mice in the medium dose group? Both of these questions can be approached in ways that relate to the particular sample, which is kind of interesting descriptions of the data. For instance, if you install the dplyr package in your R environment, we can make those calculations right now. with( mice_pot, by(percent_of_act, group, mean)) ## group: 0.3 ## [1] 97.3225 ## ------------------------------------------------------------ ## group: 1 ## [1] 99.05235 ## ------------------------------------------------------------ ## group: 3 ## [1] 70.66787 ## ------------------------------------------------------------ ## group: VEH ## [1] 100 The means aren’t identical - there are clearly differences between all of the groups? Yes, in terms of the sample. But if you want to generalize your conclusion to cover what would happen to other mice that weren’t in the study, then you need to think about the population. In this case, that’s the population of all the mice that could have been dosed with THC. Because we don’t see we don’t see data from mice that weren’t part of the study, we rely on statistical inference to reach conclusions about the population. How is that possible? Well, some theorems from mathematical statistics can tell us about the distribution of the sample, relative to the population 19.1.2 barnacles dataset This dataset was collected by counting the barnacles in 88 grid squares on the Flower Garden Banks coral reef in the Gulf of Mexico. The counts were normalized to barnacles per square meter. Some questions that you might approach with statistical methods are, what is the average number of barnacles per square meter, and is it greater than 300? mean( barnacles$per_m ) ## [1] 332.0186 From that calculation, we see that the mean is 332 barnacles per square meter, which is greater than 300. But again, the first calculation has told us only about the mean of the particular locations that were sampled. Wouldn’t it be better to answer the questions by reference to the number of barnaces per square meter of reef, rather than square meter of measurement? Here, the population is then entire area of the Flower Garden Banks reef. Again, we will be able to answer the questions relative to the entire reef by working out the sample mean’s distribution, relative to the population. 19.1.3 Sample and population I’ve made reference to samples and populations, and they are fundamental concepts in statistics. A sample is a pretty easy thing to understand - it is the data, the hard numbers that go into your calculations. The population is trickier. It’s the units to which you are able to generalize your conclusions. For the barnacle data, in order for the population to be the entire Flower Garden Banks reef, the sample must have been carefully selected to ensure it was representative of the entire reef (for instance, randomly sampling locations so that any location on the reef might have been selected). For the mice on THC, the population is all the mice that might have been selected for use in the experiment. How big that population is depends on how the mice were selected for the experiment. Randomly selecting the experimental units from a group is a common way of ensuring that the results can generalize to that whole group. A non-random sample tends to mean that the population to which you can generalize is quite limited. What sort of population do you think we could generalize about if we recorded the age of everyone in this class? 19.2 Uses of simulation The study of statistics started in the 1800s but only barely. The field’s modern version was almost entirely developed during the first half of the 20th century - notably a time when data and processing power were in short supply. Today, that’s not so much the case and if you did the assigned reading then you saw that statisticians are very much still grappling with how to teach statistics in light of the developments in computational power over the past 40 years. Traditionally, statisticians are very concerned with assessing the normality of a sample, because the conclusions you get from traditional statistical methods depend on a sample coming from a normal distribution. Nowadays, there are a lot of clever methods that can avoid the need to assume normality. I’m going to show you some of those methods, because they help avoid getting into mathematical statistics. If you want to know more, one of the assigned readings was the introduction to a book that would be a great reference for self-guided study. We will use simulation-based methods extensively today. This is the density curve of a standard normal distribution: # set z to -4, 4 z = seq(-4, 4, length.out = 1000) # plot the standard normal density function with some annotations plot( x=z, y=dnorm(z), bty=&#39;n&#39;, type=&#39;l&#39;, main=&quot;standard normal density&quot;) # annotate the density function with the 5% probability mass tails polygon(x=c(z[z&lt;=qnorm(0.025)], qnorm(0.025), min(z)), y=c(dnorm(z[z&lt;=qnorm(0.025)]), 0, 0), col=grey(0.8)) polygon(x=c(z[z&gt;=qnorm(0.975)], max(z), qnorm(0.975)), y=c(dnorm(z[z&gt;=qnorm(0.975)]), 0, 0), col=grey(0.8)) And this is a histogram of samples taken from that same distribution: # sample 20 numbers from a standard normal and draw the histogram x = rnorm(20) round( sort(x), 2) ## [1] -2.60 -1.44 -1.21 -0.72 -0.52 -0.42 -0.32 0.16 0.20 0.32 0.35 0.48 ## [13] 0.60 0.83 1.04 1.09 1.09 1.25 2.40 2.72 hist(x) Do the numbers seem to come from the high-density part of the Normal density curve? Are there any that don’t? It isn’t surprising if some of your x samples are not particularly close to zero. One out of twenty (that’s five percent) samples from a standard Normal population are greater than two or less than negative two, on average. That’s “on average” over the population. Your sample may be different. Here is the density of the exponential distribution: # draw the desity of an Exponential distribution t = seq(-1, 5, length.out=1000) plot( x=t, y=dexp(t), bty=&#39;n&#39;, type=&#39;l&#39;) And here is a histogram of 20 samples taken from that distribution: # sample 20 numbers from a histogram and plot the histogram ex = rexp( 20 ) round( sort(ex), 2) ## [1] 0.00 0.05 0.08 0.17 0.38 0.42 0.46 0.46 0.62 0.82 1.03 1.07 1.24 1.31 1.48 ## [16] 1.55 1.67 1.99 3.22 3.92 hist( ex ) The histograms are clearly different, but it would be difficult to definitively name the distribution of the data by looking at a sample. 19.3 Mathematical statistics The mean has some special properties: you’ve seen how we can calculate the frequency of samples being within an interval based on known distributions. But we need to know the distribution. It turns out that the distribution of the sample mean approaches the Normal distribution as the sample size increases, for almost any independent data. That allows us to create intervals and reason about the distribution of real data, even though the data’s distribution is unknown. 19.3.1 Law of large numbers The law of large numbers says that if the individual measurements are independent, then the mean of a sample tends toward the mean of the population as the sample size gets larger. This is what we’d expect, since we showed the rate at which the variance of the sample mean gets smaller is \\(1/n\\). nn = c(1, 2, 4, 8, 12, 20, 33, 45, 66, 100) means = sapply( nn, function(n) mean( rnorm(n) ) ) plot(nn, means, bty=&#39;n&#39;, ylab = &quot;sample mean&quot;) abline(h=0, lty=2) 19.3.2 Central limit theorem The most important mathematical result in statistics, the Central Limit Theorem says that if you take (almost) any sample of random numbers and calculate its mean, the distribution of the mean tends toward a normal distribution. We illustrate the “tending toward” with an arrow and it indicates that the distribution of a sample mean is only approximately Normal. But if the original samples were from a Normal distribution then the sample mean has an exactly Normal distribution. From here, I’ll start writing the mean of a random variable \\(X\\) as \\(\\bar{X}\\) and the mean of a sample \\(x\\) as \\(\\bar{x}\\). \\[ \\bar{X} \\rightarrow N(\\mu, \\frac{\\sigma^2}{n}) \\] And because of the identities we learned before, you can write this as \\[\\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\rightarrow N(0, 1) \\] This is significant because we can use the standard normal functions on the right, and the data on the left, to start answering questions like, “what is the 95% confidence interval for the population mean?” # generate 20 samples from a uniform distribution and plot their histogram N = 20 u = rexp( N ) hist( u ) # generate 100 repeated samples of the same size, calculate the mean of each one, and plot the histogram of the means. B = 100 uu = numeric( B ) for ( i in 1:B ) { uu[[i]] = mean( rexp(N) ) } hist(uu) # what happens as B and N get larger and smaller? Do they play different roles? 19.4 Statistical inference 19.4.1 Confidence intervals In the fosdata package there is a dataset called mice_pot, which contains data from an experiment where mice were dosed with THC and then measured for motor activity as a percentage of their baseline activity. We are going to look at the group that got a medium dose of THC. # extract just the mice that got the medium dose of THC mice_med = mice_pot[ mice_pot$group == 1, ] # assess normality with histogram and QQ plot hist( mice_med$percent_of_act ) qqnorm( mice_med$percent_of_act ) 19.4.1.1 Find the 80% confidence interval for the population mean Now we are using our sample to make some determination about the population, so this is statistical inference. Our best guess of the population mean is the sample mean, mean( mice_med$percent_of_act ), which is 99.1%. But to get a confidence interval, we need to use the formula \\[ \\bar{x} \\pm t_{n-1, 0.1} * S / \\sqrt{n} \\] Fortunately, R can do all the work for us: # 80% confidence interval for location of mice_med mean: t.test( mice_med$percent_of_act, conf.level=0.8 ) ## ## One Sample t-test ## ## data: mice_med$percent_of_act ## t = 13.068, df = 11, p-value = 4.822e-08 ## alternative hypothesis: true mean is not equal to 0 ## 80 percent confidence interval: ## 88.71757 109.38712 ## sample estimates: ## mean of x ## 99.05235 19.4.2 Two-population test The test of \\(\\mu_0 = 100\\) is a one-population test because it seeks to compare a single population against a specified standard. On the other hand, you may wish to assess the null hypothesis that the movement of mice in the high-THC group is equal to the movement of mice in the medium-THC group. This is called a two-population test, since there are two populations to compare against each other. The null hypothesis is \\(\\mu_{0, med} = \\mu_{0, high}\\). Testing a two-population hypothesis requires first assessing normality and also checking whether the variances are equal. There are separate procedures when the variances are equal vs. unequal. #extract the samples to be compared a = mice_pot$percent_of_act[ mice_pot$group == 1] b = mice_pot$percent_of_act[ mice_pot$group == 3] # check for equal variances - these are close enough var(a) ## [1] 689.4729 var(b) ## [1] 429.4551 # confirm equal variances with a boxplot boxplot(a, b) # check whether the high-THC mice movement is Normal # (we already checked for the medium-dose mice) qqnorm(b) # two pop test t.test(a, b, var.equal=TRUE) ## ## Two Sample t-test ## ## data: a and b ## t = 2.7707, df = 20, p-value = 0.0118 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 7.014608 49.754345 ## sample estimates: ## mean of x mean of y ## 99.05235 70.66787 19.4.3 Hypothesis tests for non-normal data Just as with the confidence intervals, there is a bootstrap hypothesis test that can be used where the data are not normal. There are other options, too, with clever derivations. The one I’ll show you is the Wilcoxon test, which is based on the ranks of the data. Since we’e already seen that the barnacles per square meter data are not normal, I will illustrate testing the null hypothesis that \\(\\mu_0\\) = 300 barnacles per square meter. This is a one-population test, and a two-sided alternative. # wilcoxon test for 300 barnacles per square meter wilcox.test( barnacles$per_m ) ## ## Wilcoxon signed rank test with continuity correction ## ## data: barnacles$per_m ## V = 3916, p-value = 3.797e-16 ## alternative hypothesis: true location is not equal to 0 19.5 Regression Regression is a mathematical tool that allows you to estimate how some response variable is related to some predictor variable(s). There are methods that handle continuous or discrete responses of many different distributions, but we are going to focus on linear regression here. Linear regression means that the relationship between the predictor variable(s) and the response is a linear one. To illustrate, we’ll create a plot of the relationship between the waist measurement and bmi of 81 adults: # plot the relationship between the waist_cm and bmi variables with(adipose, plot(waist_cm, bmi), bty=&#39;n&#39; ) The relationship between the two is apparently linear (you can imagine drawing a straight line through the data). The general mathematical form of a linear regression line is \\[ y = a + \\beta x + \\epsilon \\] Here, the response variable (e.g., BMI) is called \\(y\\) and the predictor (e.g. waist measurement) is \\(x\\). The coefficient \\(\\beta\\) indicates how much the response changes for a change in the predictors (e.g., the expected change in BMI with a 1cm change in waist measurement). Variable \\(a\\) denotes the intercept, which is a constant offset that aligns the mean of \\(y\\) with the mean of \\(x\\). Finally, \\(\\epsilon\\) is the so-called residual error in the relationship. It represents the variation in the response that is not due to the predictor(s). 19.5.1 Fitting a regression line The R function to fit the model is called lm(). Let’s take a look at an example: # fit the linear regression BMI vs waist_cm fit = lm( bmi ~ waist_cm, data=adipose ) # plot the fitted regression: begin with the raw data with( adipose, plot(waist_cm, bmi, bty=&#39;n&#39;) ) #now plot the fitted regression line (in red) abline( coef(fit)[[1]], coef(fit)[[2]], col=&#39;red&#39; ) 19.5.2 Assumptions and diagnostics “Fitting” a linear regression model involves estimating \\(a\\) and \\(\\beta\\) in the regression equation. You can can do this fitting procedure using any data, but the results won’t be reliable unless some conditions are met. The conditions are: Observations are independent. The linear model is correct. The residual error is Normally distributed. The variance of the residual error is constant for all observations. The first of these conditions can’t be checked - it has to do with the design of the experiment. The rest can be checked, though, and I’ll take them in order. 19.5.2.1 Checking that the linear model is correct In the cast of a simple linear regression model (one predictor variable), you can check this by plotting the predictor against the response and looking for a linear trend. If you have more than one predictor variable, then you need to plot the predictions against the response to look for a linear trend. We’ll see an example by adding height as a predictor for BMI (in addition to waist measurement). # linear model for BMI using waist size and height as predictors fit2 = lm( bmi ~ waist_cm + stature_cm, data=adipose ) # plot the fitted versus the predicted values plot( fit2$fitted.values, adipose$bmi, bty=&#39;n&#39; ) 19.5.2.2 Checking that the residuals are normally distributed We have already learned about the QQ plot, which shows visually whether some values are Normally distributed. In order to depend upon the fit from a linear regression model, we need to see that the residuals are Normally distributed, and we use the QQ plot to check. 19.5.2.3 Checking that the vaiance is constant In an earlier part, we saw that the variance is the average of the squared error. But that would just be a single number, when we want to see if there is a trend. So like the QQ plot, you’l plot the residuals and use your eyeball to discern whether there is a trend in the residuals or if they are approximately constant - this is called the scale-location plot. The QQ plot and scale-location plot are both created by plotting the fitted model object # set up the pattern of the panels layout( matrix(1:4, 2, 2) ) # make the diagnostic plots plot( fit ) The “Residuals vs. Fitted” plot is checking whether the linear model is correct. There should be no obvious pattern if the data are linear (as is the casre here). The Scale-Location plot will have no obvios pattern if the variance of the residuals is constant, as is the case here (you might see a slight pattern in the smoothed red line but it isn’t obvious). And the QQ plot will look like a straight line if the residuals are from a Normal distribution, as is the case here. So this model is good. The fourth diagnostic plot is the Residuals vs. Leverage plot, which is used to identify influential outliers. We won’t get into that here. 19.5.3 Functions for inspecting regression fits When you fit a linear regression model, you are estimating the parameters of the regression equation. In order to see those estimates, use the summary() function on the fitted model object. # get the model summary summary( fit2 ) ## ## Call: ## lm(formula = bmi ~ waist_cm + stature_cm, data = adipose) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.1290 -1.0484 -0.2603 1.2661 5.2572 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.38196 3.82700 3.758 0.000329 *** ## waist_cm 0.29928 0.01461 20.491 &lt; 2e-16 *** ## stature_cm -0.08140 0.02300 -3.539 0.000680 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.724 on 78 degrees of freedom ## Multiple R-squared: 0.844, Adjusted R-squared: 0.84 ## F-statistic: 211 on 2 and 78 DF, p-value: &lt; 2.2e-16 Here you can see that the average marginal effect of one additional centimeter of waist measurement is to increase BMI by 0.3 and an additional centimeter of height is associated with a change to BMI of -0.08. You can get the coefficients from the fitted model object using the coef() function, and there are some other functions that allow you to generate the values shown in the summary table. # get the coefficients of the fitted regression beta = coef( fit2 ) round( beta, 2 ) ## (Intercept) waist_cm stature_cm ## 14.38 0.30 -0.08 Get the variance-covariance matrix: round( vcov( fit2 ), 4) # compare the square root of the diagonals of the variance-covariance matrix # to the standard errors are reported in the summary table: se = sqrt( diag( vcov(fit2) )) # here are the standard errors: round( se, 3 ) ## (Intercept) waist_cm stature_cm ## 3.827 0.015 0.023 # calculate the t-statistics for the regression coefficients # (compare these to the t-statistics reorted in the summary table) t_stats = beta / se # show the t-statistics: round( t_stats, 2 ) ## (Intercept) waist_cm stature_cm ## 3.76 20.49 -3.54 # calculate the p-values: pval = 2 * pt( abs(t_stats), df=78, lower.tail=FALSE ) round(pval, 4) ## (Intercept) waist_cm stature_cm ## 3e-04 0e+00 7e-04 # this is the residual standard error: sd( fit2$residuals ) * sqrt(80 / 78) ## [1] 1.72357 # R-squared is the proportion of variance # explained by the regression model round( 1 - var(fit2$residuals) / var(adipose$bmi), 3 ) ## [1] 0.844 19.5.4 A model that fails diagnostics We’ve seen a model that has good diagnostics. Now let’s look at one that doesn’t. This time, we’ll use linear regression to make a model of the relationship between waist measurement and the visceral adipose tissue fat (measured in grams). The visceral adipose tissue fat is abbreviated vat in the data. First, since the model uses a single predictor variable, let’s look at the relationship with a pair plot. # plot the relationship between waist_cm and vat with( adipose, plot( waist_cm, vat, bty=&#39;n&#39; )) The plot is obviously not showing a linear relationship, which will violate one of the conditions for linear regression. Also, you can see that there is less variance of vat among the observations that have smaller waist measurements. So that will violate the assumption that the residual variance has no relationship to the fitted values. To see how these will show up in the diagnostic plots, we need to fit the linear regression model. # estimate the model for vat fit_vat = lm( vat ~ waist_cm, data = adipose ) # there is no problem creating the summary table: summary( fit_vat ) ## ## Call: ## lm(formula = vat ~ waist_cm, data = adipose) ## ## Residuals: ## Min 1Q Median 3Q Max ## -996.25 -265.96 -61.87 191.24 1903.46 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3604.196 334.241 -10.78 &lt;2e-16 *** ## waist_cm 51.353 3.937 13.04 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 479 on 79 degrees of freedom ## Multiple R-squared: 0.6829, Adjusted R-squared: 0.6789 ## F-statistic: 170.2 on 1 and 79 DF, p-value: &lt; 2.2e-16 # show the diagnostic plots layout( matrix(1:4, 2, 2) ) plot( fit_vat ) There is obviously a curved pattern in the Residuals vs. Fitted plot, and in the Scale vs. Location plot. Residuals vs. Fitted shows a fan-shaped pattern, too, which reflects the increasing variance among the greater fitted values. The QQ plot is not a straight line, although the difference is not as obvious. In particular, the upper tail of residuals is heavier than expected. Together, all of these are indications that we may need to do a log transformation of the response. A log transformation helps to exaggerate the differences between smaller numbers (make the lower tail heavier) and collapse some difference among larger numbers (make the upper tail less heavy). # fit a regression model where the response is log-transformed fit_log = lm( log(vat) ~ waist_cm, data=adipose ) # plot the diagnostics for the log-transformed model plot( fit_log ) The diagnostics do not look good after the log transformation, but now the problem is the opposite: a too-heavy lower tail and residual variance decreases as the fitted value increases. Perhaps a better transformation is something in between the raw data and the log transform. Try a square-root transformation. # fit a model where the vat is square root transformed fit_sqrt = lm( sqrt(vat) ~ waist_cm, data=adipose ) # plot the diagnostics for the log-transformed model plot( fit_sqrt ) These look acceptable for real-world data. 19.5.5 Predictions and variability There are two scales of uncertainty for a regression model: uncertainty in the fitted relationship, and the uncertainty of a predicted outcome. The uncertainty of a prediction is always greater because it is calculated by adding the uncertainty of the fitted line to the uncertainty of a single data point around that fitted line. We can illustrate using the example of the model we just created to relate the waist measurement to the square root of vat. For this block of code, we’ll need the mvtnorm library to be loaded. # import mvtnorm. install it if necessary. library( mvtnorm ) # draw the data on the transformed scale with( adipose, plot(waist_cm, sqrt(vat), bty=&#39;n&#39;) ) # plot the fitted regression line abline( coef(fit_sqrt)[[1]], coef(fit_sqrt)[[2]], col=&#39;red&#39; ) # plot 100 samples from the distributon of the regression line. for (i in 1:100) { cc = rmvnorm( n=1, mean=coef(fit_sqrt), sigma=vcov(fit_sqrt) ) abline( cc[[1]], cc[[2]], col=grey(0.8)) } Clearly, the variability of the data points is greater than the variability of the fitted line (that’s why they lie outside the envelope of the fitted lines). We can extract a confidence interval for fitted values or predictions with the predict() function. # draw the data on the transformed scale with( adipose, plot(waist_cm, sqrt(vat), bty=&#39;n&#39;) ) # plot the fitted regression line abline( coef(fit_sqrt)[[1]], coef(fit_sqrt)[[2]], col=&#39;red&#39; ) # define some waist measurements where we&#39;ll construct confidence intervals pred_pts = data.frame( waist_cm = c(70, 85, 110) ) # calculate the 90% CI at each of the pred_pts ff = predict(fit_sqrt, pred_pts, interval=&quot;confidence&quot;, level=0.9) pp = predict(fit_sqrt, pred_pts, interval=&quot;prediction&quot;, level=0.9) # convert the confidence intervals to data.frames ff = as.data.frame(ff) pp = as.data.frame(pp) # add the three confidence intervals to the plots # (offset them a bit for clarity in the plot) for (i in 1:3) { lines( x=rep( pred_pts$waist_cm[[i]] - 0.5, 2), y=c( ff$lwr[[i]], ff$upr[[i]] ), col=&#39;blue&#39;, lwd=2 ) lines( x=rep( pred_pts$waist_cm[[i]] + 0.5, 2), y=c( pp$lwr[[i]], pp$upr[[i]] ), col=&#39;orange&#39;, lwd=2 ) } # add a legend legend(c(&quot;90% CI (fitted values)&quot;, &quot;90% CI (predicted values)&quot;), x=&quot;topleft&quot;, lwd=2, col=c(&quot;blue&quot;, &quot;orange&quot;), bty=&#39;n&#39;) One thing to notice about the confidence intervals is that the interval is smallest (so the precision of the estimation is greatest) at the mean of the predictor variable. This is a general rule of fitting regression. 19.6 Model selection Choosing how to represent your data is a common task in statistics. The most common target is to choose the representation (or model) that does the best job of predicting new data. We set this target because if we have a representation that predicts the future, then we can say it must accurately represent the process that generates the data. 19.7 Cross-validation Unfortunately, we rarely have information about the future, so there isn’t new data to predict. One way to do prediction with the available data is to break it into a training part and a testing part. You make represent the training part with a model, and then use it to predict the left-out testing part. If you then swap the to parts and repeat the process, you’ll have a prediction for every data point. This would be called two-fold cross valdation because the data was broken into two parts. It’s more common to break the data into more than two parts - typically five or ten or one per data point. Then one part is taken as the testing part and all the others go into the training part. The result is five-fold or ten-fold, or leave-one-out cross validation. Let’s use cross-validation to do model selection. The model this time is a representation of the number of births per day in 1978 in the United States. # plot the data gf_point( births ~ day_of_year, color = ~wknd, data=Births78 ) # make models with two through ten knots in the spline for day_of_year bmod2 = lm( births ~ wknd + ns(day_of_year, 2), data=Births78 ) bmod4 = lm( births ~ wknd + ns(day_of_year, 4), data=Births78 ) bmod6 = lm( births ~ wknd + ns(day_of_year, 6), data=Births78 ) bmod8 = lm( births ~ wknd + ns(day_of_year, 8), data=Births78 ) bmod10 = lm( births ~ wknd + ns(day_of_year, 10), data=Births78 ) # plot the 2 and 10 knot models mod_plot(bmod2, births~day_of_year + wknd) + geom_point(mapping=aes(x=day_of_year, y=births, color=wknd), data=Births78) mod_plot(bmod10, births~day_of_year + wknd) + geom_point(mapping=aes(x=day_of_year, y=births, color=wknd), data=Births78) # cross-validate to choose the best model mod_cv( bmod2, bmod4, bmod6, bmod8, bmod10, k=nrow(Births78), ntrials=1 ) ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## Warning in mean.default((actual - model_output)^2, na.rm = TRUE): argument is ## not numeric or logical: returning NA ## mse model ## 1 NA bmod2 ## 2 NA bmod4 ## 3 NA bmod6 ## 4 NA bmod8 ## 5 NA bmod10 # plot the data mod_plot(bmod6, births~day_of_year + wknd) + geom_point(mapping=aes(x=day_of_year, y=births, color=wknd), data=Births78) Cross-validation suggests that six knots is the ideal number, because it has the smallest mean-squared error (mse). The resulting model looks good, too. "],["reshaping-tabular-data.html", "20 Reshaping Tabular Data 20.1 Learning Objectives 20.2 Introduction", " 20 Reshaping Tabular Data 20.1 Learning Objectives After this lesson, you should be able to: Cleaning: Pivot columns in a data set to make it tidy Separate values in a column that contains multiple values per cell Convert columns to appropriate data types 20.2 Introduction This lesson focuses on how to identify untidy tabular data sets and reshape them to be tidy, in the sense descriped in Section 11.2.2. Let’s look at some examples of tidy and untidy data sets. The tidyr package provides examples, and as we’ll see later, it also provides functions to make untidy data sets tidy. As usual, we first need to load the package: # install.packages(&quot;tidyr&quot;) library(tidyr) ## Registered S3 methods overwritten by &#39;tibble&#39;: ## method from ## format.tbl pillar ## print.tbl pillar Let’s start with an example of tidy data. This data set is included in the tidyr package and records the number of tuberculosis cases across several different countries and years: table1 ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 When you first look at a data set, think about what the observations are and what the features are. If the data set comes with documentation, it may help you figure this out. Since this data set is a tidy data set, we already know each row is an observation and each column is a feature. Features in a data set tend to take one of two roles. Some features are identifiers that describe the observed subject. These are usually not what the researcher collecting the data is trying to find out. For example, in the tuberculosis data set, the country and year columns are identifiers. Other features are measurements. These are usually the reason the researcher collected the data. For the tuberculosis data set, the cases and population columns are measurements. Thinking about whether features are identifiers or measurements can be helpful when you need to use tidyr to rearrange a data set. 20.2.1 Columns into Rows Tidy data rule 1 says each observation must have its own row. Here’s a table that breaks rule 1: table4a ## # A tibble: 3 x 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 All of the numbers measure the same thing: cases. To make the data tidy, we must rotate the 1999 and 2000 column names into rows, one for each value in the columns. The new columns are year and cases. This process means less columns (generally) and more rows, so the data set becomes longer. We can use the pivot_longer function to rotate columns into rows. We need to specify: Columns to rotate as cols. Name(s) of new identifier column(s) as names_to. Name(s) of new measuerment column(s) as values_to. Here’s the code: pivot_longer(table4a, -country, names_to = &quot;year&quot;, values_to = &quot;cases&quot;) ## # A tibble: 6 x 3 ## country year cases ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 ## 2 Afghanistan 2000 2666 ## 3 Brazil 1999 37737 ## 4 Brazil 2000 80488 ## 5 China 1999 212258 ## 6 China 2000 213766 20.2.1.1 How to Pivot Longer without tidyr You also can do this without tidyr: Subset columns to separate 1999 and 2000 into two data frames. Add a year column to each. Rename the 1999 and 2000 columns to cases. Stack the two data frames with rbind. # Step 1 df99 = table4a[-3] df00 = table4a[-2] # Step 2 df99$year = &quot;1999&quot; df00$year = &quot;2000&quot; # Step 3 names(df99)[2] = &quot;cases&quot; names(df00)[2] = &quot;cases&quot; # Step 4 rbind(df99, df00) ## # A tibble: 6 x 3 ## country cases year ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Afghanistan 745 1999 ## 2 Brazil 37737 1999 ## 3 China 212258 1999 ## 4 Afghanistan 2666 2000 ## 5 Brazil 80488 2000 ## 6 China 213766 2000 20.2.2 Rows into Columns Tidy data rule 2 says each feature must have its own column. Let’s look at a table that breaks rule 2: table2 ## # A tibble: 12 x 4 ## country year type count ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 cases 745 ## 2 Afghanistan 1999 population 19987071 ## 3 Afghanistan 2000 cases 2666 ## 4 Afghanistan 2000 population 20595360 ## 5 Brazil 1999 cases 37737 ## 6 Brazil 1999 population 172006362 ## 7 Brazil 2000 cases 80488 ## 8 Brazil 2000 population 174504898 ## 9 China 1999 cases 212258 ## 10 China 1999 population 1272915272 ## 11 China 2000 cases 213766 ## 12 China 2000 population 1280428583 Here the count column contains two different features: cases and population. To make the data tidy, we must rotate the count values into columns, one for each type value. New columns are cases and population. This process means less rows and more columns, so the data set becomes wider. We can use pivot_wider to rotate rows into columns. We need to specify: Column names to rotate as names_from. Measurements to rotate as values_from. Here’s the code: pivot_wider(table2, names_from = type, values_from = count) ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 20.2.2.1 How to Pivot Wider without tidyr You can also do this without tidyr: Subset rows to separate cases and population values. Remove the type column from each. Rename the count column to cases and population. Merge the two subsets by matching country and year. # Step 1 cases = table2[table2$type == &quot;cases&quot;, ] pop = table2[table2$type == &quot;population&quot;, ] # Step 2 cases = cases[-3] pop = pop[-3] # Step 3 names(cases)[3] = &quot;cases&quot; names(pop)[3] = &quot;population&quot; # Step 4 tidy = cbind(cases, pop[3]) This code uses the cbind function to merge the two subsets, but it would be better to use the merge function. The cbind function does not use identifier columns to check that the rows in each subset are from the same observations. Run vignette(\"pivot\") for more examples of how to use tidyr. 20.2.3 Separating Values Tidy data rule 3 says each value must have its own cell. Here’s a table that breaks rule 3: table3 ## # A tibble: 6 x 3 ## country year rate ## * &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745/19987071 ## 2 Afghanistan 2000 2666/20595360 ## 3 Brazil 1999 37737/172006362 ## 4 Brazil 2000 80488/174504898 ## 5 China 1999 212258/1272915272 ## 6 China 2000 213766/1280428583 Cells in the rate column contain two values: cases and population. These are two different features, so to make the data set tidy, we need to separate them into two different columns. So how can we separate the rate column? The rate column is a character vector (you can check this with str(table3)), so we can use the string processing functions in the stringr package. In particular, we can use the str_split_fixed function: library(stringr) columns = str_split_fixed(table3$rate, fixed(&quot;/&quot;), 2) Now we have a character matrix where the values are in separate columns. Now we need to combine these with the original data frame. There are several ways to approach this, but to be safe, let’s make a new data frame rather than overwrite the original. First we make a copy of the original: tidy_tb = table3 Next, we need to assign each column in the character matrix to a column in the tidy_tb data frame. Since the columns contain numbers, we can also use the as.numeric function to convert them to the correct data type: tidy_tb$cases = as.numeric(columns[, 1]) tidy_tb$population = as.numeric(columns[, 2]) Extracting values, converting to appropriate data types, and then combining everything into a single data frame is an extremely common pattern in data science. Using stringr functions is the most general way to separate out values in a column, but the tidyr package also provides a function separate specifically for the case we just worked through. Either package is appropriate for solving this problem. "],["data-forensics-and-cleaning-unstructured-data.html", "21 Data Forensics and Cleaning: Unstructured Data 21.1 Preliminaries 21.2 From File Names to Metadata 21.3 Loading a Corpus 21.4 Preprocessing 21.5 Counting Terms 21.6 Text Mining Pipepline 21.7 Document Term Matrix 21.8 Corpus Analytics", " 21 Data Forensics and Cleaning: Unstructured Data 21.1 Preliminaries Lesson Objectives By the end of this lesson, you should: Be able to identify patterns in unstructured data Create metadata about a collection of documents Load and clean a collection of text files into R, which entails: Tokenizing words Determining and applying stop words Normalizing, lemmatizing, and stemming texts Creating a document-term matrix Getting high-level data about text documents (term frequencies, tf–idf scores) Understand how preprocessing steps impact analysis Packages install.packages(c(&quot;tidyverse&quot;, &quot;tokenizers&quot;, &quot;tm&quot;, &quot;cluster&quot;)) 21.2 From File Names to Metadata First, let’s get some information about a collection of files. input_dir &lt;- &quot;./IST8_text_corpus/&quot; fnames &lt;- list.files(input_dir) While we could start analyzing these files immediately, their names contain a lot of metadata, which could be helpful. We’ll need to structure this info first (yes, we’re structuring unstructured data so we can structure more unstructured data—welcome to data forensics!). Mercifully, whoever created these files had a convention in mind for giving them names. We can latch onto the patterns within this convention to make our own representation of the files’ metadata. Here’s the pattern: [LANGUAGE]_[YEAR]_[LASTNAME,FIRSTNAME]_[N OR G].txt Let’s use it to make a data frame. Using stringr in combination with regex patterns will be essential to do so. First, let’s break apart the strings on their underscores and transform that output into a data frame. library(stringr) C19_novels &lt;- str_split_fixed(fnames, &quot;_&quot;, 5) C19_novels &lt;- as.data.frame(C19_novels) This is already pretty close to a good data sheet for us, but we’ll want to refine it a little further. First, let’s name our columns. The letters in the file names are genre tags, which stand for either “gothic” or “not gothic,” so we’ll be sure to record them. colnames(C19_novels) &lt;- c(&quot;lang&quot;, &quot;year&quot;, &quot;author_name&quot;, &quot;title&quot;, &quot;genre&quot;) Now, let’s split author names into “first” and “last” and add them back to the data frame. author_names &lt;- str_split_fixed(C19_novels$author_name, &quot;,&quot;, 2) C19_novels$last_name &lt;- author_names[, 1] C19_novels$first_name &lt;- author_names[, 2] And for good measure, let’s remove the .txt extension in the genre tags and convert those tags to factors. C19_novels$genre &lt;- sapply(C19_novels$genre, function(x) str_remove_all(x, &quot;.txt&quot;)) C19_novels$genre &lt;- as.factor(C19_novels$genre) Finally, we’ll clean up, removing the author_names and lang columns and doing a bit of reordering. (Language could be useful in some instances, but we don’t need it for now, especially because these novels are all in English.) C19_novels &lt;- subset(C19_novels, select= -c(lang, author_name)) C19_novels &lt;- C19_novels[, c(4,5,2,1,3)] C19_novels ## last_name first_name title year genre ## 1 Beckford William Vathek 1786 G ## 2 Radcliffe Ann ASicilianRomance 1790 G ## 3 Radcliffe Ann TheMysteriesofUdolpho 1794 G ## 4 Lewis Matthew TheMonk 1795 G ## 5 Austen Jane SenseandSensibility 1811 N ## 6 Shelley Mary Frankenstein 1818 G ## 7 Scott Walter Ivanhoe 1820 N ## 8 Poe EdgarAllen TheNarrativeofArthurGordonPym 1838 N ## 9 Bronte Emily WutheringHeights 1847 G ## 10 Hawthorne Nathaniel TheHouseoftheSevenGables 1851 N ## 11 Gaskell Elizabeth NorthandSouth 1854 N ## 12 Collins Wilkie TheWomaninWhite 1860 N ## 13 Dickens Charles GreatExpectations 1861 N ## 14 James Henry PortraitofaLady 1881 N ## 15 Stevenson RobertLouis TreasureIsland 1882 N ## 16 Stevenson RobertLouis JekyllandHyde 1886 G ## 17 Wilde Oscar ThePictureofDorianGray 1890 G ## 18 Stoker Bram Dracula 1897 G Nice and tidy! 21.3 Loading a Corpus With our metadata structured, it’s time to load our files. files &lt;- lapply(paste0(input_dir, fnames), readLines) Loading our files like this will create a giant list of vectors, where each vector is a full text file. Those vectors are chunked by paragraph right now, but for our purposes it would be easier if each vector was a single stream of text (like the output of ocr(), if you’ll remember). We can collapse them together with paste(). files &lt;- lapply(files, function(x) paste(x, collapse=&quot; &quot;)) From here, we can wrap these files in a special “corpus” object, which the tm package enables (a corpus is a large collection of texts). A tm corpus works somewhat like a database. It has a section for “content”, which contains text data, as well as various metadata sections, which we can populate with additional information about our texts, if we wished. Taken together, these features make it easy to streamline workflows with text data. To make a corpus with tm, we call the Corpus() function, specifying with VectorSource() (because our texts are vectors): library(tm) corpus &lt;- Corpus(VectorSource(files)) Here’s a high-level glimpse at what’s in this object: corpus ## &lt;&lt;SimpleCorpus&gt;&gt; ## Metadata: corpus specific: 1, document level (indexed): 0 ## Content: documents: 18 Zooming in to metadata about a text in the corpus: corpus[[6]] ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 424017 Not much here so far, but we’ll add more later. Finally, we can get content from a text: str_sub(corpus[[6]]$content, 1, 500) ## [1] &quot;FRANKENSTEIN: OR, THE MODERN PROMETHEUS. BY MARY W. SHELLEY. PREFACE. The event on which this fiction is founded, has been supposed, by Dr. Darwin, and some of the physiological writers of Germany, as not of impossible occurrence. I shall not be supposed as according the remotest degree of serious faith to such an imagination; yet, in assuming it as the basis of a work of fancy, I have not considered myself as merely weaving a series of supernatural terrors. The event on which the interest &quot; In this last view, you can see that the text file is still formatted (at least we didn’t have to OCR it!). This formatting is unwieldy and worse, it makes it so we can’t really access the elements that comprise each novel. We’ll need to do more work to preprocess our texts before we can analyze them. 21.4 Preprocessing Part of preprocessing entails making decisions about the kinds of information we want to know about our data. Knowing what information we want often guides the way we structure data. Put another way: research questions drive preprocessing. 21.4.1 Tokenizing and Bags of Words For example, it’d be helpful to know how many words are in each novel, which might enable us to study patterns and differences between authors’ styles. To get word counts, we need to split the text vectors into individual words. One way to do this would be to first strip out everything in each novel that isn’t an alphabetic character or a space. Let’s grab one text to experiment with. frankenstein &lt;- corpus[[6]]$content frankenstein &lt;- str_replace_all(frankenstein, &quot;[^A-Za-z]&quot;, &quot; &quot;) From here, it would be easy enough to count the words in a novel by splitting its vector on spaces, removing empty elements in the vector, and calling length() on the vector. The end result is what we call a bag of words. frankenstein &lt;- str_split(frankenstein, &quot; &quot;) frankenstein &lt;- lapply(frankenstein, function(x) x[x != &quot;&quot;]) length(frankenstein[[1]]) ## [1] 76015 And here are the first nine words (or “tokens”): frankenstein[[1]][1:9] ## [1] &quot;FRANKENSTEIN&quot; &quot;OR&quot; &quot;THE&quot; &quot;MODERN&quot; &quot;PROMETHEUS&quot; ## [6] &quot;BY&quot; &quot;MARY&quot; &quot;W&quot; &quot;SHELLEY&quot; 21.4.2 Text Normalization While easy, producing our bag of words this way is a bit clunky. And further, this process can’t handle contractions (“I’m”, “don’t”, “that’s”) or differences in capitalization. frankenstein[[1]][188:191] ## [1] &quot;Midsummer&quot; &quot;Night&quot; &quot;s&quot; &quot;Dream&quot; Should be: Midsummer Night&#39;s Dream And &quot;FRANKENSTEIN&quot;, &quot;Frankenstein&quot; Should be: &quot;Frankenstein&quot; Or, even better: frankenstein Typically, when we work with text data we want all of our words to be in the same case because this makes it easier to do things like counting operations. Remember that, to a computer, “Word” and “word” are two separate words, and if we want to count them together, we need to pick one version or the other. Making all words lowercase (even proper nouns) is the standard. Doing this is part of what’s called text normalization. (Other forms of normalization might entail handling orthographic differences between British and American English, like “color” and “colour”.) As for contractions, we have some decisions to make. On the one hand, it’s important to retain as much information as we can about the original text, so keeping “don’t” or “what’s” (which would be “don t” and “what s” in our current method) is important. One way corpus linguists handle these words is to lemmatize them. Lemmatizing involves removing inflectional endings to return words to their base form: car, cars, car’s, cars’ =&gt; car don’t =&gt; do This is a helpful step if what we’re primarily interested in is doing a high- level analysis of semantics. On the other hand, though, many words that feature contractions are high-frequency function words, which don’t have much meaning beyond the immediate context of a sentence or two. Words like “that’s” or “won’t” appear in huge numbers in text data, but they don’t carry much information in and of themselves—it may in fact be the case that we could get rid of them entirely… 21.4.3 Stop Words …and indeed this is the case! When structuring text data to study it at scale, it’s common to remove, or stop out, words that don’t have much meaning. This makes it much easier to identify significant (i.e. unique) features in a text, without having to swim through all the noise of “the” or “that,” which would almost always show up as the highest-occurring words in an analysis. But what words should we remove? Ultimately, this depends on your text data. We can usually assume that function words will be on our list of stop words, but it may be that you’ll have to add or subtract others depending on your data and, of course, your research question. The tm package has a good starting list. Let’s look at the first 100 words. head(stopwords(&quot;SMART&quot;), 100) ## [1] &quot;a&quot; &quot;a&#39;s&quot; &quot;able&quot; &quot;about&quot; &quot;above&quot; ## [6] &quot;according&quot; &quot;accordingly&quot; &quot;across&quot; &quot;actually&quot; &quot;after&quot; ## [11] &quot;afterwards&quot; &quot;again&quot; &quot;against&quot; &quot;ain&#39;t&quot; &quot;all&quot; ## [16] &quot;allow&quot; &quot;allows&quot; &quot;almost&quot; &quot;alone&quot; &quot;along&quot; ## [21] &quot;already&quot; &quot;also&quot; &quot;although&quot; &quot;always&quot; &quot;am&quot; ## [26] &quot;among&quot; &quot;amongst&quot; &quot;an&quot; &quot;and&quot; &quot;another&quot; ## [31] &quot;any&quot; &quot;anybody&quot; &quot;anyhow&quot; &quot;anyone&quot; &quot;anything&quot; ## [36] &quot;anyway&quot; &quot;anyways&quot; &quot;anywhere&quot; &quot;apart&quot; &quot;appear&quot; ## [41] &quot;appreciate&quot; &quot;appropriate&quot; &quot;are&quot; &quot;aren&#39;t&quot; &quot;around&quot; ## [46] &quot;as&quot; &quot;aside&quot; &quot;ask&quot; &quot;asking&quot; &quot;associated&quot; ## [51] &quot;at&quot; &quot;available&quot; &quot;away&quot; &quot;awfully&quot; &quot;b&quot; ## [56] &quot;be&quot; &quot;became&quot; &quot;because&quot; &quot;become&quot; &quot;becomes&quot; ## [61] &quot;becoming&quot; &quot;been&quot; &quot;before&quot; &quot;beforehand&quot; &quot;behind&quot; ## [66] &quot;being&quot; &quot;believe&quot; &quot;below&quot; &quot;beside&quot; &quot;besides&quot; ## [71] &quot;best&quot; &quot;better&quot; &quot;between&quot; &quot;beyond&quot; &quot;both&quot; ## [76] &quot;brief&quot; &quot;but&quot; &quot;by&quot; &quot;c&quot; &quot;c&#39;mon&quot; ## [81] &quot;c&#39;s&quot; &quot;came&quot; &quot;can&quot; &quot;can&#39;t&quot; &quot;cannot&quot; ## [86] &quot;cant&quot; &quot;cause&quot; &quot;causes&quot; &quot;certain&quot; &quot;certainly&quot; ## [91] &quot;changes&quot; &quot;clearly&quot; &quot;co&quot; &quot;com&quot; &quot;come&quot; ## [96] &quot;comes&quot; &quot;concerning&quot; &quot;consequently&quot; &quot;consider&quot; &quot;considering&quot; That looks pretty comprehensive so far, though the only way we’ll know whether it’s a good match for our corpus is to process our corpus with it. At first glance, the extra random letters in this list seem like they could be a big help, on the off chance there’s some noise from OCR. If you look at the first novel in the corpus, for example, there are a bunch of stray p’s, which is likely from a pattern for marking pages (“p. 7”): cat(str_sub(corpus[[1]]$content, 1, 1000)) ## VATHEK; AN ARABIAN TALE, BY WILLIAM BECKFORD, ESQ. p. 7VATHEK. Vathek, ninth Caliph [7a] of the race of the Abassides, was the son of Motassem, and the grandson of Haroun Al Raschid. From an early accession to the throne, and the talents he possessed to adorn it, his subjects were induced to expect that his reign would be long and happy. His figure was pleasing and majestic; but when he was angry, one of his eyes became so terrible [7b] that no person could bear to behold it; and the wretch upon whom it was fixed instantly fell backward, and sometimes expired. For fear, however, of depopulating his dominions, and making his palace desolate, he but rarely gave way to his anger. Being much addicted to women, and the pleasures of the table, he sought by his affability to procure agreeable companions; and he succeeded the better, p. 8as his generosity was unbounded and his indulgences unrestrained; for he was by no means scrupulous: nor did he think, with the Caliph Omar Ben A Our stop word list would take care of this. With it, we could return to our original collection of novels, split them on spaces as before, and filter out everything that’s stored in our stop_list variable. Before we did the filtering, though, we’d need to transform the novels into lowercase (which can be done with R’s tolower() function). 21.4.4 Tokenizers This whole process is ultimately straightforward so far, but it would be nice to collapse all its steps. Luckily, there are packages we can use to streamline our process. tokenizers has functions that split a text vector, turn words into lowercase forms, and remove stop words, all in a few lines of code. Further, we can combine these functions with a special tm_map() function in the tm package, which will globally apply our changes. library(tokenizers) cleaned_corpus &lt;- tm_map(corpus, function(x) tokenize_words(x, stopwords=stopwords(&quot;SMART&quot;), lowercase=TRUE, strip_punct=TRUE, strip_numeric=TRUE)) You may see a “transformation drops documents” warning after this. You can disregard it. It has to do with the way tm references text changes against a corpus’s metadata, which we’ve left blank. We can compare our tokenized output with the text data we had been working with earlier: list(untokenized=frankenstein[[1]][1:9], tokenized=cleaned_corpus[[6]]$content[1:5]) ## $untokenized ## [1] &quot;FRANKENSTEIN&quot; &quot;OR&quot; &quot;THE&quot; &quot;MODERN&quot; &quot;PROMETHEUS&quot; ## [6] &quot;BY&quot; &quot;MARY&quot; &quot;W&quot; &quot;SHELLEY&quot; ## ## $tokenized ## [1] &quot;frankenstein&quot; &quot;modern&quot; &quot;prometheus&quot; &quot;mary&quot; &quot;shelley&quot; From the title alone we can see how much of a difference tokenizing with stop words makes. And while we lose a bit of information by doing this, what we can is a much clearer picture of key words we’d want to further analyze. 21.4.5 Document Chunking and N-grams Finally, it’s possible to change the way we separate out our text data. Instead of tokenizing on words, we could use tokenizers to break apart our texts on paragraphs (tokenize_paragraphs()), sentences (tokenize_sentences), and more. There might be valuable information to be learned about the average sentence length of a novel, for example, so we might chunk it accordingly. We might also want to see whether a text contains repeated phrases, or if two or three words often occur in the same sequence. We could investigate this by adjusting the window around which we tokenize individual words. So far we’ve used the “unigram,” or a single word, as our basic unit of counting, but we could break our texts into “bigrams” (two word phrases), “trigrams” (three word phrases), or, well any sequence of n units. Generally, you’ll see these sequences referred to as n-grams: frankenstein_bigrams &lt;- tokenize_ngrams(corpus[[6]]$content, n=2, stopwords=stopwords(&quot;SMART&quot;)) Here, n=2 sets the n-gram window at two: frankenstein_bigrams[[1]][1:20] ## [1] &quot;frankenstein modern&quot; &quot;modern prometheus&quot; &quot;prometheus mary&quot; ## [4] &quot;mary shelley&quot; &quot;shelley preface&quot; &quot;preface event&quot; ## [7] &quot;event fiction&quot; &quot;fiction founded&quot; &quot;founded supposed&quot; ## [10] &quot;supposed dr&quot; &quot;dr darwin&quot; &quot;darwin physiological&quot; ## [13] &quot;physiological writers&quot; &quot;writers germany&quot; &quot;germany impossible&quot; ## [16] &quot;impossible occurrence&quot; &quot;occurrence supposed&quot; &quot;supposed remotest&quot; ## [19] &quot;remotest degree&quot; &quot;degree faith&quot; Note though that, for this function, we’d need to do some preprocessing on our own to remove numeric characters and punctuation; tokenize_ngrams() won’t do it for us. 21.5 Counting Terms Let’s return to our single word counts. Now that we’ve transformed our novels into bags of single words, we can start with some analysis. Simply counting the number of times a word appears in some data can tell us a lot about a text. The following steps should feel familiar: we did them with OCR. Let’s look at Wuthering Heights, which is our ninth text: library(tidyverse) wuthering_heights &lt;- table(cleaned_corpus[[9]]$content) wuthering_heights &lt;- data.frame(word=names(wuthering_heights), count=as.numeric(wuthering_heights)) wuthering_heights &lt;- arrange(wuthering_heights, desc(count)) head(wuthering_heights, 30) ## word count ## 1 heathcliff 422 ## 2 linton 348 ## 3 catherine 339 ## 4 mr 312 ## 5 master 185 ## 6 hareton 169 ## 7 answered 156 ## 8 till 151 ## 9 house 144 ## 10 door 133 ## 11 mrs 133 ## 12 joseph 130 ## 13 miss 129 ## 14 time 127 ## 15 back 121 ## 16 thought 118 ## 17 cathy 117 ## 18 good 117 ## 19 replied 117 ## 20 earnshaw 116 ## 21 eyes 116 ## 22 cried 114 ## 23 young 107 ## 24 day 106 ## 25 father 106 ## 26 asked 105 ## 27 make 105 ## 28 edgar 104 ## 29 night 104 ## 30 made 102 Looks good! The two main characters in this novel are named Heathcliff and Catherine, so it makes sense that these words would appear a lot. You can see, however, that we might want to fine tune our stop word list so that it removes “mr” and “mrs” from the text. Though again, it depends on our research question. If we’re exploring gender roles in nineteenth-century literature, we’d probably keep those words in. In addition to fine tuning stop words, pausing here at these counts would be a good way to check whether some other form of textual noise is present in your data, which you haven’t yet caught. There’s nothing like that here, but you might imagine how consistent OCR noise could make itself known in this view. 21.5.1 Term Frequency After you’ve done your fine tuning, it would be good to get a term frequency number for each word in this data frame. Raw counts are nice, but expressing those counts in proportion to the total words in a document will tell us more information about a word’s contribution to the document as a whole. We can get term frequencies for our words by dividing a word’s count by document length (which is the sum of all words in the document). wuthering_heights$term_frequency &lt;- sapply(wuthering_heights$count, function(x) (x/sum(wuthering_heights$count))) head(wuthering_heights, 30) ## word count term_frequency ## 1 heathcliff 422 0.009619549 ## 2 linton 348 0.007932709 ## 3 catherine 339 0.007727552 ## 4 mr 312 0.007112084 ## 5 master 185 0.004217101 ## 6 hareton 169 0.003852379 ## 7 answered 156 0.003556042 ## 8 till 151 0.003442066 ## 9 house 144 0.003282500 ## 10 door 133 0.003031754 ## 11 mrs 133 0.003031754 ## 12 joseph 130 0.002963368 ## 13 miss 129 0.002940573 ## 14 time 127 0.002894983 ## 15 back 121 0.002758212 ## 16 thought 118 0.002689827 ## 17 cathy 117 0.002667031 ## 18 good 117 0.002667031 ## 19 replied 117 0.002667031 ## 20 earnshaw 116 0.002644236 ## 21 eyes 116 0.002644236 ## 22 cried 114 0.002598646 ## 23 young 107 0.002439080 ## 24 day 106 0.002416285 ## 25 father 106 0.002416285 ## 26 asked 105 0.002393490 ## 27 make 105 0.002393490 ## 28 edgar 104 0.002370695 ## 29 night 104 0.002370695 ## 30 made 102 0.002325104 21.5.2 Plotting Term Frequency Let’s plot the top 50 words in Wuthering Heights. We’ll call fct_reorder() in the aes() field of ggplot to sort words in the descending order of their term frequency. library(ggplot2) ggplot(data=wuthering_heights[1:50, ], aes(x=fct_reorder(word, -term_frequency), y=term_frequency)) + geom_bar(stat=&quot;identity&quot;) + theme(axis.text.x=element_text(angle=90, vjust=0.5, hjust=1)) + labs(title=&quot;Top 50 words in Wuthering Heights&quot;, x=&quot;Word&quot;, y=&quot;Term Frequency&quot;) This is a good start for creating a high-level view of the novel, but further tuning might be in order. We’ve already mentioned “mrs” and “mr” as two words that we could cut out of the text. Another option would be to collapse these two words together into a base form by stemming them. Though this would overweight their base form (which in this case is “mr”) in terms of term frequency, it would also free up space to see other terms in the document. Other examples of stemming words would be transforming “fishing”, “fished”, and “fisher” all into “fish.” That said, like all preprocessing, lemmatizing words is an interpretive decision, which comes with its own consequences. Maybe it’s okay to transform “mr” and “mrs” into “mr” for some analyses, but it’s also the case that we’d be erasing potentially important gender differences in the text—and would do so by overweighting the masculine form of the word. Regardless of what you decide, it’s important to keep track of these decisions as you make them because they will impact the kinds of claims you make about your data later on. 21.5.3 Comparing Term Frequencies Across Documents Term frequency is helpful if we want to start comparing words across two texts. We can make some comparisons by transforming the above code into a function: term_table &lt;- function(text) { term_tab &lt;- table(text) term_tab &lt;- data.frame(word=names(term_tab), count=as.numeric(term_tab)) term_tab$term_frequency &lt;- sapply(term_tab$count, function(x) (x/sum(term_tab$count))) term_tab &lt;- arrange(term_tab, desc(count)) return(term_tab) } We already have a term table for Wuthering Heights. Let’s make one for Dracula. dracula &lt;- term_table(cleaned_corpus[[18]]$content) head(dracula, 30) ## word count term_frequency ## 1 time 387 0.007280458 ## 2 van 321 0.006038829 ## 3 helsing 299 0.005624953 ## 4 back 261 0.004910076 ## 5 room 231 0.004345699 ## 6 good 225 0.004232824 ## 7 lucy 225 0.004232824 ## 8 man 224 0.004214012 ## 9 dear 219 0.004119949 ## 10 mina 217 0.004082324 ## 11 night 217 0.004082324 ## 12 hand 209 0.003931823 ## 13 face 205 0.003856573 ## 14 door 201 0.003781323 ## 15 made 193 0.003630822 ## 16 poor 192 0.003612010 ## 17 sleep 190 0.003574385 ## 18 eyes 186 0.003499135 ## 19 looked 185 0.003480322 ## 20 friend 183 0.003442697 ## 21 great 182 0.003423884 ## 22 jonathan 182 0.003423884 ## 23 dr 178 0.003348634 ## 24 things 174 0.003273384 ## 25 make 163 0.003066446 ## 26 day 160 0.003010008 ## 27 professor 155 0.002915946 ## 28 count 153 0.002878320 ## 29 found 153 0.002878320 ## 30 thought 153 0.002878320 Now we can compare the relative frequency of a word across two novels: comparison_words &lt;- c(&quot;dark&quot;, &quot;night&quot;, &quot;ominous&quot;) for (i in comparison_words) { wh &lt;- list(wh=subset(wuthering_heights, word==i)) drac &lt;- list(drac=subset(dracula, word==i)) print(wh) print(drac) } ## $wh ## word count term_frequency ## 183 dark 32 0.0007294445 ## ## $drac ## word count term_frequency ## 90 dark 77 0.001448566 ## ## $wh ## word count term_frequency ## 29 night 104 0.002370695 ## ## $drac ## word count term_frequency ## 11 night 217 0.004082324 ## ## $wh ## word count term_frequency ## 7283 ominous 1 2.279514e-05 ## ## $drac ## word count term_frequency ## 7217 ominous 1 1.881255e-05 Not bad! We might be able to make a few generalizations from this, but to say anything definitively, we’ll need to scale our method. Doing so wouldn’t be easy with this setup as it stands now. While it’s true that we could write some functions to roll through these two data frames and systematically compare the words in each, it would take a lot of work to do so. Luckily, the tm package (which we’ve used to make our stop word list) features generalized functions for just this kind of thing. 21.6 Text Mining Pipepline Before going further, we should note that tm has its own functions for preprocessing texts. To send raw files directly through those functions, you’d call tm_map() in conjunction with these functions. You can think of tm_map() as a cognate to the apply() family. corpus_2 &lt;- Corpus(VectorSource(files)) corpus_2 &lt;- tm_map(corpus_2, removeNumbers) corpus_2 &lt;- tm_map(corpus_2, removeWords, stopwords(&quot;SMART&quot;)) corpus_2 &lt;- tm_map(corpus_2, removePunctuation) corpus_2 &lt;- tm_map(corpus_2, stripWhitespace) Note the order of operations here: because our stop words list takes into account punctuated words, like “don’t” or “i’m”, we want to remove stop words before removing punctuation. If we didn’t do this, removeWords() wouldn’t catch the un-punctuated “dont” or “im”. This won’t always be the case, since we can use different stop word lists, which may have a different set of terms, but in this instance, the order in which we preprocess matters. Preparing your text files like this would be fine, and indeed sometimes it’s preferable to sequentially step through each part of the preprocessing workflow. That said, tokenizers manages the order of operations above on its own and its preprocessing functions are generally a bit faster to run (in particular, removeWords() is quite slow in comparison to tokenize_words()). There is, however, one caveat to using tokenizers. It splits documents up to do text cleaning, but other functions in tm require non-split documents. If we use tokenizers, then, we need to do a quick workaround with paste(). cleaned_corpus &lt;- lapply(cleaned_corpus, function(x) paste(x, collapse=&quot; &quot;)) And then reformat that output as a corpus object: cleaned_corpus &lt;- Corpus(VectorSource(cleaned_corpus)) Ultimately, it’s up to you to decide what workflow makes sense. Personally, I (Tyler) like to do exploratory preprocessing steps with tokenizers, often with a sample set of all the documents. Then, once I’ve settled on my stop word list and so forth, I reprocess all my files with the tm-specific functions above. Regardless of what workflow you choose, preprocessing can take a while, so now would be a good place to save your data. That way, you can retrieve your corpus later on. saveRDS(cleaned_corpus, &quot;./data/C19_novels_cleaned.rds&quot;) Loading it back in is straightforward: cleaned_corpus &lt;- readRDS(&quot;./data/C19_novels_cleaned.rds&quot;) 21.7 Document Term Matrix The advantage of using a tm corpus is that it makes comparing data easier. Remember that, in our old workflow, looking at the respective term frequencies in two documents entailed a fair bit of code. And further, we left off before generalizing that code to the corpus as a whole. But what if we wanted to look at a term across multiple documents? To do so, we need to create what’s called a document-term matrix, or DTM. A DTM describes the frequency of terms across an entire corpus (rather than just one document). Rows of the matrix correspond to documents, while columns correspond to the terms. For a given document, we count the number of times that term appears and enter that number in the column in question. We do this even if the count is 0; key to the way a DTM works is that it’s a corpus-wide representation of text data, so it matters if a text does or doesn’t contain a term. Here’s a simple example with three documents: Document 1: “I like cats” Document 2: “I like dogs” Document 3: “I like both cats and dogs” Transforming these into a document-term matrix would yield: n_doc I like both cats and dogs 1 1 1 0 1 0 0 2 1 1 0 0 0 1 3 1 1 1 1 1 1 Representing texts in this way is incredibly useful because it enables us to easily discern similarities and differences in our corpus. For example, we can see that each of the above documents contain the words “I” and “like.” Given that, if we wanted to know what makes documents unique, we can ignore those two words and focus on the rest of the values. Now, imagine doing this for thousands of words! What patterns might emerge? Let’s try it on our corpus. We can transform a tm corpus object into a DTM by calling DocumentTermMatrix(). (Note: this is one of the functions in tm that requires non-split documents, so before you call it make sure you know how you’ve preprocessed your texts!) dtm &lt;- DocumentTermMatrix(cleaned_corpus) This object is quite similar to the one that results from Corpus(): it contains a fair bit of metadata, as well as an all-important “dimnames” field, which records the documents in the matrix and the entire term vocabulary. We access all of this information with the same syntax we use for data frames. Let’s look around a bit and get some high-level info. 21.8 Corpus Analytics Number of columns in the DTM (i.e. the vocabulary size): dtm$ncol ## [1] 34925 Number of rows in the DTM (i.e. the number of documents this matrix represents): dtm$nrow ## [1] 18 Right now, the document names are just a numbers in a vector: dtm$dimnames$Docs ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; &quot;14&quot; &quot;15&quot; ## [16] &quot;16&quot; &quot;17&quot; &quot;18&quot; But they’re ordered according to the sequence in which the corpus was originally created. This means we can use our metadata from way back when to associate a document with its title: dtm$dimnames$Docs &lt;- C19_novels$title dtm$dimnames$Docs ## [1] &quot;Vathek&quot; &quot;ASicilianRomance&quot; ## [3] &quot;TheMysteriesofUdolpho&quot; &quot;TheMonk&quot; ## [5] &quot;SenseandSensibility&quot; &quot;Frankenstein&quot; ## [7] &quot;Ivanhoe&quot; &quot;TheNarrativeofArthurGordonPym&quot; ## [9] &quot;WutheringHeights&quot; &quot;TheHouseoftheSevenGables&quot; ## [11] &quot;NorthandSouth&quot; &quot;TheWomaninWhite&quot; ## [13] &quot;GreatExpectations&quot; &quot;PortraitofaLady&quot; ## [15] &quot;TreasureIsland&quot; &quot;JekyllandHyde&quot; ## [17] &quot;ThePictureofDorianGray&quot; &quot;Dracula&quot; With this information associated, we can use inspect() to get a high-level view of the corpus. inspect(dtm) ## &lt;&lt;DocumentTermMatrix (documents: 18, terms: 34925)&gt;&gt; ## Non-/sparse entries: 145233/483417 ## Sparsity : 77% ## Maximal term length: 19 ## Weighting : term frequency (tf) ## Sample : ## Terms ## Docs back day eyes good great long made man thought time ## Dracula 261 160 186 225 182 147 193 224 153 387 ## GreatExpectations 244 216 180 256 198 173 300 307 238 373 ## Ivanhoe 77 138 100 298 111 154 151 235 46 182 ## NorthandSouth 184 257 197 316 179 211 234 270 332 423 ## PortraitofaLady 210 241 226 520 421 187 381 317 302 339 ## TheHouseoftheSevenGables 79 113 72 100 144 153 144 211 60 113 ## TheMonk 81 106 184 80 66 108 167 95 72 162 ## TheMysteriesofUdolpho 117 167 225 186 164 359 316 213 341 367 ## TheWomaninWhite 417 351 233 235 112 188 244 443 183 706 ## WutheringHeights 121 106 116 117 63 97 102 88 118 127 Of special note here is sparsity. Sparsity measures the amount of 0s in the data. This happens when a document does not contain a term that appears elsewhere in the corpus. In our case, of the 628,650 entries in this matrix, 80% of them are 0. Such is the way of working with DTMs: they’re big, expansive data structures that have a lot of empty space. We can zoom in and filter on term counts with findFreqTerms(). Here are terms that appear more than 1,000 times in the corpus: findFreqTerms(dtm, 1000) ## [1] &quot;answered&quot; &quot;appeared&quot; &quot;asked&quot; &quot;back&quot; &quot;day&quot; &quot;dear&quot; ## [7] &quot;death&quot; &quot;door&quot; &quot;eyes&quot; &quot;face&quot; &quot;father&quot; &quot;felt&quot; ## [13] &quot;found&quot; &quot;friend&quot; &quot;gave&quot; &quot;give&quot; &quot;good&quot; &quot;great&quot; ## [19] &quot;half&quot; &quot;hand&quot; &quot;hands&quot; &quot;head&quot; &quot;hear&quot; &quot;heard&quot; ## [25] &quot;heart&quot; &quot;hope&quot; &quot;kind&quot; &quot;knew&quot; &quot;lady&quot; &quot;leave&quot; ## [31] &quot;left&quot; &quot;life&quot; &quot;light&quot; &quot;long&quot; &quot;looked&quot; &quot;love&quot; ## [37] &quot;made&quot; &quot;make&quot; &quot;man&quot; &quot;men&quot; &quot;mind&quot; &quot;moment&quot; ## [43] &quot;morning&quot; &quot;mother&quot; &quot;night&quot; &quot;part&quot; &quot;passed&quot; &quot;people&quot; ## [49] &quot;person&quot; &quot;place&quot; &quot;poor&quot; &quot;present&quot; &quot;put&quot; &quot;replied&quot; ## [55] &quot;returned&quot; &quot;round&quot; &quot;side&quot; &quot;speak&quot; &quot;stood&quot; &quot;thing&quot; ## [61] &quot;thou&quot; &quot;thought&quot; &quot;till&quot; &quot;time&quot; &quot;told&quot; &quot;turned&quot; ## [67] &quot;voice&quot; &quot;woman&quot; &quot;words&quot; &quot;world&quot; &quot;young&quot; &quot;count&quot; ## [73] &quot;house&quot; &quot;madame&quot; &quot;room&quot; &quot;sir&quot; &quot;emily&quot; &quot;margaret&quot; ## [79] &quot;miss&quot; &quot;mrs&quot; &quot;isabel&quot; Using findAssocs(), we can also track which words rise and fall in usage alongside a given word. (The number in the third argument position of this function is a cutoff for the strength of a correlation.) Here’s “boat”: findAssocs(dtm, &quot;boat&quot;, .85) ## $boat ## thumping scoundrels midday direction ## 0.94 0.88 0.87 0.85 Here’s “writing” (there are a lot of terms, so we’ll limit to 15): writing &lt;- findAssocs(dtm, &quot;writing&quot;, .85) writing[[1]][1:15] ## letter copy disposal inquiries bedrooms hindrance ## 0.99 0.97 0.97 0.97 0.97 0.97 ## messages certificate distrust plainly drawings anonymous ## 0.97 0.97 0.96 0.96 0.96 0.96 ## ladyship plantation lodgings ## 0.96 0.96 0.96 21.8.1 Corpus Term Counts From here, it would be useful to get a full count of all the terms in the corpus. We can transform the DTM into a matrix and then a data frame. term_counts &lt;- as.matrix(dtm) term_counts &lt;- data.frame(sort(colSums(term_counts), decreasing=TRUE)) term_counts &lt;- cbind(newColName=rownames(term_counts), term_counts) colnames(term_counts) &lt;- c(&quot;term&quot;, &quot;count&quot;) As before, let’s plot the top 50 terms in these counts, but this time, they will cover the entire corpus: ggplot(data=term_counts[1:50, ], aes(x=fct_reorder(term, -count), y=count)) + geom_bar(stat=&quot;identity&quot;) + theme(axis.text.x=element_text(angle=90, vjust=0.5, hjust=1)) + labs(title=&quot;Top 50 words in 18 Nineteenth-Century Novels&quot;, x=&quot;Word&quot;, y=&quot;Count&quot;) This looks good, though the words here are all pretty common. In fact, many of them are simply the most common words in the English language. “Time” is the 64th-most frequent word in English; “make” is the 50th. As it stands, then, this graph doesn’t tell us very much about the specificity of our particular collection of texts; if we ran the same process on English novels from the twentieth century, we’d probably produce very similar output. 21.8.2 tf–idf Scores Given this, if we want to know what makes our corpus special, we need a measure of uniqueness for the terms it contains. One of the most common ways to do this is to get what’s called a tf–idf score (short for “term frequency—inverse document frequency”) for each term in our corpus. tf–idf is a weighting method. It increases proportionally to the number of times a word appears in a document but is importantly offset by the number of documents in the corpus that contain this term. This offset adjusts for common words across a corpus, pushing their scores down while boosting the scores of rarer terms in the corpus. Inverse document frequency can be expressed as: \\[\\begin{align*} idf_i = log(\\frac{n}{df_i}) \\end{align*}\\] Where \\(idf_i\\) is the idf score for term \\(i\\), \\(df_i\\) is the number of documents that contain \\(i\\), and \\(n\\) is the total number of documents. A tf-idf score can be calculated by the following: \\[\\begin{align*} w_i,_j = tf_i,_j \\times idf_i \\end{align*}\\] Where \\(w_i,_j\\) is the tf–idf score of term \\(i\\) in document \\(j\\), \\(tf_i,_j\\) is the term frequency for \\(i\\) in \\(j\\), and \\(idf_i\\) is the inverse document score. While it’s good to know the underlying equations here, you won’t be tested on the math specifically. And as it happens, tm has a way to perform the above math for each term in a corpus. We can implement tf–idf scores when making a document-term matrix: dtm_tfidf &lt;- DocumentTermMatrix(cleaned_corpus, control=list(weighting=weightTfIdf)) dtm_tfidf$dimnames$Docs &lt;- C19_novels$title To see what difference it makes, let’s plot the top terms in our corpus using their tf–idf scores. tfidf_counts &lt;- as.matrix(dtm_tfidf) tfidf_counts &lt;- data.frame(sort(colSums(tfidf_counts), decreasing=TRUE)) tfidf_counts &lt;- cbind(newColName=rownames(tfidf_counts), tfidf_counts) colnames(tfidf_counts) &lt;- c(&quot;term&quot;, &quot;tfidf&quot;) ggplot(data=tfidf_counts[1:50, ], aes(x=fct_reorder(term, -tfidf), y=tfidf)) + geom_bar(stat=&quot;identity&quot;) + theme(axis.text.x=element_text(angle=90, vjust=0.5, hjust=1)) + labs(title=&quot;Words with the 50-highest tf--idf scores in 18 Nineteenth-Century Novels&quot;, x=&quot;Word&quot;, y=&quot;TF-IDF&quot;) Lots of names! That makes sense: heavily weighted terms in these novels are going to be terms that are unique to each text. Main characters’ names are used a lot in novels, and the main character names in these novels are all unique. To see in more concrete way how tf–idf scores might make a difference in the way we analyze our corpus, we’ll do two last things. First, we’ll look again at term correlations, using the same words from above with findAssoscs(), but this time we’ll use tf–idf scores. Here’s “boat”: findAssocs(dtm_tfidf, &quot;boat&quot;, .85) ## $boat ## thumping shore bucket cables doo geese ## 0.95 0.93 0.92 0.92 0.92 0.92 ## pickled sea rudder gunwale scoundrels boats ## 0.92 0.91 0.91 0.91 0.91 0.90 ## keel sailed crew baffling biscuit bowsprit ## 0.90 0.89 0.89 0.89 0.89 0.89 ## hauling muskets ripped splash anchor oar ## 0.89 0.89 0.89 0.89 0.88 0.88 ## rattling sandy cook patted shipped beach ## 0.88 0.88 0.88 0.88 0.88 0.87 ## pistols seamen tobacco lee bulwarks hauled ## 0.87 0.87 0.87 0.87 0.87 0.87 ## inkling musket navigation rags steering island ## 0.87 0.87 0.87 0.87 0.87 0.86 ## bottle tumbled avast belay bilge broadside ## 0.86 0.86 0.86 0.86 0.86 0.86 ## cruising cutlasses diagonal furtively headway jupiter ## 0.86 0.86 0.86 0.86 0.86 0.86 ## mainland marlin midday monthly mutineers outnumbered ## 0.86 0.86 0.86 0.86 0.86 0.86 ## plumped riggers schooner schooners seaworthy swamping ## 0.86 0.86 0.86 0.86 0.86 0.86 ## tide&#39;s tiller tonnage towed yawed sail ## 0.86 0.86 0.86 0.86 0.86 0.85 ## ship tap loading sails aft berths ## 0.85 0.85 0.85 0.85 0.85 0.85 ## pinned ## 0.85 Here’s “writing”: findAssocs(dtm_tfidf, &quot;writing&quot;, .85) ## $writing ## hindrance messages disposal inquiries bedrooms ## 0.92 0.91 0.90 0.90 0.89 ## ladyship copy lodgings london unforeseen ## 0.88 0.87 0.87 0.87 0.87 ## drawings plantation explanations certificate dears ## 0.86 0.86 0.86 0.86 0.86 ## neighbourhood allowances ## 0.85 0.85 The semantics of these results have changed. For “boats”, we get much more terms related to sefaring. Most probably this is because only a few novels talk about boats so these terms correlate highly with one another. For “writing”, we’ve interestingly lost a lot of the words associated with writing in a strict sense (“copy”, “message”) but we’ve gained instead a list of terms that seem to situate us in where writing takes place in these novels, or what characters write about. So far though this is speculation; we’d have to look into this further to see whether the hypothesis holds. Finally, we can disaggregate our giant term count graph from above to focus more closely on the uniqueness of individual novels in our corpus. First, we’ll make a data frame from our tf–idf DTM. We’ll transpose the DTM so the documents are our variables (columns) and the corpus vocabulary terms are our observations (or rows). Don’t forget the t! tfidf_df &lt;- as.matrix(dtm_tfidf) tfidf_df &lt;- as.data.frame(t(tfidf_df)) colnames(tfidf_df) &lt;- C19_novels$title 21.8.3 Unique Terms in a Document With this data frame made, we can order our rows by the highest value for a given column. In other words, we can find out not only the top terms for a novel, but the top most unique terms in that novel. Here’s Dracula: rownames(tfidf_df[order(tfidf_df$Dracula, decreasing=TRUE)[1:50],]) ## [1] &quot;helsing&quot; &quot;mina&quot; &quot;lucy&quot; &quot;jonathan&quot; &quot;van&quot; ## [6] &quot;harker&quot; &quot;godalming&quot; &quot;quincey&quot; &quot;seward&quot; &quot;professor&quot; ## [11] &quot;morris&quot; &quot;lucy&#39;s&quot; &quot;harker&#39;s&quot; &quot;diary&quot; &quot;seward&#39;s&quot; ## [16] &quot;arthur&quot; &quot;renfield&quot; &quot;westenra&quot; &quot;whilst&quot; &quot;undead&quot; ## [21] &quot;tonight&quot; &quot;whitby&quot; &quot;dracula&quot; &quot;varna&quot; &quot;carfax&quot; ## [26] &quot;journal&quot; &quot;helsing&#39;s&quot; &quot;count&quot; &quot;count&#39;s&quot; &quot;hawkins&quot; ## [31] &quot;madam&quot; &quot;galatz&quot; &quot;jonathan&#39;s&quot; &quot;mina&#39;s&quot; &quot;pier&quot; ## [36] &quot;wolves&quot; &quot;tomorrow&quot; &quot;czarina&quot; &quot;telegram&quot; &quot;boxes&quot; ## [41] &quot;today&quot; &quot;holmwood&quot; &quot;hypnotic&quot; &quot;garlic&quot; &quot;vampire&quot; ## [46] &quot;phonograph&quot; &quot;transylvania&quot; &quot;cliff&quot; &quot;piccadilly&quot; &quot;slovaks&quot; Note here that some contractions have slipped through. Lemmatizing would take care of this, though we could also go back to the corpus object and add in another step with tm_map() and then make another DTM: cleaned_corpus &lt;- tm_map(cleaned_corpus, function(x) str_remove_all(x, &quot;\\\\&#39;s&quot;, &quot; &quot;)) We won’t bother to do this whole process now, but it’s a good example of how iterative the preprocessing workflow is. Here’s Frankenstein: rownames(tfidf_df[order(tfidf_df$Frankenstein, decreasing=TRUE)[1:50],]) ## [1] &quot;clerval&quot; &quot;justine&quot; &quot;elizabeth&quot; &quot;felix&quot; &quot;geneva&quot; ## [6] &quot;frankenstein&quot; &quot;safie&quot; &quot;cottagers&quot; &quot;dæmon&quot; &quot;ingolstadt&quot; ## [11] &quot;kirwin&quot; &quot;agatha&quot; &quot;victor&quot; &quot;ernest&quot; &quot;mont&quot; ## [16] &quot;krempe&quot; &quot;lacey&quot; &quot;waldman&quot; &quot;agrippa&quot; &quot;walton&quot; ## [21] &quot;mountains&quot; &quot;creator&quot; &quot;cottage&quot; &quot;sledge&quot; &quot;hovel&quot; ## [26] &quot;switzerland&quot; &quot;ice&quot; &quot;beaufort&quot; &quot;cornelius&quot; &quot;william&quot; ## [31] &quot;protectors&quot; &quot;moritz&quot; &quot;henry&quot; &quot;labours&quot; &quot;chamounix&quot; ## [36] &quot;glacier&quot; &quot;jura&quot; &quot;blanc&quot; &quot;endeavoured&quot; &quot;lake&quot; ## [41] &quot;leghorn&quot; &quot;monster&quot; &quot;rhine&quot; &quot;magistrate&quot; &quot;belrive&quot; ## [46] &quot;lavenza&quot; &quot;salêve&quot; &quot;saville&quot; &quot;strasburgh&quot; &quot;werter&quot; And here’s Sense and Sensibility: rownames(tfidf_df[order(tfidf_df$SenseandSensibility, decreasing=TRUE)[1:50],]) ## [1] &quot;elinor&quot; &quot;marianne&quot; &quot;dashwood&quot; &quot;jennings&quot; &quot;willoughby&quot; ## [6] &quot;lucy&quot; &quot;brandon&quot; &quot;barton&quot; &quot;ferrars&quot; &quot;colonel&quot; ## [11] &quot;mrs&quot; &quot;marianne&#39;s&quot; &quot;edward&quot; &quot;middleton&quot; &quot;elinor&#39;s&quot; ## [16] &quot;norland&quot; &quot;palmer&quot; &quot;steele&quot; &quot;dashwoods&quot; &quot;jennings&#39;s&quot; ## [21] &quot;willoughby&#39;s&quot; &quot;edward&#39;s&quot; &quot;delaford&quot; &quot;steeles&quot; &quot;cleveland&quot; ## [26] &quot;mama&quot; &quot;dashwood&#39;s&quot; &quot;lucy&#39;s&quot; &quot;brandon&#39;s&quot; &quot;fanny&quot; ## [31] &quot;allenham&quot; &quot;middletons&quot; &quot;devonshire&quot; &quot;combe&quot; &quot;ferrars&#39;s&quot; ## [36] &quot;sister&quot; &quot;morton&quot; &quot;miss&quot; &quot;margaret&quot; &quot;park&quot; ## [41] &quot;charlotte&quot; &quot;exeter&quot; &quot;magna&quot; &quot;berkeley&quot; &quot;harley&quot; ## [46] &quot;john&quot; &quot;middleton&#39;s&quot; &quot;parsonage&quot; &quot;beaux&quot; &quot;behaviour&quot; Names still rank high, but we can see in these results other words that indeed seem to be particular to each novel. With this data, we now have a sense of what makes each document unique in its relationship with all other documents in a corpus "],["data-forensics-and-cleaning-geospatial-data.html", "22 Data Forensics and Cleaning: Geospatial Data 22.1 Learning Objectives 22.2 What is Geospatial Data? 22.3 Geospatial Data Models 22.4 Data Structures Applied to Geospatial Data 22.5 Cleaning Geospatial Data 22.6 Conclusions 22.7 Optional Further Reading", " 22 Data Forensics and Cleaning: Geospatial Data 22.1 Learning Objectives This lecture is designed to introduce you to the basics of geospatial data. By the end of this lecture you will understand the main components of geospatial data are locations, attributes, and a coordinate reference system understand how geospatial data can be represented with different data models understand that the data structures we were already familiar with can be modified to contain spatial data. learn some common processes for cleaning our geospatial data 22.2 What is Geospatial Data? Geospatial data (also known as spatial data, GIS data, and other names) is information that can be attributed to a real-world location or can relate to each other in space. Technically, “geospatial” refers to locations on Earth, while “spatial” can be locations anywhere, including other planets or even ficticious places (like J.R.R. Tolkien’s hand-drawn maps for his novels), but quite often the terms are used interchangably. You use geospatial data every day on your smart phone through spatially-enabled apps like Google Maps, food delivery apps, fitness trackers, weather, or games like Pokemon Go. (geo)Spatial Data = Attributes + Locations Location = Coordinate Reference System (CRS) + Coordinates So… (geo)Spatial Data = Attributes + Coordinate Reference System (CRS) + Coordinates 22.2.1 Attributes Attributes are pieces of information about a location. For example, if I’m mapping gas stations, my attributes might be something like the price of gas, the address of the station, and the company that runs it (Shell, Arco, etc.). This isn’t the same thing as metadata, which is information about the entire dataset such as who made the data, when they made it, and how the data was created. 22.2.2 Coordinate Reference System The earth is generally round. Maps are generally flat, with a few exceptions. If you were to try to flatten out the earth, you would create some fairly major distortions. Next time you eat an orange or a tangerine, try taking off the peel and then try to create a flat solid sheet of peel from it. You’ll end up needing to cut it or smash it to get a flat surface. The same things happens to geospatial data when we try to translate it from a round globe to a flat map. But, there are ways to minimize distortions. A coordinate reference system (sometimes called a projection) is a set of mathematical formulas that translate measurements on a round globe to a flat piece of paper. The coordinate reference system also specifies the linear units of measure (i.e. feet, meters, decimal degrees, or something else) and a set of reference lines. For our purposes, we can think of coordinate reference systems coming in two flavors. One is geographic coordinate systems. For simplicity’s sake, we can think of these as coordinate reference systems that apply to latitude and longitude coordinates. Projected coordinate systems translate latitude and longitude coordinates into linear units from a specified baseline and aim to reduce some aspect of the distortion introduced in the round to flat translation. (I am very much simplifying this concept so we can learn the basics without getting overwhelmed.) To work with more than one digital spatial dataset, the coordinate reference systems must match. If they don’t match, you can transform your data into a different coordinate reference system. 22.2.3 Coordinates Coordinates are given in the distance (in the linear units specified in the CRS) from the baselines (specified in the CRS). Coordinates can be plotted just like coordinates on a graph (cartesian coordinate system). Sometimes we refer to these as X and Y, just like a graph, but sometimes you’ll hear people refer to the cardinal directions (north, south, east, and west). Let’s take a moment to talk about latitude and longitude. You’re probably at least a little familiar with latitude (Y) and longitude (X), but this is a special case that’s more complex than we probably initially realize. Latitude and longitude are angular measurements (with units in degrees) from a set of baselines - usually the Equator and the Greenwhich Meridian. We can plot latitude and longitude on a cartesian coordinate system, but this introduces major distortions increasing as you approach the poles. You never want to use straight latitude/longitude coordinates (commonly in North America, you’ll see data in the geographic coordinate reference system called WGS84) for an analysis. Always translate them into a projected coordinate system first. In addition, because the units are degrees, they are rather hard for us to interpret when we make measurements. How many degrees is it from the UC Davis campus to your appartment? It’s probably a very small fraction of a degree. Area measurements make even less sense. (What is a square degree and what does that look like?) Latitude/longitude coordinates are a great starting place, we just need to handle them correctly. 22.3 Geospatial Data Models Now we have an idea of what makes data spatial, but what does spatial data look like in a computer? There are two common data models for geospatial data: Vector and Raster. Data Model Geometry Example Vector Points Very small things, like cities at world scale . Lines Linear things, like roads at city scale . Polygons Larger things that take up space, like parks at a city scale Raster Grid Digital Photo A visual table of raster vs. vector data as continuous and discrete data. 22.3.1 Vector Data Vector data represents discrete objects in the real world with points, lines, and polygons in the dataset. If you were to draw a map to your house for a friend, you would typically use vector data - roads would be lines, a shopping center included as an important landmark might be a rectangle of sorts, and your house might be a point (perhaps represented by a star or a house icon). For this lecture, we will focus on point data. 22.3.2 Raster Data Raster data represents continuous fields or discrete objects on a grid, storing measurements or category codes in each cell of the grid. Digital photos are raster data you are already familiar with. If you zoom in far enough on a digital photo, you’ll see that photo is made up of pixels, which appear as colored squares. Pixels are cells in a regular grid and each contains the digital code that corresponds to the color that should be displayed there. Satellite images (like you see in Google Maps) are a very similar situation. 22.4 Data Structures Applied to Geospatial Data In the lecture on Data Structures (week 6), you learned that data can be structured in a number of ways, including tabular, tree (XML and JSON), relational databases, and non-hierarchical data. All of these structures can include spatial information. Data Structure Example File Type How it’s implemented Tabular CSV One or more columns hold spatial data (like Latitude &amp; Longitude) Tree geoJSON Tags in the structure indicate spatial information like geometry type and vertex locations Relational Database PostGIS or Spatialite One column holds the “geometry” information (vertexes &amp; CRS) Non-Hierarchical Relational Data Spatial Graph Databases Nodes have locations associated with them, edges represent flow (think: transportation networks or stream networks) For visualization purposes, geospatial software typically show all of these data structures as a map where each entity is linked with a table of the attribute data - one row of data in the table relates to one entity on the map. So regardless of the underlying data structure, you can think of these as interactive maps like you find on Google Maps. 22.5 Cleaning Geospatial Data What Can Go Wrong? Location data isn’t usable Location data is incorrect Attribute data is incorrect Coordinate Reference System (CRS) is improperly defined 22.5.1 Example Data The dataset we’ll be working with as an example contains locations and attributes about lake monsters. Lake monsters are fictional creatures like sea monsters, but they live in lakes and not the ocean. The most famous lake monster is probably Nessie, who lives in Loch Ness. The dataset we’re working with today is the early stages of a now much cleaner dataset. This data came from a Wikipedia page and the locations were geocoded (a process that matches text locations with real-world locations). We’ll walk through some common processes and challenges with point data stored in a csv file. 22.5.2 Making Location Data Usable Someone sends you a CSV file. At first glance, nothing looks amiss. There is a column for latitude and another for longitude, but how is it formatted? It’s degrees-minutes-seconds (DMS)! DMS looks like this: 34° 36’ 31.774” - 34 degrees, 36 minutes, 31.447 seconds - and sometimes people put in the symbols for degree (°), minutes (’), and seconds (“), and sometimes not. The computer can’t read this format, especially the symbols. It has to be converted to decimal degrees (DD), which looks like this: 34.60882611 To convert it, we need to know that there are 60 minutes in a degree and 60 seconds in a minute. Decimal Degrees = Degrees + (Minutes/60) + (Seconds/3600) 34.60882611 = 34 + (36/60) + (31.447/3600) First, we need to load the libraries we’ll need and then load the data. # Load Libraries ---------------------------------------------------------- library(&quot;sf&quot;) ## Registered S3 methods overwritten by &#39;tibble&#39;: ## method from ## format.tbl pillar ## print.tbl pillar ## Linking to GEOS 3.9.1, GDAL 3.4.0, PROJ 8.2.0; sf_use_s2() is TRUE library(&quot;mapview&quot;) ## Warning: multiple methods tables found for &#39;direction&#39; ## Warning: multiple methods tables found for &#39;gridDistance&#39; ## Warning: multiple methods tables found for &#39;crop&#39; ## Warning: multiple methods tables found for &#39;extend&#39; library(&quot;gdtools&quot;) #makes the display... dependency of mapview but it&#39;s not loading library(&quot;leafem&quot;) #makes the labels work... dependency of mapview but it&#39;s not loading library(&quot;leaflet&quot;) # Read Data --------------------------------------------------------------- monsters.raw&lt;-read.csv(&quot;data/lake_monsters.csv&quot;, stringsAsFactors = FALSE, encoding = &quot;utf-8&quot;) #Explore the data head(monsters.raw) ## fid field_1 Lake Area Country Continent ## 1 1 1 Arenal Lagoon Alajuela Costa Rica North America ## 2 2 2 Bangweulu Swamp Zambia Africa ## 3 3 3 Bassenthwaite Lake England United Kingdom Europe ## 4 4 4 Bear Lake Idaho, Utah USA North America ## 5 5 5 Brosno Lake Tver Oblast Russia Europe ## 6 6 6 Bueng Khong Long Bueng Kan Thailand Asia ## Name lat lon lat_dms ## 1 unnamed 10.49143 -84.851696 10°29?29.1304? ## 2 Nsanga -11.14741 29.784582 -11°8?50.6760? ## 3 Eachy 54.65279 -3.213612 54°39?10.0359? ## 4 Bear Lake Monster, Isabella 42.21721 -111.319881 42°13?1.9643? ## 5 Brosno Dragon 56.82407 31.914652 56°49?26.6520? ## 6 Phaya Naga 18.02363 104.014360 18°1?25.0676? ## lon_dms coords_3395 lon_3395 ## 1 -84°51?6.1069? Point (-9445647.63285386 1166706.48735204) -9445647.6 ## 2 29°47?4.4935? Point (3315604.44829715 -1240572.14607131) 3315604.4 ## 3 -3°12?49.0016? Point (-357737.60213262 7259890.14217639) -357737.6 ## 4 -111°19?11.5709? Point (-12392072.44582391 5164853.16566773) -12392072.4 ## 5 31°54?52.7472? Point (3552722.80948453 7688451.06249625) 3552722.8 ## 6 104°0?51.6974? Point (11578825.63491604 2027100.69217741) 11578825.6 ## lat_3395 ## 1 1166706 ## 2 -1240572 ## 3 7259890 ## 4 5164853 ## 5 7688451 ## 6 2027101 Next, we need to write some functions to deal with our specific DMS data and how its formatted. # Functions --------------------------------------------------------------- #This function splits up the DMS column into three columns - D, M, &amp; S split.dms&lt;-function(dms.column){ #separate the pieces of the DMS column variable&lt;-do.call(rbind, args=c(strsplit(dms.column, &#39;[°?]+&#39;))) #makes a matrix of characters mode(variable)=&quot;numeric&quot; #assigning the data type to numeric instead of character dms.split&lt;-as.data.frame(variable) split.string&lt;-strsplit(dms.column, &#39;[°?]+&#39;) # naming the columns names(dms.split)&lt;-c(&quot;D&quot;, &quot;M&quot;, &quot;S&quot;) return(dms.split) } # this function coverts a 3 column dataframe of DMS to DD, like the data created by split.dms() decimaldegrees&lt;-function(dms.df){ dd&lt;-data.frame() for (i in 1:dim(dms.df)[1]){ if (dms.df[i, 1]&gt;0){ #Decimal Degrees = Degrees + (Minutes/60) + (Seconds/3600) dd.row&lt;-dms.df[i,1]+(dms.df[i,2]/60)+(dms.df[i,3]/3600) dd&lt;-rbind(dd, dd.row) } else{ #-Decimal Degrees = Degrees - (Minutes/60) - (Seconds/3600) dd.row&lt;-dms.df[i,1]-(dms.df[i,2]/60)-(dms.df[i,3]/3600) dd&lt;-rbind(dd, dd.row) } } return(dd) } Finally, we can process our DMS data to convert it to Decimal Degreess (DD). # Process Latitude dms.split&lt;-split.dms(monsters.raw$lat_dms) dd&lt;-decimaldegrees(dms.split) monsters.df&lt;-cbind(monsters.raw, dd) names(monsters.df)[15]&lt;-&quot;lat_dd&quot; # Process Longitude dms.split&lt;-split.dms(monsters.raw$lon_dms) dd&lt;-decimaldegrees(dms.split) monsters.df&lt;-cbind(monsters.df, dd) names(monsters.df)[16]&lt;-&quot;lon_dd&quot; # Look at the data head(monsters.df) ## fid field_1 Lake Area Country Continent ## 1 1 1 Arenal Lagoon Alajuela Costa Rica North America ## 2 2 2 Bangweulu Swamp Zambia Africa ## 3 3 3 Bassenthwaite Lake England United Kingdom Europe ## 4 4 4 Bear Lake Idaho, Utah USA North America ## 5 5 5 Brosno Lake Tver Oblast Russia Europe ## 6 6 6 Bueng Khong Long Bueng Kan Thailand Asia ## Name lat lon lat_dms ## 1 unnamed 10.49143 -84.851696 10°29?29.1304? ## 2 Nsanga -11.14741 29.784582 -11°8?50.6760? ## 3 Eachy 54.65279 -3.213612 54°39?10.0359? ## 4 Bear Lake Monster, Isabella 42.21721 -111.319881 42°13?1.9643? ## 5 Brosno Dragon 56.82407 31.914652 56°49?26.6520? ## 6 Phaya Naga 18.02363 104.014360 18°1?25.0676? ## lon_dms coords_3395 lon_3395 ## 1 -84°51?6.1069? Point (-9445647.63285386 1166706.48735204) -9445647.6 ## 2 29°47?4.4935? Point (3315604.44829715 -1240572.14607131) 3315604.4 ## 3 -3°12?49.0016? Point (-357737.60213262 7259890.14217639) -357737.6 ## 4 -111°19?11.5709? Point (-12392072.44582391 5164853.16566773) -12392072.4 ## 5 31°54?52.7472? Point (3552722.80948453 7688451.06249625) 3552722.8 ## 6 104°0?51.6974? Point (11578825.63491604 2027100.69217741) 11578825.6 ## lat_3395 lat_dd lon_dd ## 1 1166706 10.49143 -84.851696 ## 2 -1240572 -11.14741 29.784582 ## 3 7259890 54.65279 -3.213612 ## 4 5164853 42.21721 -111.319881 ## 5 7688451 56.82407 31.914652 ## 6 2027101 18.02363 104.014360 Another common issue with point data is that the latitude and longitude are not in any form of degrees, but instead are in a projected coordinate system with linear units (usually feet or meters). If the data doesn’t come with metadata, you may be left guessing which coordinate system it is in. With experience, you’ll get better at guessing, but sometimes the data is not usable. Our monsters dataset has latitude and longitude in the World Mercator (EPSG: 3395) projection as well. Let’s briefly look at that here, but we’ll play with that more later in this document. monsters.df[1:10,13:16] ## lon_3395 lat_3395 lat_dd lon_dd ## 1 -9445647.6 1166706 10.49143 -84.851696 ## 2 3315604.4 -1240572 -11.14741 29.784582 ## 3 -357737.6 7259890 54.65279 -3.213612 ## 4 -12392072.4 5164853 42.21721 -111.319881 ## 5 3552722.8 7688451 56.82407 31.914652 ## 6 11578825.6 2027101 18.02363 104.014360 ## 7 -7845463.9 5433619 43.98618 -70.477001 ## 8 -12704546.6 6054836 47.87777 -114.126884 ## 9 -9467608.2 5128572 41.97447 -85.048971 ## 10 -12525669.2 5011829 41.18707 -112.520000 Note that data preparation and cleaning is the vast majority of the work for all data, not just spatial data. All of the code we just looked at was just to get the data in a usable format. We’ll convert it to a spatial data type and map it in the next section. 22.5.3 Cleaning Location Data Sometimes, the locations in your dataset are incorrect. This can happen for a number of reasons. For example, it’s fairly common for data to get truncated or rounded if you open a CSV in Excel. Removing decimal places from coordinate data loses precision. People often swap their latitude and longitude columns as well, which make data show up in the wrong cartesian coordinate, for example, (-119, 34) is a verry different location than (34, -119)… -119 is actually out of the range of latitude data and will often break your code. Another common source of error is in the way the data was made. If data is produced by geocoding, turning an address or place name into a coordinate, the location may have been matched badly. If the data was made by an analysis process, an unexpected aspect of the data could cause problems, like a one-to-many join when you thought you had a one-to-one join in a database. Regardless of how the errors came about, how do we find incorrect locations? Start by mapping the data and see where it lands. Is it where you expect the data to be? Sometimes you can’t tell it’s wrong because the data looks normal. # Convert the monsters dataframe into an SF (spatial) object # Note: x is the dataframe, not longitude. # Coordinate Reference System (CRS) - we&#39;re using lat/long here so we need WGS84 which is EPSG code 4326 - we just need to tell R what the CRS is, we don&#39;t change it this way. If we want to change it, we need to use st_transform(). monsters.sf&lt;-st_as_sf(x=monsters.df, coords=c(&quot;lon_dd&quot;, &quot;lat_dd&quot;), crs = 4326) # Notice we added a geometry column! names(monsters.sf) ## [1] &quot;fid&quot; &quot;field_1&quot; &quot;Lake&quot; &quot;Area&quot; &quot;Country&quot; ## [6] &quot;Continent&quot; &quot;Name&quot; &quot;lat&quot; &quot;lon&quot; &quot;lat_dms&quot; ## [11] &quot;lon_dms&quot; &quot;coords_3395&quot; &quot;lon_3395&quot; &quot;lat_3395&quot; &quot;geometry&quot; # Plot a map mapview(monsters.sf) This is a screen capture of the output for the mapview function. Running this code in a regular R session (i.e. not in knitr or bookdown like we need to for this reader) will make an interactive map. In the interactive version of this map, you can pan and zoom to different areas to see more detail. Clicking on a point will open a popup with attribute information. First impressions: This map looks good! The points are all on land masses, none in the ocean. Let’s see if they are on the correct continent… mapview(monsters.sf, zcol=&quot;Continent&quot;, legend= TRUE) Map of monster locations by continent It’s hard to see, but there’s a point in Michigan that’s the wrong color for North America! Map of monster locations by continent zoomed in to the Great Lakes Whoops! Lakes of Killarney isn’t in Michigan! That point should be in Ireland! If we zoom in, we can see why the geocoder got confused. The lake names are very similar. 22.5.4 Cleaning Attribute Data Attribute data can be proofed in much the same way tabular data can be proofed. You can look at the statistical properties of numeric data or the unique entities in a list of categorical variables to see if any values are odd or out of place. With spatial data, we can also map the data and visualize it by attribute values to see if anything is out of place spatially. Labels are another helpful tool. Sometimes cleaning attributes uncovers issues with the locations. Let’s make sure the lake names match the lakes the points are in. We’ll make a map and if you zoom in enough, the lake names will appear in the background map data. my.label.options&lt;-labelOptions(clickable=TRUE) #makes a popup with attribute information map.lakename&lt;-mapview(monsters.sf, zcol=&quot;Lake&quot;, legend= FALSE) labels.lakename&lt;-addStaticLabels(map.lakename, label=monsters.sf$Lake, labelOption=my.label.options) labels.lakename Map of monster locations by lake name zoomed in to the Great Lakes Map of monster locations by lake name zoomed in to the Great Lakes And for fun, let’s look at the monster names: map.monstername&lt;-mapview(monsters.sf, zcol=&quot;Name&quot;, legend= FALSE) labels.monstername&lt;-addStaticLabels(map.monstername, label=monsters.sf$Name, labelOption=my.label.options) labels.monstername Map of monster locations by monster name zoomed in to the Great Lakes Map of monster locations by monster name zoomed in to the Great Lakes Yikes! That needs some clean-up too! The name column is missing some names and some records have extra information in them. 22.5.5 Checking Coordinate Reference Systems “Why is my California data showing up in Arizona?” is a common question UC Davis researchers ask on the Geospatial email list. Why does this happen? It’s usually because the CRS for their data is impropperly defined. Someone changed the defnition but didn’t reproject the data (the mathematical process of switching CRSs). Using the wrong CRS will often shift data just enough to look really funny on a map, but sometimes it won’t show up at all. “Why don’t my datasets line up in my map?” Again, it’s your CRS. In this case, they could be correct for all of the datasets you’re using, but each dataset has a different CRS. You can think of CRSs as different dimensions in your favorite SciFi story. Sometimes you can see the other person in the other dimension (CRS), but usually they are too different and you’re nowhere near each other. Datasets have to have the same CRS to make a map or do any analysis. Our data came with lat/long data in another coordinate reference system - EPSG 3395 “World Mercator”, a world projection centered on Europe. Notice how the coordinates look very different from the lat/long coordinates in EPSG 4326 “WGS 84” monsters.df[1:10,13:16] ## lon_3395 lat_3395 lat_dd lon_dd ## 1 -9445647.6 1166706 10.49143 -84.851696 ## 2 3315604.4 -1240572 -11.14741 29.784582 ## 3 -357737.6 7259890 54.65279 -3.213612 ## 4 -12392072.4 5164853 42.21721 -111.319881 ## 5 3552722.8 7688451 56.82407 31.914652 ## 6 11578825.6 2027101 18.02363 104.014360 ## 7 -7845463.9 5433619 43.98618 -70.477001 ## 8 -12704546.6 6054836 47.87777 -114.126884 ## 9 -9467608.2 5128572 41.97447 -85.048971 ## 10 -12525669.2 5011829 41.18707 -112.520000 # Let&#39;s make our World Mercator data spatial so we can explore its CRS monsters.sf.3395&lt;-st_as_sf(x=monsters.df, coords=c(&quot;lon_3395&quot;, &quot;lat_3395&quot;), crs = 3395) # st_crs tells us what the CRS is in well known text (WKT) and EPSG (if it&#39;s avaialble) st_crs(monsters.sf) ## Coordinate Reference System: ## User input: EPSG:4326 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## ENSEMBLE[&quot;World Geodetic System 1984 ensemble&quot;, ## MEMBER[&quot;World Geodetic System 1984 (Transit)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G730)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G873)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1150)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1674)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1762)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G2139)&quot;], ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]], ## ENSEMBLEACCURACY[2.0]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;geodetic latitude (Lat)&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;geodetic longitude (Lon)&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## USAGE[ ## SCOPE[&quot;Horizontal component of 3D system.&quot;], ## AREA[&quot;World.&quot;], ## BBOX[-90,-180,90,180]], ## ID[&quot;EPSG&quot;,4326]] st_crs(monsters.sf.3395) ## Coordinate Reference System: ## User input: EPSG:3395 ## wkt: ## PROJCRS[&quot;WGS 84 / World Mercator&quot;, ## BASEGEOGCRS[&quot;WGS 84&quot;, ## ENSEMBLE[&quot;World Geodetic System 1984 ensemble&quot;, ## MEMBER[&quot;World Geodetic System 1984 (Transit)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G730)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G873)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1150)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1674)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1762)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G2139)&quot;], ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]], ## ENSEMBLEACCURACY[2.0]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4326]], ## CONVERSION[&quot;World Mercator&quot;, ## METHOD[&quot;Mercator (variant A)&quot;, ## ID[&quot;EPSG&quot;,9804]], ## PARAMETER[&quot;Latitude of natural origin&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8801]], ## PARAMETER[&quot;Longitude of natural origin&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8802]], ## PARAMETER[&quot;Scale factor at natural origin&quot;,1, ## SCALEUNIT[&quot;unity&quot;,1], ## ID[&quot;EPSG&quot;,8805]], ## PARAMETER[&quot;False easting&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8806]], ## PARAMETER[&quot;False northing&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8807]]], ## CS[Cartesian,2], ## AXIS[&quot;(E)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;metre&quot;,1]], ## AXIS[&quot;(N)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;metre&quot;,1]], ## USAGE[ ## SCOPE[&quot;Very small scale conformal mapping.&quot;], ## AREA[&quot;World between 80°S and 84°N.&quot;], ## BBOX[-80,-180,84,180]], ## ID[&quot;EPSG&quot;,3395]] # Check to see if they are identical, returning a logical vector identical(st_crs(monsters.sf), st_crs(monsters.sf.3395)) ## [1] FALSE 22.6 Conclusions We’ve learned some of the basics of geospatial data. We learned that the main components of geospatial data are locations, attributes, and a coordinate reference system. We saw how geospatial data can be represented with different data models, but we focused on point vector data. We learned that the data structures we were already familiar with can be modified to contain spatial data. And finally, we looked at some common processes for cleaning our geospatial data. This was a lot to cover, but we just scratched the surface of all your can do with geospatial data science! If you want to learn more, UC Davis has some fantastic introductory classes for GIS (Geographic Information Systems/Science) and Remote Sensing (working satelite data and air photos). 22.7 Optional Further Reading Bolstad, P. 2019. GIS Fundamentals: A first text on geographic information systems. Sixth Edition. XanEdu. Ann Arbor, MI. 764 pp. Sutton, T., O. Dassau, &amp; M. Sutton. 2021. A Gentle Introduction to GIS. https://docs.qgis.org/3.16/en/docs/gentle_gis_introduction/preamble.html (accessed on 2021-02-11) "],["references.html", "23 References", " 23 References An, Weihua, and Yu-Hsin Liu. 2016. “Keyplayer: An R Package for Locating Key Players in Social Networks.” The R Journal 8 (1): 257. https://doi.org/10.32614/RJ-2016-018. Balaban, Alexandru T. 1985. “Applications of Graph Theory in Chemistry.” Journal of Chemical Information and Modeling 25 (3): 334–43. https://doi.org/10.1021/ci00047a033. Bassett, Danielle S., and Olaf Sporns. 2017. “Network Neuroscience.” Nature Neuroscience 20 (3): 353–64. https://doi.org/10.1038/nn.4502. Fan, Chao, and Ali Mostafavi. 2019. “A Graph-Based Method for Social Sensing of Infrastructure Disruptions in Disasters.” Computer-Aided Civil and Infrastructure Engineering 34 (12): 1055–70. https://doi.org/10.1111/mice.12457. Kadushin, Charles. 2012. Understanding Social Networks: Theories, Concepts, and Findings. New York, NY: Oxford University Press. Krebs, Valdis E. 2002. “Mapping Networks of Terrorist Cells.” Connections 24 (3): 43–52. Moreno, Jacob L. 1953. “Who Shall Survive?: Foundations of Sociometry, Group Psychotherapy and Sociodrama.” In. Beacon, N.Y.: Beacon House Inc. Page, Lawrence. 2001. Method for node ranking in a linked database. US6285999B1, issued September 2001. https://patents.google.com/patent/US6285999/en. Robins, Garry, Pip Pattison, Yuval Kalish, and Dean Lusher. 2007. “An Introduction to Exponential Random Graph (p*) Models for Social Networks.” Social Networks, Special section: Advances in Exponential Random Graph (p*) Models, 29 (2): 173–91. https://doi.org/10.1016/j.socnet.2006.08.002. Robins, Garry, Tom Snijders, Peng Wang, Mark Handcock, and Philippa Pattison. 2007. “Recent Developments in Exponential Random Graph (p*) Models for Social Networks.” Social Networks, Special section: Advances in Exponential Random Graph (p*) Models, 29 (2): 192–215. https://doi.org/10.1016/j.socnet.2006.08.003. Wasserman, Stanley, and Katherine Faust. 1994. Social Network Analysis: Methods and Applications. Cambridge, UK: Cambridge University Press. "]]
